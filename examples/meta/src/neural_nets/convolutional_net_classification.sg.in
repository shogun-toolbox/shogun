File f_feats_train = read_csv("@SHOGUN_DATA@/mnist_3class_256d_features_train.dat")
File f_feats_test = read_csv("@SHOGUN_DATA@/mnist_3class_256d_features_test.dat")
File f_labels_train = read_csv("@SHOGUN_DATA@/mnist_3class_256d_labels_train.dat")
File f_labels_test = read_csv("@SHOGUN_DATA@/mnist_3class_256d_labels_test.dat")

#![create_features]
Features features_train = create_features(f_feats_train)
Features features_test = create_features(f_feats_test)
Labels labels_train = create_labels(f_labels_train)
Labels labels_test = create_labels(f_labels_test)
#![create_features]

#![create_instance]
Machine network = create_machine("NeuralNetwork", auto_quick_initialize=True, max_num_epochs=4, epsilon=0.0, optimization_method="NNOM_GRADIENT_DESCENT", gd_learning_rate=0.01, gd_mini_batch_size=3, max_norm=1.0, dropout_input=0.5)
#![create_instance]

#![add_layers]
NeuralLayer input = create_layer("NeuralInputLayer", width=16, height=16, num_neurons=256) 
network.add("layers", input)
NeuralLayer conv1 = create_layer("NeuralConvolutionalLayer", num_maps=3, radius_x=2, radius_y=2, pooling_width=2, pooling_height=2, stride_x=1, stride_y=1)
network.add("layers", conv1)
NeuralLayer conv2 = create_layer("NeuralConvolutionalLayer", num_maps=3, radius_x=2, radius_y=2, pooling_width=2, pooling_height=2, stride_x=1, stride_y=1)
network.add("layers", conv2)
NeuralLayer softmax = create_layer("NeuralSoftmaxLayer", num_neurons=3)
network.add("layers", softmax)
network.put("seed", 10)
#![add_layers]

#![train_and_apply]
network.train(features_train, labels_train)
Labels labels_predict = network.apply(features_test)
#![train_and_apply]

#![evaluate_accuracy]
Evaluation eval = create_evaluation("MulticlassAccuracy")
real accuracy = eval.evaluate(labels_predict, labels_test)
#![evaluate_accuracy]

# additional integration testing variables
RealVector output = labels_predict.get_real_vector("labels")
