{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression comparison between Shogun and Sklearn.\n",
    "\n",
    "We will do a comparison between Shogun's and OpenCV's KRR implementations using a standard regression data-set available [here](https://www.kaggle.com/new-york-city/nyc-property-sales). Our dataset consists of 84548 examples in which we will use \"m\" examples which will be increased progressively. \n",
    "\n",
    "### Note\n",
    "Due to memory restriction we shall only be working with at max m = 10000 examples\n",
    "\n",
    "Let's start with the imports!"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMPY & PANDAS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# MATPLOTLIB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SCIKIT LEARN\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Data reading\n",
    "import io\n",
    "import requests\n",
    "url=\"https://raw.githubusercontent.com/NanuSai/shogun-benchmarks/master/data/nyc-sales.csv\"\n",
    "\n",
    "\n",
    "# SHOGUN\n",
    "from shogun import *\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the CSV File \n",
    "Let's explore the dataset and see it's components."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=requests.get(url).content\n",
    "df=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "We will be applying regression on the column `SALE PRICE`. \n",
    "\n",
    "The data preprocessing will be done in three steps\n",
    "\n",
    "1. Separate data to training and test\n",
    "2. Encode Categorical Variables using LabelEncoding\n",
    "3. Convert features and labels to Shogun Friendly form"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate train and test\n",
    "\n",
    "In the dataset empty values of `SALE PRICE` is set `-` we need to fill it with ```np.nan```."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_nan(df):\n",
    "    for i in range(len(df)):\n",
    "        if(df.loc[i,'SALE PRICE'] == ' -  '):\n",
    "            df.at[i,'SALE PRICE'] = np.nan\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to separate filter our dataframe based on the value of `SALE PRICE`. We will filter out all the data with `np.nan` and use the remaining data for train and validation purpose."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df):\n",
    "    \n",
    "       \n",
    "    #train is the cleaned data\n",
    "    df_train = df[df['SALE PRICE'].notna()]\n",
    "    # Separate target var\n",
    "    col_x = [col for col in df.columns\n",
    "          if col != 'SALE PRICE']\n",
    "\n",
    "    X = df_train[col_x] #Training set\n",
    "\n",
    "    # target variable\n",
    "    target = ['SALE PRICE']\n",
    "\n",
    "    y= df_train[target] #Training targets\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding\n",
    "\n",
    "We first need to identify all the categorical categorical form of data and then convert them into encoded form"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(X):\n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    #cat_vars have all categorical variable name\n",
    "    s = (X.dtypes == 'object')\n",
    "    cat_vars = list(s[s].index)\n",
    "\n",
    "    # drop the unwanted columns\n",
    "    X = X.drop(columns=['Unnamed: 0','NEIGHBORHOOD','ADDRESS','EASE-MENT',\n",
    "                                  'BUILDING CLASS CATEGORY','SALE DATE','APARTMENT NUMBER'])\n",
    "\n",
    "    \n",
    "    #Recheck if any categorical variable remaining.\n",
    "    s = (X.dtypes == 'object')\n",
    "    cat_varsx = list(s[s].index)\n",
    "\n",
    "    #  Labeling properly the columns with object data type\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    num_X= X.copy()\n",
    "    for col in cat_varsx:\n",
    "        num_X[col] = le.fit_transform(X[col])\n",
    "        \n",
    "    return num_X    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert feature to shogun friendly form\n",
    "\n",
    "Now we need to convert pandas to numpy form and reshape label column so that shogun may process them."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(num_X,y):\n",
    "    num_X= num_X.to_numpy().astype(float)  \n",
    "    y= y.to_numpy().astype(float)\n",
    "    y= y.reshape(y.shape[0],) #reshape to (num_samples,)\n",
    "    \n",
    "    return num_X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "\n",
    "df = replace_with_nan(df)        \n",
    "X,y = filter_data(df)\n",
    "num_X= label_encode(X)\n",
    "num_X,y = process_data(num_X,y)\n",
    "\n",
    "#Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(num_X, y, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for plotting\n",
    "time_skl = []\n",
    "time_sgn = []\n",
    "time_diff = []\n",
    "size = [1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]\n",
    "error_skl=[]\n",
    "error_sgn=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "As noted above we will only use at max 10000 examples and see how it predicts on the entire test set. The result will be graphed. We're using RMSE as our error metric."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for m in size:\n",
    "    \n",
    "    print(m)\n",
    "\n",
    "#SKLEARN\n",
    "\n",
    "    X_train_temp = X_train[:m]\n",
    "    y_train_temp = y_train[:m]\n",
    "    skr = KernelRidge(kernel='rbf', gamma=0.1)\n",
    "\n",
    "    t0 = time()\n",
    "    skr.fit(X_train_temp, y_train_temp)\n",
    "    tsk1 = time() - t0\n",
    "\n",
    "    #Apply to test set\n",
    "    y_predict = skr.predict(X_test) #Predict on the entire test set\n",
    "    err_skl = mean_squared_error(y_test,y_predict,squared=False)\n",
    "    error_skl.append(err_skl) #Get error and append to list\n",
    "    \n",
    "\n",
    "    time_skl.append(tsk1) \n",
    "\n",
    "\n",
    "#SHOGUN        \n",
    "    train_features = features(X_train_temp.T)\n",
    "    labels_train = RegressionLabels(y_train_temp)\n",
    "\n",
    "    width = 0.0\n",
    "    gaussian_kernel =  kernel(\"GaussianKernel\", log_width=width)\n",
    "    gaussian_kernel.init(train_features, train_features)\n",
    "    krr = machine(\"KernelRidgeRegression\", labels=labels_train, tau=0.001, kernel=gaussian_kernel)\n",
    "\n",
    "    t0=time()\n",
    "    krr.train()\n",
    "    tsg1=time()-t0\n",
    "\n",
    "    #Apply to test set\n",
    "    labels_predict = krr.apply_regression(features(X_test.T))\n",
    "    err_sgn = mean_squared_error(y_test,labels_predict.get_labels(),squared=False)\n",
    "    error_sgn.append(err_sgn) #get error and append to list\n",
    "    \n",
    "\n",
    "    time_sgn.append(tsg1)\n",
    "   \n",
    "    \n",
    "    \n",
    "    print(\"Sklearn time: \" + str(tsk1))\n",
    "    print(\"Shogun time: \" + str(tsg1))\n",
    "    print(\"SKL Error: \" + str(err_skl))\n",
    "    print(\"Shogun Error: \" + str(err_sgn))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "Both Shogun and Sklearn **perform poorly** because the number of training samples feeded to the machine is very less (10000) compared to the actual given (nearly 80000).Increasing number of samples used by more than 20k will cause `MemoryError` in most computer as it involves inversion of which costs nearly $\\mathcal{O(m^2)}$ or worse where `m=Number of samples`\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting \n",
    "\n",
    "The below graph will plot KRR Computation time and Error RMSE plot for Shogun and SKLearn. Let's see the plot."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING Time_taken\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.title('Computation time KRR : SKL vs SG')\n",
    "plt.plot(size,time_skl,color='red',label='SKlearn')\n",
    "plt.plot(size,time_sgn,color='yellow',label='Shogun')\n",
    "plt.xlabel('Dataset-size')\n",
    "plt.ylabel('time-taken')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING Error\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.title('Error RMSE plot')\n",
    "plt.plot(size,error_skl,color='green',label='SKlearn')\n",
    "plt.plot(size,error_sgn,color='blue',label='Shogun')\n",
    "plt.xlabel('Data-set size')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('shogun': conda)",
   "language": "python",
   "name": "python361064bitshoguncondab2320d49398149f5913e1398ff058ea7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}