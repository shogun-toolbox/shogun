{
 "metadata": {
  "name": "MKL"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Multiple Kernel Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook is about multiple kernel learning in shogun. We will see how to construct a combined kernel, determine optimal kernel weights using MKL and use them for [classification](http://en.wikipedia.org/wiki/Statistical_classification)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import all shogun classes\n",
      "from modshogun import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<em>Multiple kernel learning</em> is about using a combined kernel i.e. a kernel consisting of a linear combination of arbitrary kernels over different domains. The coefficients or weights of the linear combination can be learned as well.\n",
      "\n",
      "[Kernel based methods](http://en.wikipedia.org/wiki/Kernel_methods) such as support vector machines (SVMs)  employ a so-called kernel function $k(x_{i},x_{j})$  which intuitively computes the similarity between two examples $x_{i}$ and $x_{j}$. </br>\n",
      "Selecting the kernel function\n",
      "$k()$  and its parameters is an important issue in training. Kernels designed by humans usually capture one aspect of data. Choosing one kernel means to select exactly one such aspect. Which means combining such ascpects is often better than selecting.\n",
      "\n",
      "In shogun the [CMKL](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CMKL.html) is the base class for MKL. We can do classifications:  [binary](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CMKLClassification.html), [one-class](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CMKLOneClass.html), [multiclass](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CMKLMulticlass.html) and regression too: [regression](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CMKLRegression.html)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Mathematical formulation (skip if just you want code examples)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "</br>So in a svm, defined as:\n",
      "$$f({\\bf x})=\\text{sign} \\left(\\sum_{i=0}^{N-1} \\alpha_i k({\\bf x}, {\\bf x_i})+b\\right)$$</brr>\n",
      "\n",
      "\n",
      "One could make a combination of kernels like:\n",
      "$${\\bf k}(x_i,x_j)=\\sum_{k=0}^{K} \\beta_k {\\bf k_k}(x_i, x_j)$$\n",
      "where $\\beta_k > 0$ and   $\\sum_{k=0}^{K} \\beta_k = 1$\n",
      "\n",
      "In MKL $\\alpha_i$,$\\beta$ and bias are determined by solving the following optimization program\n",
      "\n",
      "$$\\mbox{min} \\hspace{4mm} \\gamma-\\sum_{i=1}^N\\alpha_i$$\n",
      "$$ \\mbox{w.r.t.} \\hspace{4mm}   \\gamma\\in R, \\alpha\\in R^N \\nonumber$$\n",
      "$$\\mbox {s.t.} \\hspace{4mm}  {\\bf 0}\\leq\\alpha\\leq{\\bf 1}C,\\;\\;\\sum_{i=1}^N \\alpha_i y_i=0 \\nonumber$$\n",
      "$$  \\frac{1}{2}\\sum_{i,j=1}^N \\alpha_i \\alpha_j y_i y_j   \\forall k=1,\\ldots,K\\nonumber\\\\\n",
      "$$\n",
      "\n",
      "\n",
      " here C is a pre-specified regularization parameter\n",
      "Within shogun this optimization problem is solved using [semi-infinite programming](http://en.wikipedia.org/wiki/Semi-infinite_programming). For 1-norm MKL using one of the two approaches described in\n",
      "[1].\n",
      "The first approach (also called the wrapper algorithm) wraps around a single kernel SVMs, alternatingly solving for \u03b1 and \u03b2. It is using a traditional SVM to generate new violated constraints and thus requires a single kernel SVM and any of the SVMs contained in shogun can be used. In the MKL step either a linear program is solved via glpk or cplex or analytically or a newton (for norms>1) step is performed.\n",
      "\n",
      "The second much faster but also more memory demanding approach performing interleaved optimization, is integrated into the chunking-based SVMlight.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using a Combined kernel:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Shogun provides an easy way to make combination of kernels using the [CombinedKernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CCombinedKernel.html) class, to which we can append any [Kernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CKernel.html) from the many options shogun provides. It is especially useful to combine kernels working on different domains and to combine kernels looking at independent features and requires [CombinedFeatures](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CCombinedFeatures.html) to be used. Similarly the CombinedFeatures is used to combine a number of feature objects into a single CombinedFeatures object"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kernel = CombinedKernel()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To see the preiction capabilities, lets generate some data using the [GMM](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGMM.html) class. The data is sampled by setting means ([GMM notebook](http://www.shogun-toolbox.org/static/notebook/current/GMM.html)) such that it sufficiently covers X-Y grid and is not too easy to classify."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num=30;\n",
      "num_components=4\n",
      "means=zeros((num_components, 2))\n",
      "means[0]=[-1,1]\n",
      "means[1]=[2,-1.5]\n",
      "means[2]=[-1,-3]\n",
      "means[3]=[2,1]\n",
      "\n",
      "covs=array([[1.0,0.0],[0.0,1.0]])\n",
      "\n",
      "gmm=GMM(num_components)\n",
      "[gmm.set_nth_mean(means[i], i) for i in range(num_components)]\n",
      "[gmm.set_nth_cov(covs,i) for i in range(num_components)]\n",
      "gmm.set_coef(array([1.0,0.0,0.0,0.0]))\n",
      "xntr=array([gmm.sample() for i in xrange(num)]).T\n",
      "xnte=array([gmm.sample() for i in xrange(5000)]).T\n",
      "gmm.set_coef(array([0.0,1.0,0.0,0.0]))\n",
      "xntr1=array([gmm.sample() for i in xrange(num)]).T\n",
      "xnte1=array([gmm.sample() for i in xrange(5000)]).T\n",
      "gmm.set_coef(array([0.0,0.0,1.0,0.0]))\n",
      "xptr=array([gmm.sample() for i in xrange(num)]).T\n",
      "xpte=array([gmm.sample() for i in xrange(5000)]).T\n",
      "gmm.set_coef(array([0.0,0.0,0.0,1.0]))\n",
      "xptr1=array([gmm.sample() for i in xrange(num)]).T\n",
      "xpte1=array([gmm.sample() for i in xrange(5000)]).T\n",
      "traindata=concatenate((xntr,xntr1,xptr,xptr1), axis=1)\n",
      "trainlab=concatenate((-ones(2*num), ones(2*num)))\n",
      "\n",
      "testdata=concatenate((xnte,xnte1,xpte,xpte1), axis=1)\n",
      "testlab=concatenate((-ones(10000), ones(10000)))\n",
      "\n",
      "feats_train=RealFeatures(traindata)   #convert to shogun features\n",
      "labels=BinaryLabels(trainlab)         #generate labels for data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_=jet()\n",
      "figure(figsize(18,5))\n",
      "subplot(121)\n",
      "_=scatter(traindata[0,:], traindata[1,:], c=trainlab, s=100)   # plot train data\n",
      "colors=[\"blue\",\"blue\",\"red\",\"red\"]\n",
      "# a tool for visualisation\n",
      "from matplotlib.patches import Ellipse\n",
      "def get_gaussian_ellipse_artist(mean, cov, nstd=1.96, color=\"red\", linewidth=3):\n",
      "    vals, vecs = eigh(cov)\n",
      "    order = vals.argsort()[::-1]\n",
      "    vals, vecs = vals[order], vecs[:, order]    \n",
      "    theta = numpy.degrees(arctan2(*vecs[:, 0][::-1]))\n",
      "    width, height = 2 * nstd * sqrt(vals)\n",
      "    e = Ellipse(xy=mean, width=width, height=height, angle=theta, \\\n",
      "               edgecolor=color, fill=False, linewidth=linewidth)\n",
      "    \n",
      "    return e\n",
      "for i in range(num_components):\n",
      "    gca().add_artist(get_gaussian_ellipse_artist(means[i], covs, color=colors[i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generating Kernel weights"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Just to help us visualize lets use two gaussian kernels ([CGaussianKernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGaussianKernel.html)) with considerably different widths. We need to append them to the Combined kernel. To generate the optimal weights (i.e $\\beta$s in the above equation), training of [MKL](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CMKLClassification.html) is required. This generates the weights as seen in this example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "width0=0.5\n",
      "kernel0=GaussianKernel(feats_train, feats_train, width0)\n",
      "\n",
      "width1=25\n",
      "kernel1=GaussianKernel(feats_train, feats_train, width1)\n",
      "\n",
      "kernel.append_kernel(kernel0)   #combine kernels\n",
      "kernel.append_kernel(kernel1)\n",
      "kernel.init(feats_train, feats_train)\n",
      "\n",
      "mkl = MKLClassification()\n",
      "mkl.set_mkl_norm(1)  #set the norm, weights sum to 1.\n",
      "mkl.set_C(1, 1)\n",
      "mkl.set_kernel(kernel)\n",
      "mkl.set_labels(labels)\n",
      "\n",
      "mkl.train()    #train to get weights\n",
      "\n",
      "w=kernel.get_subkernel_weights()\n",
      "print w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Prediction using MKL"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The weights generated can be intuitively understood too. We will see that on plotting individual subkernels outputs and and outputs of the MKL classification. To apply on test features, we can reinitialize the kernel with `kernel.init` and pass the test features. After that its just a matter of doing  `mkl.apply` to generate outputs. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "size=100\n",
      "x1=linspace(-5, 5, size)\n",
      "x2=linspace(-5, 5, size)\n",
      "x, y=meshgrid(x1, x2)\n",
      "grid=RealFeatures(array((ravel(x), ravel(y)))) # generate X-Y grid test data\n",
      "\n",
      "kernel0t=GaussianKernel(feats_train, grid, width0)\n",
      "kernel1t=GaussianKernel(feats_train, grid, width1)\n",
      "\n",
      "kernelt=CombinedKernel()\n",
      "kernelt.append_kernel(kernel0t)\n",
      "kernelt.append_kernel(kernel1t)\n",
      "kernelt.init(feats_train, grid)  #initailize with test grid\n",
      "\n",
      "mkl.set_kernel(kernelt)\n",
      "grid_out=mkl.apply()    #prediction\n",
      "\n",
      "z=grid_out.get_values().reshape((size, size))\n",
      "\n",
      "figure(figsize=(10,5))\n",
      "title(\"Classification using MKL\")\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To justify the weights, lets train and compare two subkernels with the MKL classification output. Training MKL classifier with a single kernel appended to a combined kernel makes no sense and is just like normal single kernel based classification, but lets do it for comparison."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z=grid_out.get_labels().reshape((size, size))\n",
      "\n",
      "figure(figsize=(20,5))  # MKL\n",
      "subplot(131, title=\"Multiple Kernels combined\")\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n",
      "comb_ker0=CombinedKernel()\n",
      "comb_ker0.append_kernel(kernel0)\n",
      "comb_ker0.init(feats_train, feats_train)\n",
      "mkl.set_kernel(comb_ker0)\n",
      "mkl.train()\n",
      "comb_ker0t=CombinedKernel()\n",
      "comb_ker0t.append_kernel(kernel0)\n",
      "comb_ker0t.init(feats_train, grid)\n",
      "mkl.set_kernel(comb_ker0t)\n",
      "out0=mkl.apply()\n",
      "\n",
      "z=out0.get_labels().reshape((size, size))  #subkernel 1\n",
      "subplot(132, title=\"Kernel 1\")\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n",
      "comb_ker1=CombinedKernel()\n",
      "comb_ker1.append_kernel(kernel1)\n",
      "comb_ker1.init(feats_train, feats_train)\n",
      "mkl.set_kernel(comb_ker1)\n",
      "mkl.train()\n",
      "comb_ker1t=CombinedKernel()\n",
      "comb_ker1t.append_kernel(kernel1)\n",
      "comb_ker1t.init(feats_train, grid)\n",
      "mkl.set_kernel(comb_ker1t)\n",
      "out1=mkl.apply()\n",
      "\n",
      "z=out1.get_labels().reshape((size, size))  #subkernel 2\n",
      "subplot(133, title=\"kernel 2\")\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see the multiple kernel output seems just about right. Kernel 1 gives a sort of overfiting output while the kernel 2 seems not so accurate. The kernel weights are hence so adjusted to get a refined output. We can have a look at the errors by these subkernels to have more food for thought. Most of the times, the MKL error is lesser as it incorporates aspects of both kernels. One of them is strict while other is lenient, MKL finds a balance between those."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kernelt.init(feats_train, RealFeatures(testdata))\n",
      "mkl.set_kernel(kernelt)\n",
      "out=mkl.apply()\n",
      "\n",
      "evaluator=ErrorRateMeasure()\n",
      "print \"Test error is %2.2f%% :MKL\" % (100*evaluator.evaluate(out,BinaryLabels(testlab)))\n",
      "\n",
      "\n",
      "comb_ker0t.init(feats_train,RealFeatures(testdata)) \n",
      "mkl.set_kernel(comb_ker0t)\n",
      "out=mkl.apply()\n",
      "\n",
      "evaluator=ErrorRateMeasure()\n",
      "print \"Test error is %2.2f%% :Subkernel1\"% (100*evaluator.evaluate(out,BinaryLabels(testlab)))\n",
      "\n",
      "comb_ker1t.init(feats_train, RealFeatures(testdata))\n",
      "mkl.set_kernel(comb_ker1t)\n",
      "out=mkl.apply()\n",
      "\n",
      "evaluator=ErrorRateMeasure()\n",
      "print \"Test error is %2.2f%% :subkernel2\" % (100*evaluator.evaluate(out,BinaryLabels(testlab)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "MKL for knowledge discovery:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "MKL can recover information about the problem at hand. Let us see this with a binary classification problem. The task is to separate two concentric classes shaped like circles. By varying the distance between the boundary of the circles we can control the separability of the problem. Starting with an almost non-separable scenario, the data quickly becomes separable as the distance between the circles increases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def circle(x, radius, neg):\n",
      "        y=sqrt(square(radius)-square(x))\n",
      "        if neg:\n",
      "            return[x, -y]\n",
      "        else:\n",
      "            return [x,y]\n",
      "        \n",
      "def get_circle(radius):\n",
      "    neg=False\n",
      "    range0=linspace(-radius,radius,100)\n",
      "    pos_a=array([circle(i, radius, neg) for i in range0]).T\n",
      "    neg=True\n",
      "    neg_a=array([circle(i, radius, neg) for i in range0]).T\n",
      "    c=concatenate((neg_a,pos_a), axis=1)\n",
      "    return c\n",
      "\n",
      "def get_data(r1, r2):\n",
      "    c1=get_circle(r1)\n",
      "    c2=get_circle(r2)\n",
      "    c=concatenate((c1, c2), axis=1)\n",
      "    feats_tr=RealFeatures(c)\n",
      "    return c, feats_tr\n",
      "\n",
      "l=concatenate((-ones(200),ones(200)))\n",
      "lab=BinaryLabels(l)\n",
      "\n",
      "c, feats_tr=get_data(2,4) #get 2 circles with radius 2 and 4\n",
      "c1, feats_tr1=get_data(2,3)\n",
      "_=gray()\n",
      "figure(figsize=(10,5))\n",
      "title(\"Different separations\")\n",
      "subplot(121)\n",
      "p=scatter(c[0,:], c[1,:], c=lab)\n",
      "subplot(122)\n",
      "q=scatter(c1[0,:], c1[1,:], c=lab)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are the type of circles we want to distinguish between. We can try classification with a constant separation between the circles first."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_mkl(circles, feats_tr):\n",
      "    kernel0=GaussianKernel(feats_tr, feats_tr, 1)  # four kernels with different widths \n",
      "    kernel1=GaussianKernel(feats_tr, feats_tr, 5)\n",
      "    kernel2=GaussianKernel(feats_tr, feats_tr, 7)\n",
      "    kernel3=GaussianKernel(feats_tr, feats_tr, 10)\n",
      "    kernel = CombinedKernel()\n",
      "    kernel.append_kernel(kernel0)\n",
      "    kernel.append_kernel(kernel1)\n",
      "    kernel.append_kernel(kernel2)\n",
      "    kernel.append_kernel(kernel3)\n",
      "    \n",
      "    kernel.init(feats_tr, feats_tr)\n",
      "    mkl = MKLClassification()\n",
      "    mkl.set_mkl_norm(1)\n",
      "    mkl.set_C(1, 1)\n",
      "    mkl.set_kernel(kernel)\n",
      "    mkl.set_labels(lab)\n",
      "    \n",
      "    mkl.train()\n",
      "    \n",
      "    w=kernel.get_subkernel_weights()\n",
      "    return w, mkl\n",
      "\n",
      "def test_mkl(mkl, grid):\n",
      "    kernel0t=GaussianKernel(feats_tr, grid, 1)\n",
      "    kernel1t=GaussianKernel(feats_tr, grid, 5)\n",
      "    kernel2t=GaussianKernel(feats_tr, grid, 7)\n",
      "    kernel3t=GaussianKernel(feats_tr, grid, 10)\n",
      "    kernelt = CombinedKernel()\n",
      "    kernelt.append_kernel(kernel0t)\n",
      "    kernelt.append_kernel(kernel1t)\n",
      "    kernelt.append_kernel(kernel2t)\n",
      "    kernelt.append_kernel(kernel3t)\n",
      "    kernelt.init(feats_tr, grid)\n",
      "    mkl.set_kernel(kernelt)\n",
      "    out=mkl.apply()\n",
      "    return out\n",
      "\n",
      "size=50\n",
      "x1=linspace(-10, 10, size)\n",
      "x2=linspace(-10, 10, size)\n",
      "x, y=meshgrid(x1, x2)\n",
      "grid=RealFeatures(array((ravel(x), ravel(y))))\n",
      "\n",
      "\n",
      "w, mkl=train_mkl(c, feats_tr)\n",
      "print w\n",
      "out=test_mkl(mkl,grid)\n",
      "\n",
      "z=out.get_values().reshape((size, size))\n",
      "\n",
      "figure(figsize=(5,5))\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So MKL classifier classifies them as expected. Now lets vary the separation and see how it affects the weights.The choice of the kernel width of the Gaussian kernel used for classification is expected to depend on the separation distance of the learning problem: An increased distance between the circles will correspond to a larger optimal kernel width. This effect should be visible in the results of the MKL, where we used MKL-SVMs with four kernels with different widths (1,5,7,10). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "range1=linspace(5.5,7.5,50)\n",
      "x=linspace(1.5,3.5,50)\n",
      "temp=[]\n",
      "\n",
      "for i in range1:\n",
      "    c, feats=get_data(4,i)  #vary separation between circles\n",
      "    w, mkl=train_mkl(c, feats)\n",
      "    temp.append(w)\n",
      "y=array([temp[i] for i in range(0,50)]).T\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "figure(figsize=(20,5))\n",
      "_=plot(x, y[0,:], color='k', linewidth=2)\n",
      "_=plot(x, y[1,:], color='r', linewidth=2)\n",
      "_=plot(x, y[2,:], color='g', linewidth=2)\n",
      "_=plot(x, y[3,:], color='y', linewidth=2)\n",
      "title(\"Comparison between kernel widths and weights\")\n",
      "ylabel(\"Weight\")\n",
      "xlabel(\"Distance between circles\")\n",
      "_=legend([\"1\",\"5\",\"7\",\"10\"])\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above plot we see the obtained kernel weightings for the four kernels. Every line shows one weighting. The courses of the kernel weightings reflect the development of the learning problem: as long as the problem is difficult the best separation can be obtained when using the kernel with smallest width. The low width kernel looses importance when the distance between the circle increases and larger kernel widths obtain a larger weight in MKL. Increasing the distance between the circles, kernels with greater widths are used. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "References:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[1] Soeren Sonnenburg, Gunnar Raetsch, Christin Schaefer, and Bernhard Schoelkopf. Large Scale Multiple Kernel Learning. Journal of Machine Learning Research, 7:1531-1565, July 2006.\n",
      "\n",
      "[2] Kernel Methods for Object Recognition , Christoph H. Lampert"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}