{
 "metadata": {
  "name": "HashedDocDotFeatures"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<h2>Large scale document classification with the Shogun Machine Learning Toolbox</h2>\nby Evangelos Anagnostopoulos (github: <a href=\"https://github.com/van51/\">https://github.com/van51</a>)<br>\nSpecial thanks to my mentors for this project on GSoC 2013 : Soeren Sonnenburg, Olivier Chapelle and Benoit Rostykus"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<h3>Document Classification - Background </h3>\nDocument classification consists of assigning documents to specific predefined categories. This usually works by transforming the documents into a vector space that is acceptable by common classifiers, like SVMs, and then learning a model on that new representation.\n\nThe most common and the most widely used representation of document collections is the <a href=\"http://en.wikipedia.org/wiki/Bag-of-words_model\">Bag of Words</a> model.The BoW representation considers each document as a collection of tokens. A token is defined as the minimum arbitrary sequence of characters that can be considered as atomic. Tokens depending on the choice of the user can be either whole words or n-grams, etc. A simple approach is to consider every possible token in your document collection as a different dimension in a feature space and then vectorize each document by assigning 1 to every dimension that corresponds to a token contained in that document and 0 to every other. This is the simpler approach and is known as the Boolean Vector Space Model. Another approach is, instead of using 1/0, to use a weight for every term, like their frequencies, but this is out of our scope.<br>\n\nLet's now consider a document collection consisting of the following documents:<br>\n&nbsp;&nbsp;&nbsp;&nbsp;<b>Document 1</b> = \"This is the first document\"<br>\n&nbsp;&nbsp;&nbsp;&nbsp;<b>Document 2</b> = \"Document classification: Introduction\"<br>\n&nbsp;&nbsp;&nbsp;&nbsp;<b>Document 3</b> = \"A third document about classification\"<br>\n\nand suppose that we consider as tokens, every word that appears in the collection.<br>\nThen we would arrive to the following representation:\n<table><tr><td></td><td><b>Document 1</b></td><td><b>Document 2</b></td><td><b>Document 3</b></td></tr>\n    <tr><td>[0] this</td><td>1</td><td>0</td><td>0</td></tr>\n    <tr><td>[1] is</td><td>1</td><td>0</td><td>0</td></tr>\n    <tr><td>[2] the</td><td>1</td><td>0</td><td>0</td></tr>\n    <tr><td>[3] first</td><td>1</td><td>0</td><td>0</td></tr>\n    <tr><td>[4] document</td><td>1</td><td>1</td><td>1</td></tr>\n    <tr><td>[5] classification</td><td>0</td><td>1</td><td>1</td></tr>\n    <tr><td>[6] introduction</td><td>0</td><td>1</td><td>0</td></tr>\n    <tr><td>[7] a</td><td>0</td><td>0</td><td>1</td></tr>\n    <tr><td>[8] third</td><td>0</td><td>0</td><td>1</td></tr>\n    <tr><td>[9] about</td><td>0</td><td>0</td><td>1</td></tr>\n</table>\nThe above matrix is called the <a href=\"http://en.wikipedia.org/wiki/Document-term_matrix\">document-term matrix</a> and is widely used in Information Retrieval applications. In our case since every document is now represented by a numerical vector, we can just pass this as a dataset to common classifiers, including kernel machines. "
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<h3>Limitations on large scale collections</h3>\nAlthough the aformentioned procedures are pretty intuitive and straight-forward they have some drawbacks when considered on large collections of documents.<br>\nLet's consider the BoW representation first. <br>\n<ul><li>First of all, the dimensionality of the feature space (or else, the number of distinct tokens in the collection) is usually <b>not known beforehand</b> and requires a pass over the entire dataset to be calculated. This can make the creation of the document-term matrix trickier.</li>\n    <li>Secondly, this dimensionality can grow to be <b>very large</b>; imagine considering as tokens the set of every possible word, or every possible 8-gram.</li>\n    <li>Thirdly, this approach requires an <b>additional dictionary</b> in order to work that maps every token to its appropriate dimension, for instance every appearance of 'this' above should always be mapped to dimension 0, and this mapping data structure will also grow to be <b>very large.</b></li></ul>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<h3>The hashing response</h3>\nA convenient approach that eliminates all the problems that the BoW representation introduced is to use a hash function to transform all your tokens to indices on the document-term matrix.<br>This way: <ul>\n<li>the dimensionality of the target feature space is now <b>known beforehand</b>, as it's simply the number of all possible outcomes of the hash functions (which can be restricted to a specific range [0, n-1] by using the modulo function). No extra passes over the dataset are needed this way.</li>\n<li>In addition, the dimension can now be <b>restricted to a certain size</b> that is convenient depending on the limitations of the available hardware</li>\n<li>The dictionary is also <b>not needed</b> anymore since each token is directly mapped to the appropriate dimension through the outcome of the hash function.</li></ul>\n\nThe above approach will result in a hashed document-term matrix, similar to the one shown above, of a pre-defined dimension size (number of rows).<br>\nEach token will now be assigned to a random dimension, although always the same for the same token, that is determined from the hash function.<br> This will probably incure some collisions, but if the hash functions is good enough and our choice for the dimension size is sufficiently large it will not be a problem. Especially if we are considering large collections. <br>\n\nAfter the creation of the hashed document-term matrix, the intuition is to pass as a dataset to a linear classifier that will take care of the rest!<br>\n<h3>On-the-fly Hashing</h3>\nA naive way to implement the hashing procedure is to pre-hash every token of the collection and store it to the disk for later access. However, this approach is not very convenient since it will require space analogous to the size of the original dimension. It will also make the experimentation on different parameters, like the size of the hash function or the choice of tokens a bit trickier since one would have to re-convert the entire collection based on the choice of these parameters.<br>\nThe response to that is to read our collection as it is and compute the hash of every token only when it's required, on-the-fly."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<h3>On-the-fly Hashing with Shogun</h3>\nWe will now have a look at how the above idea is represented in the Shogun Toolbox. That is we will see how we can load our document collection in memory and consider a hashed document-term matrix with the hashing of every document (or token more specifically) happening on-the-fly, only when it's required to be computed. Altough it may sound a bit tricky, it's actually pretty straightforward and here is how.<br><br>\nFirst of all we import the required components from the modshogun library."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from modshogun import StringCharFeatures, RAWBYTE, HashedDocDotFeatures, NGramTokenizer",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The <a href=\"http://shogun-toolbox.org/doc/en/current/classshogun_1_1CStringFeatures.html\">StringCharFeatures</a> are nothing more than a collection of char vectors, where each one represents a single document.<br>\n<a href=\"http://shogun-toolbox.org/doc/en/current/namespaceshogun.html#a7fffbfce3d76cf49bde3916d2\">RAWBYTE</a> is simply an enum that specifies that every possible character can be found in the collection.<br>\nThe HashedDocDotFeatures is where all the magic happens! This class is responsible for encapsulating the document collection and providing the calculation of the hashed representation of each document whenever it is required. <br>\nThe NGramTokenizer is the tokenizer we will use on our collection. Its job is to parse a document and create the tokens. As its name suggests, it creates a sequence of ngrams. Another option would be the DelimiterTokenizer, which allows the user to specify the delimiter that will be used in order to create the tokens. <br><br><br>\nSuppose now that we have the following documents:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "doc_1 = \"this is the first document\"\ndoc_2 = \"document classification introduction\"\ndoc_3 = \"a third document about classification\"\ndocument_collection = [doc_1, doc_2, doc_3]",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We will take some time off now to assign each document to a category, to ease our work later on. Since the two last documents refer to classification we will label them as \"Relevant\" and the first document as \"Not relevant\". Since we only have two categories, this makes it a binary problem and we will represent \"Relevant\" as 1 and \"Not relevant\" as -1."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from modshogun import BinaryLabels\nfrom numpy import array\n\nlabels = BinaryLabels(array([-1, 1, 1]))",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We now create our document collection:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "string_features = StringCharFeatures(document_collection, RAWBYTE)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Now we will create the object responsible for the hashed BoW representation. We are going to specify that we want a hash size of 5 bits, which will be translated to a dimension of size 2^5 = 32 (powers of 2 are considered to speed up computatins) and a tokenizer that creates tokens on whitespace and new lines. We will also specify that we want to\nnormalize each hashed vector with regards to the square root of the document length, which is a common approach."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "normalize = True\nhash_size = 8\n\ntokenizer = NGramTokenizer(5)\nhashed_feats = HashedDocDotFeatures(hash_size, string_features, tokenizer, normalize)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "And that was it!<br>\nThe hashed_feats object now has all the required parameters and knows how to communicate and provide the hashed representation <b>only</b> when it is needed by the various algorithms of Shogun. <br><br>\nSo how do we proceed to actually learn a model that can classify documents?<br>\nWe said before that the idea after we have taken care of the hashing is to use a linear classifier.<br>\nShogun has many <a href=\"http://shogun-toolbox.org/doc/en/current/classshogun_1_1CLinearMachine.html#a13f46585aefbfa0376c051817d6f5c46\">linear</a> classifiers available, but for the moment we will consider <a href=\"http://shogun-toolbox.org/doc/en/current/classshogun_1_1CSVMOcas.html\">SVMOcas</a> and we will see how to use it:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from modshogun import SVMOcas\n\nC = 0.1\nepsilon = 0.01\nsvm = SVMOcas(C, hashed_feats, labels)\nsvm.set_epsilon(epsilon)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We have now created our svm. The parameter C specifies the regularization constant. The best choice for this parameter will usually be selected after a model selection process.<br>\nNow, let's train our svm:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "svm.train()",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "When the execution finishes, we will have learned our so desired linear model!  Mind that for large collections the above call can take hours.<br>\nNow that we have a model ready, we would like to see how well it does. Let's see what it predicts for our training data:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "predicted_labels = svm.apply()\nprint (predicted_labels.get_labels())",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We can see that it misclassified the first document. This has to do with the nature of our overly-simplified toy dataset which doesn't provide enough information. However, another option of the HashedDocDotFeatures class will allow us to extract some more information from the same dataset!<br>\n<br>\n<h3>Quadratic Features</h3>\nGenerally, the quadratic features refer to the production of new features from our current dataset, by multiplying currently existing features.\n<br>For example, if we have a dataset consisting of the following example vector:\n<table><tr><td></td><td>A</td><td>B</td><td>C</td></tr>\n    <tr><td>x:</td><td>2</td><td>3</td><td>1</td></tr></table>\nWe can create the following extra quadratic features:\n<table><tr><td></td><td>A</td><td>B</td><td>C</td><td>AA</td><td>AB</td><td>AC</td><td>BB</td><td>BC</td><td>CC</td></tr>\n    <tr><td>x:</td><td>2</td><td>3</td><td>1</td><td>2*2=4</td><td>2*3=6</td><td>2*1=2</td><td>3*3=9</td><td>3*1=3</td><td>1*1=1</td></tr></table>\nThe creation of these features can be very useful, however it should be used with caution since it will also increase the training time and the size requirements. <br>\nThe idea behind the quadratic features is very simple and can be easily implemented for numerical features. However in the case of text collections, where we have tokens instead of numbers, things are not so straightforward.<br><br>\nThe approach we have selected in Shogun for the combination of different tokens is to consider a k-skip n-grams approach. A n-grams in this context refers to a collection of n consecutive tokens and should not be confused with our choice for what consists a token, where it can be whole words or ngrams themselves. The k-skip in front allows us to skip up-to-k tokens when we group them in up-to-n sized groups. Although this is a simple idea it can be a bit confusing when introduced through a definition, so let's have a look at an example:\nSuppose we have the following <b>consecutive</b> tokens:<br>\nTokens : [\"a\", \"b\", \"c\", \"d\"]<br>\nand that we want to consider a 2-skip 2-grams approach. This means that we can combine up-to-2 tokens (that is a single token or two tokens) while skipping up-to-2 tokens between them (that means 0 or 1 or 2 token skips). So we obtain the following combinations: <br>\n[\"a\" (0-skips, 1-gram), \"ab\" (0-skips, 2-gram), \"ac\" (1-skip 2-gram), \"ad\" (2-skips, 2-gram), \"b\" (0-skips, 1-gram), \"bc\" (0-skips, 2-gram),\n\"bd\" (1-skip, 2-gram), \"c\" (0-skip, 1-gram), \"cd\" (0-skip, 2-gram), \"d\" (0-skip, 1-gram)] <br>\n<br>These newly created tokens are then treated like our regular ones and hashed to find their appropriate dimension.\n<br>As discussed above this idea may enhance our information on the dataset, but it also requires more time to be computed. So if used, the choice of k and n should be kept to small numbers.<br>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<h3>Quadratic Features with HashedDocDotFeatures</h3>\n\nIn order to use the k-skip n-gram approach explained above, one only has to specify his choice for k and n to the HashedDocDotFeatures object he is creating.<br>\nSo, by keeping our previous choice of parameters intact and introducing the k and n we obtaing the following piece of code :"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "k = 3 # number of tokens, up to which we allow it to skip\nn = 3 # number of tokens, up to which we allow it to combine\n\nhashed_feats_quad = HashedDocDotFeatures(hash_size, string_features, tokenizer, normalize, n, k)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "If we do not specify these numbers, as we did not do before, then, (you maybe have guessed it!) they are set by default to the following values, n=1, k=0!\n<br>\nLet's see if there was any improvement in our predicting capabilities.<br>\nBy keeping the same svm, we will train on the new features we have created. Although we have the same document collection, we consider different parameters."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "svm.set_features(hashed_feats_quad)\nsvm.train()\npredicted_labels = svm.apply()\nprint(predicted_labels.get_labels())",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Better!<br>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<h3>Experiments on real datasets</h3>\n\nThe example that was demonstrated before was very small and simple and was only for demonstration purposes, to show the capabilities of the HashedDocDotFeatures class. <br> <br><h4>Webspam</h4>Now let's have a look at the results the HashedDocDotFeatures class together with SVMOcas produced when faced against the <a href=\"ftp://largescale.ml.tu-berlin.de/largescale/webspam/\">webspam</a> dataset, which consists of 350000 documents labelled either as \"Spam\" or as \"Not spam\".<br>We demonstrate results when compared against some base methods from <a href=\"http://sonnenburgs.de/soeren/publications/SonRaeRie07.pdf\">Large Scale Learning with String Kernels</a>, specifically table 4.4. The settings used, were the following:<br>\nnumber of hash_bits = 16, C = 1, ngrams of size 8, normalization = True, no quadratic features.\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from pylab import *\n\n# size in number of documents\nsize = [5000, 10000, 50000, 100000]\n\n# time in seconds\nspec_time = [1977, 6039, 94012, Infinity]\nlin_spec_time = [4030, 9128, 44706, 107661]\nhashed_doc_dot_features = [2472, 4662, 26467, 57258]\n\n# auROC\nspec_auROC = [99.1, 99.32, 99.59, 99.64]\nhashed_auROC = [99.5, 99.74, 99.89, 99.91]\n\nplot(size, spec_time, 'k')\nplot(size, lin_spec_time, 'm')\nplot(size, hashed_doc_dot_features, 'g')\nlegend([\"Spec\", \"Linear spec\", \"HashedDocDotFeatures with SVMOcas\"])\nxlabel(\"# of documents\")\nylabel(\"time in seconds\")",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "clf\n\nplot(size, spec_auROC, 'k')\nplot(size, hashed_auROC, 'g')\n\nlegend([\"Spec auROC\", \"HashedDocDotFeatures with SVMOcas auROC\"], \"lower center\")\nxlabel(\"# of documents\")\nylabel(\"auROC\")\nylim([97, 100])",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<h4>Language detection</h4><br>\nAnother experiment we conducted was to create a language detection application. For that purpose we created a dataset consisting of 10,000 documents for 5 different languages; English, Greek, German, Italian and Spanish. <br>The demo uses underneath it a svm model trained on 30,000 documents with Ngrams of size 4, bits of hash size = 18, quadratic features with options: 2-skips 3-grams and svm C = 0.1. The demo can be found here and you can test it on your own how well it does!"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<h3>Extra stuff</h3>\n\nThe hashing trick has also been implemented in Shogun for the DenseFeatures and SparseFeatures classes, as HashedDenseFeatures and HashedSparseFeatures classes. These classes also support quadratic features and provide the option to maintain or drop the linear terms. <br>\nFor online algorithms or for cases when the data do not fit in memory there exist similar classes with the prefix \"Streaming\" that support reading examples from the disk and providing them to some algorithms one at a time. The classes specifically are StreamingHashedDocDotFeatures, StreamingHashedDenseFeatures and StreamingHashedSparseFeatures. If one has mixed features, that are not just numerical or not just text, then he can use the CombinedDotFeatures class to combine objects of the aforementioned classes!<br>Another option is to use the Vw* algorithms and the VwExample class that require the input to be in vw format and are a bit trickier to use."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<h3>References</h3>\n\nKilian Weinberger, Anirban Dasgupta, Josh Attenberg, John Langford, Alex Smola : Feature Hashing for Large Scale Multitask Learning<br>\nOlivier Chapelle, Eren Manavoglu, Romer Rosales : Simple and scalable response prediction for display advertising<br>\nVowpal Wabbit : https://github.com/JohnLangford/vowpal_wabbit/wiki<br>\nA post in metaoptimize : http://metaoptimize.com/qa/questions/6943/what-is-the-hashing-trick<br>"
    }
   ],
   "metadata": {}
  }
 ]
}