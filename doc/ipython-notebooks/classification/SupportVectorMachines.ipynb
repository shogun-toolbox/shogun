{
 "metadata": {
  "name": "SupportVectorMachines",
  "signature": "sha256:6db2c80457528b0426120d5e611bae2b85c141567c6f8b32d9ae3dd7addc3332"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classification with Support Vector Machines"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "by Soeren Sonnenburg | Saurabh Mahindre - <a href=\\\"https://github.com/Saurabh7\\\">github.com/Saurabh7</a> as a part of <a href=\\\"http://www.google-melange.com/gsoc/project/details/google/gsoc2014/saurabh7/5750085036015616\\\">Google Summer of Code 2014 project</a> mentored by - Heiko Strathmann - <a href=\\\"https://github.com/karlnapf\\\">github.com/karlnapf</a> - <a href=\\\"http://herrstrathmann.de/\\\">herrstrathmann.de</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook illustrates how to train a binary <a href=\"http://en.wikipedia.org/wiki/Support_vector_machine\">support vector machines</a> (SVM) <a href=\"http://en.wikipedia.org/wiki/Statistical_classification\">classifier</a> using Shogun. The <a href=\"http://www.shogun-toolbox.org/doc/en/3.0.0/classshogun_1_1CLibSVM.html\">CLibSVM</a> class of shogun is used to train with the <a href=\"http://en.wikipedia.org/wiki/Sampled_Gaussian_kernel#The_sampled_Gaussian_kernel\">sampled Gaussian Kernel</a>. Multiclass classification is also demonstrated using [CGMNPSVM](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGMNPSVM.html)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Support Vector Machines (SVM's) are a learning method used for binary classi\f",
      "fication. The basic idea is to \f",
      "find a hyperplane which separates the $\\mathcal{D}$-dimensional data perfectly into its two classes. However, since example data is often not linearly separable, SVM's introduce the notion of a kernel induced feature space which casts the data into a higher dimensional space where the data is separable. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Linear Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a supervised learning problem, we are given a labeled set of input-output pairs $\\mathcal{D}=(x_i,y_i)^N_{i=1}\\subseteq \\mathcal{X} \\times \\mathcal{Y}$ where $x\\in\\mathcal{X}$ and $y\\in\\{-1,+1\\}$. [SVM](https://en.wikipedia.org/wiki/Support_vector_machine) is a binary classifier that tries to separate objects of different classes by finding a (hyper-)plane such that the margin between the two classes is maximized. A hyperplane in $\\mathcal{R}^D$ can be parameterized by a vector $\\bf{w}$ and a constant $\\text b$ expressed in the equation:$${\\bf w}\\cdot{\\bf x} + \\text{b} = 0$$\n",
      "Given\n",
      "such a hyperplane ($\\bf w$,b) that separates the data, this gives the function $$f(x) = \\text {sign} ({\\bf w}\\cdot{\\bf x} + {\\text b})$$\n",
      "\n",
      "If the training data are linearly separable, we can select two hyperplanes in a way that they separate the data and there are no points between them, and then try to maximize their distance. The region bounded by them is called \"the margin\". These hyperplanes can be described by the equations\n",
      "$$({\\bf w}\\cdot{\\bf x} + {\\text b}) = 1$$\n",
      "$$({\\bf w}\\cdot{\\bf x} + {\\text b}) = -1$$\n",
      "the distance between these two hyperplanes is $\\frac{2}{\\|\\mathbf{w}\\|}$, so we want to minimize $\\|\\mathbf{w}\\|$.\n",
      "$$\n",
      "    \\arg\\min_{(\\mathbf{w},b)}\\frac{1}{2}\\|\\mathbf{w}\\|^2  \\qquad\\qquad(1)$$\n",
      "This gives us a hyperplane that maximizes the geometric distance to the closest data points.\n",
      "As we also have to prevent data points from falling into the margin, we add the following constraint: for each ${i}$ either\n",
      "$$({\\bf w}\\cdot{x}_i + {\\text b}) \\geq 1$$ or\n",
      "$$({\\bf w}\\cdot{x}_i + {\\text b}) \\leq -1$$\n",
      "which is similar to\n",
      "$${y_i}({\\bf w}\\cdot{x}_i + {\\text b}) \\geq 1   \\forall i$$\n",
      "\n",
      "[Lagrange multipliers](http://en.wikipedia.org/wiki/Lagrange_multiplier) are used to modify equation $(1)$ and the corresponding dual of the  problem is eventually transformed into:\n",
      "\n",
      "  \\begin{eqnarray*}\n",
      "       \\max_{\\bf \\alpha} && \\sum_{i=1}^{N} \\alpha_i - \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\alpha_i y_i \\alpha_j y_j  {\\bf x_i} \\cdot {\\bf x_j}\\\\\n",
      "       \\mbox{s.t.} && 0\\leq\\alpha_i\\leq C\\\\\n",
      "                   && \\sum_{i}^{N} \\alpha_i y_i=0\\\\\n",
      "\\end{eqnarray*}\n",
      " where C is a pre-specified regularization parameter trading of the size of the margin and the training error (explained in later section).\n",
      "\n",
      "From the derivation of these equations, it was seen that the optimal hyperplane can be written as:\n",
      "$$\\mathbf{w} = \\sum_i \\alpha_i y_i \\mathbf{x}_i.  $$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Prediction using Linear SVM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let us see how one can train a linear support vector machine with shogun. Two dimensional data (having 2 attributes say: attribute1 and attribute2) is now sampled to demonstrate the classification."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#To import all shogun classes\n",
      "from modshogun import *\n",
      "\n",
      "#Generate some random data\n",
      "X = 2 * random.randn(10,2)\n",
      "traindata=r_[X + 3, X + 7].T\n",
      "\n",
      "feats_train=RealFeatures(traindata)\n",
      "\n",
      "trainlab=concatenate((ones(10),-ones(10)))\n",
      "labels=BinaryLabels(trainlab)\n",
      "\n",
      "# Plot the training data\n",
      "figure(figsize=(6,6))\n",
      "gray()\n",
      "_=scatter(traindata[0, :], traindata[1,:], c=labels, s=50)\n",
      "title(\"Training Data\")\n",
      "xlabel('attribute1')\n",
      "ylabel('attribute2')\n",
      "gray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Liblinear](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CLibLinear.html) is used to do the classification. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#prameters to svm\n",
      "C=1\n",
      "epsilon=1e-3\n",
      "\n",
      "svm=LibLinear(C, feats_train, labels)\n",
      "svm.set_liblinear_solver_type(L2R_L2LOSS_SVC)\n",
      "svm.set_epsilon(epsilon)\n",
      "\n",
      "#train\n",
      "svm.train()\n",
      "w=svm.get_w()\n",
      "b=svm.get_bias()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We solve ${\\bf w}\\cdot{\\bf x} + \\text{b} = 0$ to get the separating hyperplane. The methods `get_w()` and `get_bias()` are used to get the necessary values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#solve for w.x+b=0\n",
      "x1=linspace(-1.0, 11.0, 100)\n",
      "def solve (x1):\n",
      "    return -( ( (w[0])*x1 + b )/w[1] )\n",
      "\n",
      "x2=map(solve, x1)\n",
      "\n",
      "#plot\n",
      "figure(figsize=(6,6))\n",
      "gray()\n",
      "_=scatter(traindata[0, :], traindata[1,:], c=labels, s=50)\n",
      "_=plot(x1,x2, linewidth=2)\n",
      "title(\"Separating hyperplane\")\n",
      "xlabel('attribute1')\n",
      "ylabel('attribute2')\n",
      "gray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The classifier is now applied on a X-Y grid of points to get predictions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "size=100\n",
      "x1_=linspace(-5, 15, size)\n",
      "x2_=linspace(-5, 15, size)\n",
      "x, y=meshgrid(x1_, x2_)\n",
      "#Generate X-Y grid test data\n",
      "grid=RealFeatures(array((ravel(x), ravel(y))))\n",
      "\n",
      "#apply on test grid\n",
      "predictions = svm.apply(grid)\n",
      "\n",
      "z=predictions.get_values().reshape((size, size))\n",
      "\n",
      "#plot\n",
      "jet()\n",
      "figure(figsize=(8,6))\n",
      "title(\"Classification\")\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n",
      "gray()\n",
      "_=scatter(traindata[0, :], traindata[1,:], c=labels, s=50)\n",
      "xlabel('attribute1')\n",
      "ylabel('attribute2')\n",
      "gray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SVMs using kernels"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the data set is not linearly separable, a non-linear mapping $\\Phi:{\\bf x} \\rightarrow \\Phi({\\bf x}) \\in \\mathcal{F} $ is used. This maps the data into a higher dimensional space where it is linearly separable. Our equation requires only the inner dot products ${\\bf x_i}\\cdot{\\bf x_j}$. The equation can be defined in terms of inner products $\\Phi({\\bf x_i}) \\cdot \\Phi({\\bf x_j})$ instead. Since  $\\Phi({\\bf x_i})$ occurs only in dot products with $ \\Phi({\\bf x_j})$ it is sufficient to know the formula (kernel function) : $$K({\\bf x_i, x_j} ) =  \\Phi({\\bf x_i}) \\cdot \\Phi({\\bf x_j})$$ without dealing with the maping directly. The transformed equation is:\n",
      "\n",
      "\\begin{eqnarray*} \\max_{\\bf \\alpha} && \\sum_{i=0}^{N-1} \\alpha_i - \\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1} \\alpha_i y_i \\alpha_j y_j k({\\bf x_i}, {\\bf x_j})\\\\ \\mbox{s.t.} && 0\\leq\\alpha_i\\leq C\\\\ && \\sum_{i=0}^{N-1} \\alpha_i y_i=0 \\qquad\\qquad(2)\\\\ \\end{eqnarray*}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Kernels in Shogun"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Shogun provides many options for the above mentioned kernel functions. [CKernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CKernel.html) is the base class for kernels. Some commonly used kernels : \n",
      "\n",
      "* [Gaussian kernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGaussianKernel.html) : Popular Gaussian kernel computed as $k({\\bf x},{\\bf x'})= exp(-\\frac{||{\\bf x}-{\\bf x'}||^2}{\\tau})$\n",
      "\n",
      "* [Linear kernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CLinearKernel.html) : Computes $k({\\bf x},{\\bf x'})= {\\bf x}\\cdot {\\bf x'}$\n",
      "* [Polynomial kernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CPolyKernel.html) : Polynomial kernel computed as $k({\\bf x},{\\bf x'})= ({\\bf x}\\cdot {\\bf x'}+c)^d$\n",
      "\n",
      "* [Simgmoid Kernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CSigmoidKernel.html) : Computes $k({\\bf x},{\\bf x'})=\\mbox{tanh}(\\gamma {\\bf x}\\cdot{\\bf x'}+c)$\n",
      "\n",
      "Some of these kernels are initialised below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gaussian_kernel=GaussianKernel(feats_train, feats_train, 100)\n",
      "#Polynomial kernel of degree 2\n",
      "poly_kernel=PolyKernel(feats_train, feats_train, 2, True)\n",
      "linear_kernel=LinearKernel(feats_train, feats_train)\n",
      "\n",
      "kernels=[linear_kernel, poly_kernel, gaussian_kernel]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Prediction using kernel based SVM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we train an SVM with this GaussianKernel. We use LibSVM but we could use any of the [other SVM](http://www.shogun-toolbox.org/doc/en/current/classshogun_1_1CSVM.html) from shogun. They all utilize the same kernel framework and so are drop-in replacements."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C=1\n",
      "epsilon=1e-3\n",
      "svm=LibSVM(C, gaussian_kernel, labels)\n",
      "_=svm.train()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Just for fun we compute the kernel matrix and display it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jet()\n",
      "def display_km(kernels, svm):\n",
      "    figure(figsize=(20,6))\n",
      "    suptitle('Kernel matrices for different kernels', fontsize=12)\n",
      "    i=1\n",
      "    for kernel in kernels:\n",
      "        subplot(1, len(kernels), i)\n",
      "        title(kernel.get_name())\n",
      "        i+=1\n",
      "        svm.set_kernel(kernel)\n",
      "        svm.train()\n",
      "        km=kernel.get_kernel_matrix()\n",
      "        _=imshow(km)\n",
      "        _=colorbar()\n",
      "\n",
      "display_km(kernels, svm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could now check a number of properties like what the value of the objective function returned by the particular SVM learning algorithm or the explictly computed primal and dual objective function is"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "libsvm_obj=svm.get_objective()\n",
      "primal_obj, dual_obj=svm.compute_svm_primal_objective(), svm.compute_svm_dual_objective()\n",
      "\n",
      "print libsvm_obj, primal_obj, dual_obj"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and based on the objectives we can compute the duality gap, a measure of convergence quality of the svm training algorithm. In theory it is 0 at the optimum and in reality at least close to 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"duality_gap\", dual_obj-primal_obj"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's now apply on the X-Y grid data and plot the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out=svm.apply(RealFeatures(grid))\n",
      "z=out.get_values().reshape((size, size))\n",
      "\n",
      "#plot\n",
      "jet()\n",
      "figure(figsize=(16,7))\n",
      "subplot(121)\n",
      "title(\"Classification\")\n",
      "c=pcolor(x1_, x2_, z)\n",
      "_=contour(x1_ , x2_, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n",
      "gray()\n",
      "_=scatter(traindata[0, :], traindata[1,:], c=labels, s=50)\n",
      "xlabel('attribute1')\n",
      "ylabel('attribute2')\n",
      "jet()\n",
      "\n",
      "z=out.get_labels().reshape((size, size))\n",
      "subplot(122)\n",
      "title(\"Decision boundary\")\n",
      "c=pcolor(x1_, x2_, z)\n",
      "_=contour(x1_ , x2_, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n",
      "_=scatter(traindata[0, :], traindata[1,:], c=labels, s=50)\n",
      "xlabel('attribute1')\n",
      "ylabel('attribute2')\n",
      "gray()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Soft margins and slack variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If there is no clear classification possible using a hyperplane, we need to classify the data as nicely as possible while incorporating the misclassified samples. To do this a concept of soft margin is used. The method introduces non-negative slack variables, $\\xi_i$, which measure the degree of misclassification of the data $x_i$.\n",
      "$$\n",
      "    y_i(\\mathbf{w}\\cdot\\mathbf{x_i} + b) \\ge 1 - \\xi_i \\quad 1 \\le i \\le N  $$\n",
      "\n",
      "Introducing a linear penalty function leads to \n",
      "$$\\arg\\min_{\\mathbf{w},\\mathbf{\\xi}, b } ({\\frac{1}{2} \\|\\mathbf{w}\\|^2 +C \\sum_{i=1}^n \\xi_i) }$$\n",
      "\n",
      "This in its dual form is the familiar equation $\\qquad(2)$.\n",
      "\\begin{eqnarray*} \\max_{\\bf \\alpha} && \\sum_{i=0}^{N-1} \\alpha_i - \\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1} \\alpha_i y_i \\alpha_j y_j k({\\bf x_i}, {\\bf x_j})\\\\ \\mbox{s.t.} && 0\\leq\\alpha_i\\leq C\\\\ && \\sum_{i=0}^{N-1} \\alpha_i y_i=0 \\\\ \\end{eqnarray*}\n",
      "\n",
      "The result is that soft-margin SVM could choose decision boundary that has non-zero training error even if dataset is linearly separable but is less likely to overfit."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's an example using libSVM on the above used data set. Highlighted points show support vectors. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_sv(C_values):\n",
      "    figure(figsize=(20,6))\n",
      "    suptitle('Soft and hard margins with varying C', fontsize=12)\n",
      "    for i in range(len(C_values)): \n",
      "        subplot(1, len(C_values), i+1)\n",
      "        linear_kernel=LinearKernel(feats_train, feats_train)\n",
      "        svm1=LibSVM(C_values[i], linear_kernel, labels)\n",
      "        svm1.train()\n",
      "        vec1=svm1.get_support_vectors()\n",
      "        X_=[]\n",
      "        Y_=[]\n",
      "        new_labels=[]\n",
      "        for j in vec1:\n",
      "            X_.append(traindata[0][j])\n",
      "            Y_.append(traindata[1][j])\n",
      "            new_labels.append(trainlab[j])\n",
      "        out1=svm1.apply(RealFeatures(grid))\n",
      "        z1=out1.get_labels().reshape((size, size))\n",
      "        jet()\n",
      "        c=pcolor(x1_, x2_, z1)\n",
      "        contour(x1_ , x2_, z1, linewidths=1, colors='black', hold=True)\n",
      "        colorbar(c)\n",
      "        gray()\n",
      "        scatter(X_, Y_, c=new_labels, s=150)\n",
      "        scatter(traindata[0, :], traindata[1,:], c=labels, s=20)\n",
      "        title('Support vectors for C=%.2f'%C_values[i])\n",
      "        xlabel('attribute1')\n",
      "        ylabel('attribute2')\n",
      "        \n",
      "        \n",
      "C_values=[0.1, 1000]\n",
      "plot_sv(C_values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that lower value of C causes classifier to sacrifice linear separability in order to gain stability, in a sense that influence of any single datapoint is now bounded by C. For hard margin SVM, support vectors are the points which are \"on the margin\". In the picture above, C=1000 is pretty close to hard-margin SVM, and you can see the highlighted points are the ones that will touch the margin. Often this leads to overfitting. For soft-margin SVM, it's easer to explain them in terms of dual (equation $(2)$) variables. Support vectors are datapoints from training set which are are included in the predictor, ie, the ones with non-zero $\\alpha_i$ parameter. This includes margin errors and points on the margin of the hyperplane."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Binary classification using different kernels"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Two-dimensional Gaussians are generated as data for this section.\n",
      "\n",
      "$x_-\\sim{\\cal N_2}(0,1)-d$\n",
      "\n",
      "$x_+\\sim{\\cal N_2}(0,1)+d$\n",
      "\n",
      "and corresponding positive and negative labels. We create traindata and testdata with ```num``` of them being negatively and positively labelled in traindata,trainlab and testdata, testlab. For that we utilize shoguns gaussian mixture model class ([GMM](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGMM.html)) from which we sample the data points and plot them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num=150;\n",
      "dist=1.0;\n",
      "\n",
      "gmm=GMM(2)\n",
      "gmm.set_nth_mean(array([-dist,-dist]),0)\n",
      "gmm.set_nth_mean(array([dist,dist]),1)\n",
      "gmm.set_nth_cov(array([[1.0,0.0],[0.0,1.0]]),0)\n",
      "gmm.set_nth_cov(array([[1.0,0.0],[0.0,1.0]]),1)\n",
      "\n",
      "gmm.set_coef(array([1.0,0.0]))\n",
      "xntr=array([gmm.sample() for i in xrange(num)]).T\n",
      "\n",
      "gmm.set_coef(array([0.0,1.0]))\n",
      "xptr=array([gmm.sample() for i in xrange(num)]).T\n",
      "\n",
      "traindata=concatenate((xntr,xptr), axis=1)\n",
      "trainlab=concatenate((-ones(num), ones(num)))\n",
      "\n",
      "#shogun format features\n",
      "feats_train=RealFeatures(traindata)\n",
      "labels=BinaryLabels(trainlab)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gaussian_kernel=GaussianKernel(feats_train, feats_train, 50)\n",
      "#Polynomial kernel of degree 2\n",
      "poly_kernel=PolyKernel(feats_train, feats_train, 2, True)\n",
      "linear_kernel=LinearKernel(feats_train, feats_train)\n",
      "\n",
      "kernels=[gaussian_kernel, poly_kernel, linear_kernel]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#train machine\n",
      "C=1\n",
      "svm=LibSVM(C, gaussian_kernel, labels)\n",
      "_=svm.train()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets plot the contour output on a $-5...+5$ grid for \n",
      "\n",
      "1. The Support Vector Machines decision function $\\mbox{sign}(f(x))$\n",
      "2. The Support Vector Machines raw output $f(x)$\n",
      "3. The Original Gaussian Mixture Model Distribution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "size=100\n",
      "x1=linspace(-5, 5, size)\n",
      "x2=linspace(-5, 5, size)\n",
      "x, y=meshgrid(x1, x2)\n",
      "grid=RealFeatures(array((ravel(x), ravel(y))))\n",
      "grid_out=svm.apply(grid)\n",
      "z=grid_out.get_labels().reshape((size, size))\n",
      "\n",
      "jet()\n",
      "figure(figsize=(20,5))\n",
      "subplot(131)\n",
      "title('Decision boundary')\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n",
      "z=grid_out.get_values().reshape((size, size))\n",
      "\n",
      "subplot(132)\n",
      "title('Classification')\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n",
      "subplot(133)\n",
      "title('Original distribution')\n",
      "gmm.set_coef(array([1.0,0.0]))\n",
      "gmm.set_features(grid)\n",
      "grid_out=gmm.get_likelihood_for_all_examples()\n",
      "zn=grid_out.reshape((size, size))\n",
      "gmm.set_coef(array([0.0,1.0]))\n",
      "grid_out=gmm.get_likelihood_for_all_examples()\n",
      "zp=grid_out.reshape((size, size))\n",
      "z=zp-zn\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And voila! The SVM decision rule reasonably distinguishes the red from the blue points. Despite being optimized for learning the discriminative function maximizing the margin, the SVM output quality wise remotely resembles the original distribution of the gaussian mixture model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The support vectors (as explained in previous section) are now plotted to give some intuition about which points are included in the predictor for different kernels. The `get_support_vectors()` method is used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_support_vectors(kernels, svm):\n",
      "    figure(figsize=(20,5))\n",
      "    suptitle('Support vectors using different kernels', fontsize=12)\n",
      "    for i in range(len(kernels)):\n",
      "        subplot(1,len(kernels),i+1)\n",
      "        title(kernels[i].get_name())\n",
      "        svm.set_kernel(kernels[i])\n",
      "        svm.train()\n",
      "        vec=svm.get_support_vectors()\n",
      "        X=[]\n",
      "        Y=[]\n",
      "        new_labels=[]\n",
      "        for j in vec:\n",
      "            X.append(traindata[0][j])\n",
      "            Y.append(traindata[1][j])\n",
      "            new_labels.append(trainlab[j])\n",
      "        scatter(X, Y, c=new_labels, s=35)\n",
      "               \n",
      "plot_support_vectors(kernels, svm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_outputs(kernels):\n",
      "    figure(figsize=(20,5))\n",
      "    suptitle('Binary Classification using different kernels', fontsize=12)\n",
      "    for i in range(len(kernels)):\n",
      "        subplot(1,len(kernels),i+1)\n",
      "        title(kernels[i].get_name())\n",
      "        svm.set_kernel(kernels[i])\n",
      "        svm.train()\n",
      "        grid_out=svm.apply(grid)\n",
      "        z=grid_out.get_values().reshape((size, size))\n",
      "        c=pcolor(x, y, z)\n",
      "        contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "        colorbar(c)\n",
      "        scatter(traindata[0,:], traindata[1,:], c=trainlab, s=35)\n",
      "\n",
      "plot_outputs(kernels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Kernel Normalizers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Kernel normalizers post-process kernel values by carrying out normalization in feature space. Kernel Normalization is not strictly-speaking a form of preprocessing since it is not applied directly on the input vectors but can be seen as a kernel interpretation of the preprocessing. The [CKernelNormalizer](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CKernelNormalizer.html) class provides tools for kernel normalization. Some of the kernel normalizers in Shogun:\n",
      "\n",
      "* [SqrtDiagKernelNormalizer](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CSqrtDiagKernelNormalizer.html) : This normalization in the feature space amounts to defining a new kernel $k'({\\bf x},{\\bf x'}) = \\frac{k({\\bf x},{\\bf x'})}{\\sqrt{k({\\bf x},{\\bf x})k({\\bf x'},{\\bf x'})}}$\n",
      "\n",
      "* [AvgDiagKernelNormalizer](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CAvgDiagKernelNormalizer.html) : Scaling with a constant $k({\\bf x},{\\bf x'})= \\frac{1}{c}\\cdot k({\\bf x},{\\bf x'})$\n",
      "\n",
      "* [ZeroMeanCenterKernelNormalizer](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CZeroMeanCenterKernelNormalizer.html) : Centers the kernel in feature space and ensures each feature must have zero mean after centering.\n",
      "\n",
      "The `set_normalizer()` method of [CKernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CKernel.html) is used to add a normalizer.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poly_kernel=PolyKernel()\n",
      "poly_kernel.set_normalizer(SqrtDiagKernelNormalizer())\n",
      "poly_kernel.init(feats_train, feats_train)\n",
      "svm.set_kernel(poly_kernel)\n",
      "_=svm.train()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Multiclass classification "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multiclass classification can be done using SVM by reducing the problem to binary classification. More on multiclass reductions in [this notebook](http://www.shogun-toolbox.org/static/notebook/current/multiclass_reduction.html). [CGMNPSVM](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGMNPSVM.html) class provides a built in [one vs rest multiclass](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CMulticlassOneVsRestStrategy.html) classification using [GMNPlib](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGMNPLib.html). Let us see classification using it on four classes. [CGMM](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGMM.html) class is used to sample the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num=30;\n",
      "num_components=4\n",
      "means=zeros((num_components, 2))\n",
      "means[0]=[-1.5,1.5]\n",
      "means[1]=[1.5,-1.5]\n",
      "means[2]=[-1.5,-1.5]\n",
      "means[3]=[1.5,1.5]\n",
      "\n",
      "covs=array([[1.0,0.0],[0.0,1.0]])\n",
      "\n",
      "gmm=GMM(num_components)\n",
      "[gmm.set_nth_mean(means[i], i) for i in range(num_components)]\n",
      "[gmm.set_nth_cov(covs,i) for i in range(num_components)]\n",
      "gmm.set_coef(array([1.0,0.0,0.0,0.0]))\n",
      "xntr=array([gmm.sample() for i in xrange(num)]).T\n",
      "xnte=array([gmm.sample() for i in xrange(5000)]).T\n",
      "gmm.set_coef(array([0.0,1.0,0.0,0.0]))\n",
      "xntr1=array([gmm.sample() for i in xrange(num)]).T\n",
      "xnte1=array([gmm.sample() for i in xrange(5000)]).T\n",
      "gmm.set_coef(array([0.0,0.0,1.0,0.0]))\n",
      "xptr=array([gmm.sample() for i in xrange(num)]).T\n",
      "xpte=array([gmm.sample() for i in xrange(5000)]).T\n",
      "gmm.set_coef(array([0.0,0.0,0.0,1.0]))\n",
      "xptr1=array([gmm.sample() for i in xrange(num)]).T\n",
      "xpte1=array([gmm.sample() for i in xrange(5000)]).T\n",
      "traindata=concatenate((xntr,xntr1,xptr,xptr1), axis=1)\n",
      "testdata=concatenate((xnte,xnte1,xpte,xpte1), axis=1)\n",
      "\n",
      "l0 = array([0.0 for i in xrange(num)])\n",
      "l1 = array([1.0 for i in xrange(num)])\n",
      "l2 = array([2.0 for i in xrange(num)])\n",
      "l3 = array([3.0 for i in xrange(num)])\n",
      "\n",
      "trainlab=concatenate((l0,l1,l2,l3))\n",
      "testlab=concatenate((l0,l1,l2,l3))\n",
      "\n",
      "title('Toy data for multiclass classification')\n",
      "_=jet()\n",
      "_=scatter(traindata[0,:], traindata[1,:], c=trainlab, s=75)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feats_train=RealFeatures(traindata)\n",
      "labels=MulticlassLabels(trainlab)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us try the multiclass classification for different kernels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gaussian_kernel=GaussianKernel(feats_train, feats_train, 2)\n",
      "poly_kernel=PolyKernel(feats_train, feats_train, 4, True)\n",
      "linear_kernel=LinearKernel(feats_train, feats_train)\n",
      "\n",
      "kernels=[gaussian_kernel, poly_kernel, linear_kernel]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm=GMNPSVM(1, gaussian_kernel, labels)\n",
      "_=svm.train(feats_train)\n",
      "\n",
      "size=100\n",
      "x1=linspace(-6, 6, size)\n",
      "x2=linspace(-6, 6, size)\n",
      "x, y=meshgrid(x1, x2)\n",
      "grid=RealFeatures(array((ravel(x), ravel(y))))\n",
      "def plot_outputs(kernels):\n",
      "    figure(figsize=(20,5))\n",
      "    suptitle('Multiclass Classification using different kernels', fontsize=12)\n",
      "    for i in range(len(kernels)):\n",
      "        subplot(1,len(kernels),i+1)\n",
      "        title(kernels[i].get_name())\n",
      "        svm.set_kernel(kernels[i])\n",
      "        svm.train(feats_train)\n",
      "        grid_out=svm.apply(grid)\n",
      "        z=grid_out.get_labels().reshape((size, size))\n",
      "        c=pcolor(x, y, z)\n",
      "        contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "        colorbar(c)\n",
      "        scatter(traindata[0,:], traindata[1,:], c=trainlab, s=35)\n",
      "\n",
      "plot_outputs(kernels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The distinguishing properties of the kernels are visible in these classification outputs."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}