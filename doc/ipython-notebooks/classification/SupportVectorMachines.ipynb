{
 "metadata": {
  "name": "SupportVectorMachines",
  "signature": "sha256:6db2c80457528b0426120d5e611bae2b85c141567c6f8b32d9ae3dd7addc3332"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classification with Support Vector Machines"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "by Soeren Sonnenburg, Saurabh Mahindre - <a href=\"https://github.com/Saurabh7\">github.com/Saurabh7</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook illustrates how to train a binary <a href=\"http://en.wikipedia.org/wiki/Support_vector_machine\">support vector machines</a> (SVM) <a href=\"http://en.wikipedia.org/wiki/Statistical_classification\">classifier</a> using Shogun. The <a href=\"http://www.shogun-toolbox.org/doc/en/3.0.0/classshogun_1_1CLibSVM.html\">CLibSVM</a> class of shogun is used to train with the <a href=\"http://en.wikipedia.org/wiki/Sampled_Gaussian_kernel#The_sampled_Gaussian_kernel\">sampled Gaussian Kernel</a>. Multiclass classification is also demonstrated using [CGMNPSVM](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGMNPSVM.html)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More formally, we want to learn a decision function $f(x) \\mapsto \\{+1,-1\\}$ based on a number of training examples $(x,y)_{i=0}^{N-1}$ where $x\\in\\mathcal{X}$ and $y\\in\\{-1,+1\\}$. \n",
      "Now a [SVM](https://en.wikipedia.org/wiki/Support_vector_machine) is a binary classifier that tries to separate objects of different classes by finding a (hyper-)plane such that the margin between the two classes is maximized:\n",
      "\n",
      "More formally, a support vector machine is defined as\n",
      " $$f({\\bf x})=sign\\left(\\sum_{i=0}^{N-1} \\alpha_i k({\\bf x}, {\\bf x_i})+b\\right)$$\n",
      " \n",
      " where $N$ is the number of training examples\n",
      " $\\alpha_i$ are the weights assigned to each training example\n",
      " $k(x,x')$ is some kernel function\n",
      " and $b$ the bias.\n",
      " \n",
      " Using an a-priori choosen kernel, the $\\alpha_i$ and bias are determined\n",
      " by solving the following quadratic program\n",
      " \n",
      "  \\begin{eqnarray*}\n",
      "       \\max_{\\bf \\alpha} && \\sum_{i=0}^{N-1} \\alpha_i - \\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1} \\alpha_i y_i \\alpha_j y_j  k({\\bf x_i}, {\\bf x_j})\\\\\n",
      "       \\mbox{s.t.} && 0\\leq\\alpha_i\\leq C\\\\\n",
      "                   && \\sum_{i=0}^{N-1} \\alpha_i y_i=0\\\\\n",
      "\\end{eqnarray*}\n",
      " where C is a pre-specified regularization parameter trading of the size of the margin and the training error.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let us see how one can train a support vector machine with shogun:\n",
      "\n",
      "In a first step we just generate some two-dimensional Gaussians.\n",
      "\n",
      "$x_-\\sim{\\cal N_2}(0,1)-d$\n",
      "\n",
      "$x_+\\sim{\\cal N_2}(0,1)+d$\n",
      "\n",
      "and corresponding positive and negative labels. We create traindata and testdata with ```num``` of them being negatively and positively labelled in traindata,trainlab and testdata, testlab. For that we utilize shoguns gaussian mixture model class (GMM) from which we sample the data points and plot them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import *\n",
      "\n",
      "num=1000;\n",
      "dist=1.0;\n",
      "\n",
      "gmm=GMM(2)\n",
      "gmm.set_nth_mean(array([-dist,-dist]),0)\n",
      "gmm.set_nth_mean(array([dist,dist]),1)\n",
      "gmm.set_nth_cov(array([[1.0,0.0],[0.0,1.0]]),0)\n",
      "gmm.set_nth_cov(array([[1.0,0.0],[0.0,1.0]]),1)\n",
      "\n",
      "gmm.set_coef(array([1.0,0.0]))\n",
      "xntr=array([gmm.sample() for i in xrange(num)]).T\n",
      "xnte=array([gmm.sample() for i in xrange(num)]).T\n",
      "\n",
      "gmm.set_coef(array([0.0,1.0]))\n",
      "xptr=array([gmm.sample() for i in xrange(num)]).T\n",
      "xpte=array([gmm.sample() for i in xrange(num)]).T\n",
      "\n",
      "\n",
      "traindata=concatenate((xntr,xptr), axis=1)\n",
      "testdata=concatenate((xnte,xpte), axis=1)\n",
      "\n",
      "trainlab=concatenate((-ones(num), ones(num)))\n",
      "testlab=concatenate((-ones(num), ones(num)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_=scatter(traindata[0,:], traindata[1,:], c=trainlab, s=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using this data we create shogun features and label objects:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feats_train=RealFeatures(traindata)\n",
      "labels=BinaryLabels(trainlab)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Just for fun we compute the kernel matrix and display it - the nice block structure of the matrix suggests that the data will be nicely separable (examples 0..999 when compared to each other have mostly higher scores than when compared to example 1000...1999 and vice versa)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "width=2\n",
      "kernel=GaussianKernel(feats_train, feats_train, width)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "km=kernel.get_kernel_matrix()\n",
      "_=imshow(km)\n",
      "_=colorbar()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we train an SVM with this GaussianKernel. We use LibSVM but we could use any of the [other SVM](http://www.shogun-toolbox.org/doc/en/current/classshogun_1_1CSVM.html) from shogun. They all utilize the same kernel framework and so are drop-in replacements."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C=1.0\n",
      "svm=LibSVM(C, kernel, labels)\n",
      "_=svm.train()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could now check a number of properties like how many support vectors the trained SVM has, i.e. how many of the $\\alpha_i$ above are non-zero:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print svm.get_num_support_vectors()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "or what the value of the objective function returned by the particular SVM learning algorithm or the explictly computed primal and dual objective function is"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "libsvm_obj=svm.get_objective()\n",
      "primal_obj, dual_obj=svm.compute_svm_primal_objective(), svm.compute_svm_dual_objective()\n",
      "\n",
      "print libsvm_obj, primal_obj, dual_obj"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and based on the objectives we can compute the duality gap, a measure of convergence quality of the svm training algorithm. In theory it is 0 at the optimum and in reality at least close to 0. The"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"duality_gap\", dual_obj-primal_obj"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's now compute the test error"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out=svm.apply(RealFeatures(testdata))\n",
      "\n",
      "evaluator=ErrorRateMeasure()\n",
      "print \"Test error is %2.2f%%\" % (100*evaluator.evaluate(out,BinaryLabels(testlab)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets plot the contour output on a $-5...+5$ grid for \n",
      "\n",
      "1. The Support Vector Machines decision function $\\mbox{sign}(f(x))$\n",
      "2. The Support Vector Machines raw output $f(x)$\n",
      "3. The Original Gaussian Mixture Model Distribution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "size=100\n",
      "x1=linspace(-5, 5, size)\n",
      "x2=linspace(-5, 5, size)\n",
      "x, y=meshgrid(x1, x2)\n",
      "grid=RealFeatures(array((ravel(x), ravel(y))))\n",
      "grid_out=svm.apply(grid)\n",
      "z=grid_out.get_labels().reshape((size, size))\n",
      "\n",
      "figure(figsize=(20,5))\n",
      "subplot(131)\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n",
      "z=grid_out.get_values().reshape((size, size))\n",
      "\n",
      "subplot(132)\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n",
      "subplot(133)\n",
      "gmm.set_coef(array([1.0,0.0]))\n",
      "gmm.set_features(grid)\n",
      "grid_out=gmm.get_likelihood_for_all_examples()\n",
      "zn=grid_out.reshape((size, size))\n",
      "gmm.set_coef(array([0.0,1.0]))\n",
      "grid_out=gmm.get_likelihood_for_all_examples()\n",
      "zp=grid_out.reshape((size, size))\n",
      "z=zp-zn\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And voila! The SVM decision rule reasonably distinguishes the red from the blue points. Despite being optimized for learning the discriminative function maximizing the margin, the SVM output quality wise remotely resembles the original distribution of the gaussian mixture model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Kernels in Shogun"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[CKernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CKernel.html) is the base class for kernels in Shogun. Some commonly used kernels : \n",
      "\n",
      "* [Gaussian kernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGaussianKernel.html) : Popular Gaussian kernel computed as $k({\\bf x},{\\bf x'})= exp(-\\frac{||{\\bf x}-{\\bf x'}||^2}{\\tau})$\n",
      "\n",
      "* [Linear kernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CLinearKernel.html) : Computes $k({\\bf x},{\\bf x'})= {\\bf x}\\cdot {\\bf x'}$\n",
      "* [Polynomial kernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CPolyKernel.html) : Polynomial kernel computed as $k({\\bf x},{\\bf x'})= ({\\bf x}\\cdot {\\bf x'}+c)^d$\n",
      "\n",
      "* [Simgmoid Kernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CSigmoidKernel.html) : Computes $k({\\bf x},{\\bf x'})=\\mbox{tanh}(\\gamma {\\bf x}\\cdot{\\bf x'}+c)$\n",
      "\n",
      "Some of these kernels are initialised and used for binary classification below. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gaussian_kernel=GaussianKernel(feats_train, feats_train, 2)\n",
      "#Polynomial kernel of degree 2\n",
      "poly_kernel=PolyKernel(feats_train, feats_train, 2, True)\n",
      "linear_kernel=LinearKernel(feats_train, feats_train)\n",
      "\n",
      "kernels=[gaussian_kernel, poly_kernel, linear_kernel]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Samples on the margin of separating hyperplane are called the support vectors. These support vectors are now plotted to give some intuition about which points lie on margins for different kernels. The `get_support_vectors()` method is used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C=1.0\n",
      "svm=LibSVM(C, kernel, labels)\n",
      "svm.train()\n",
      "\n",
      "def plot_support_vectors(kernels):\n",
      "    figure(figsize=(20,5))\n",
      "    suptitle('Support vectors using different kernels', fontsize=12)\n",
      "    for i in range(len(kernels)):\n",
      "        subplot(1,len(kernels),i+1)\n",
      "        title(kernels[i].get_name())\n",
      "        svm.set_kernel(kernels[i])\n",
      "        svm.train()\n",
      "        vec=svm.get_support_vectors()\n",
      "        X=[]\n",
      "        Y=[]\n",
      "        new_labels=[]\n",
      "        for j in vec:\n",
      "            X.append(traindata[0][j])\n",
      "            Y.append(traindata[1][j])\n",
      "            new_labels.append(trainlab[j])\n",
      "        scatter(X, Y, c=new_labels, s=35)\n",
      "       \n",
      "plot_support_vectors(kernels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_outputs(kernels):\n",
      "    figure(figsize=(20,5))\n",
      "    suptitle('Binary Classification using different kernels', fontsize=12)\n",
      "    for i in range(len(kernels)):\n",
      "        subplot(1,len(kernels),i+1)\n",
      "        title(kernels[i].get_name())\n",
      "        svm.set_kernel(kernels[i])\n",
      "        svm.train()\n",
      "        grid_out=svm.apply(grid)\n",
      "        z=grid_out.get_values().reshape((size, size))\n",
      "        c=pcolor(x, y, z)\n",
      "        contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "        colorbar(c)\n",
      "        scatter(traindata[0,:], traindata[1,:], c=trainlab, s=35)\n",
      "\n",
      "plot_outputs(kernels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Kernel Normalizers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The [CKernelNormalizer](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CKernelNormalizer.html) class provides tools to post-process kernel values. Normalization in the feature space is not strictly-speaking a form of preprocessing since it is not applied directly on the input vectors but can be seen as a kernel interpretation of the preprocessing. Some of the kernel normalizers in Shogun:\n",
      "\n",
      "* [SqrtDiagKernelNormalizer](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CSqrtDiagKernelNormalizer.html) : This normalization in the feature space amounts to defining a new kernel $k'({\\bf x},{\\bf x'}) = \\frac{k({\\bf x},{\\bf x'})}{\\sqrt{k({\\bf x},{\\bf x})k({\\bf x'},{\\bf x'})}}$\n",
      "\n",
      "* [AvgDiagKernelNormalizer](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CAvgDiagKernelNormalizer.html) : Scaling with a constant $k({\\bf x},{\\bf x'})= \\frac{1}{c}\\cdot k({\\bf x},{\\bf x'})$\n",
      "\n",
      "* [ZeroMeanCenterKernelNormalizer](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CZeroMeanCenterKernelNormalizer.html) : Centers the kernel in feature space and ensures each feature must have zero mean after centering.\n",
      "\n",
      "The `set_normalizer()` method of [CKernel](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CKernel.html) is used to add a normalizer.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poly_kernel=PolyKernel()\n",
      "poly_kernel.set_normalizer(SqrtDiagKernelNormalizer())\n",
      "poly_kernel.init(feats_train, feats_train)\n",
      "svm.set_kernel(poly_kernel)\n",
      "svm.train()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Multiclass classification "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multiclass classification can be done using SVM by reducing the problem to binary classification. More on multiclass reductions in [this notebook](http://www.shogun-toolbox.org/static/notebook/current/multiclass_reduction.html). [CGMNPSVM](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGMNPSVM.html) class provides a built in [one vs rest multiclass](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CMulticlassOneVsRestStrategy.html) classification using [GMNPlib](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGMNPLib.html). Let us see classification using it on four classes. [CGMM](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CGMM.html) class is used to sample the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import *\n",
      "from numpy import *\n",
      "\n",
      "num=1000;\n",
      "dist=2.0;\n",
      "\n",
      "gmm=GMM(4)\n",
      "gmm.set_nth_mean(array([0.0, 0.0]),0)\n",
      "gmm.set_nth_mean(array([-2*dist,dist]),1)\n",
      "gmm.set_nth_mean(array([-2*dist,-dist]),2)\n",
      "gmm.set_nth_mean(array([2*dist,0]),3)\n",
      "gmm.set_nth_cov(array([[1.0,0.0],[0.0,1.0]]),0)\n",
      "gmm.set_nth_cov(array([[1.0,0.0],[0.0,1.0]]),1)\n",
      "gmm.set_nth_cov(array([[1.0,0.0],[0.0,1.0]]),2)\n",
      "gmm.set_nth_cov(array([[1.0,0.0],[0.0,1.0]]),3)\n",
      "\n",
      "gmm.set_coef(array([1.0,0.0,0.0,0.0]))\n",
      "x0=array([gmm.sample() for i in xrange(num)]).T\n",
      "x0t=array([gmm.sample() for i in xrange(num)]).T\n",
      "\n",
      "gmm.set_coef(array([0.0,1.0,0.0,0.0]))\n",
      "x1=array([gmm.sample() for i in xrange(num)]).T\n",
      "x1t=array([gmm.sample() for i in xrange(num)]).T\n",
      "\n",
      "gmm.set_coef(array([0.0,0.0,1.0,0.0]))\n",
      "x2=array([gmm.sample() for i in xrange(num)]).T\n",
      "x2t=array([gmm.sample() for i in xrange(num)]).T\n",
      "\n",
      "gmm.set_coef(array([0.0,0.0,0.0,1.0]))\n",
      "x3=array([gmm.sample() for i in xrange(num)]).T\n",
      "x3t=array([gmm.sample() for i in xrange(num)]).T\n",
      "\n",
      "\n",
      "traindata=concatenate((x0,x1,x2,x3), axis=1)\n",
      "testdata=concatenate((x0t,x1t,x2t,x3t), axis=1)\n",
      "\n",
      "l0 = array([0.0 for i in xrange(num)])\n",
      "l1 = array([1.0 for i in xrange(num)])\n",
      "l2 = array([2.0 for i in xrange(num)])\n",
      "l3 = array([3.0 for i in xrange(num)])\n",
      "\n",
      "trainlab=concatenate((l0,l1,l2,l3))\n",
      "testlab=concatenate((l0,l1,l2,l3))\n",
      "\n",
      "title('Toy data for multiclass classification')\n",
      "_=jet()\n",
      "_=scatter(traindata[0,:], traindata[1,:], c=trainlab, s=100)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feats_train=RealFeatures(traindata)\n",
      "labels=MulticlassLabels(trainlab)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us try the multiclass classification for different kernels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gaussian_kernel=GaussianKernel(feats_train, feats_train, 2)\n",
      "poly_kernel=PolyKernel(feats_train, feats_train, 2, True)\n",
      "linear_kernel=LinearKernel(feats_train, feats_train)\n",
      "\n",
      "kernels=[gaussian_kernel, poly_kernel, linear_kernel]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm=GMNPSVM(1, gaussian_kernel, labels)\n",
      "_=svm.train(feats_train)\n",
      "\n",
      "size=100\n",
      "x1=linspace(-7, 7, size)\n",
      "x2=linspace(-7, 7, size)\n",
      "x, y=meshgrid(x1, x2)\n",
      "grid=RealFeatures(array((ravel(x), ravel(y))))\n",
      "def plot_outputs(kernels):\n",
      "    figure(figsize=(20,5))\n",
      "    suptitle('Multiclass Classification using different kernels', fontsize=12)\n",
      "    for i in range(len(kernels)):\n",
      "        subplot(1,len(kernels),i+1)\n",
      "        title(kernels[i].get_name())\n",
      "        svm.set_kernel(kernels[i])\n",
      "        svm.train(feats_train)\n",
      "        grid_out=svm.apply(grid)\n",
      "        z=grid_out.get_labels().reshape((size, size))\n",
      "        c=pcolor(x, y, z)\n",
      "        contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "        colorbar(c)\n",
      "        scatter(traindata[0,:], traindata[1,:], c=trainlab, s=35)\n",
      "\n",
      "plot_outputs(kernels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The distinguishing properties of the kernels are visible in these classification outputs."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}