{
 "metadata": {
  "name": "",
  "signature": "sha256:29b9b887f681ce322d9571a92170f0bd3dc1dd20314282731b1a8648513f5abf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Active Appearance Models"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "By Abhijeet Kislay (GitHub ID: <a href='https://github.com/kislayabhi'>kislayabhi</a>)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we have developed a parameterized face needed by the Active Appearance Model (<a href=\"http://en.wikipedia.org/wiki/Active_appearance_model\">AAM</a>), where the variability of shape and texture is captured from a representative training set. <a href=\"http://en.wikipedia.org/wiki/Principal_component_analysis\">PCA</a> on shape and texture is applied to produce the face model which describes the trained faces with a photorealistic quality."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Background"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "AAM is an object model containing statistical information of its shape and texture. It is a powerful way of capturing shape and texture variation from objects. It comes along with an efficient search algorithm that can tell exactly where and how a model is located in a picture frame.\n",
      "\n",
      "The two models needed to make an AAM are:\n",
      "* Shape Models\n",
      "* Texture Models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#following libraries will be used here\n",
      "\n",
      "#import Opencv library\n",
      "try:\n",
      "    import cv2\n",
      "except ImportError:\n",
      "    print \"You must have OpenCV installed\"\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from modshogun import *\n",
      "%matplotlib inline \n",
      "\n",
      "#Use the following function when reading an image through OpenCV and displaying through plt.\n",
      "def showfig(image, ucmap):\n",
      "    #There is a difference in pixel ordering in OpenCV and Matplotlib.\n",
      "    #OpenCV follows BGR order, while matplotlib follows RGB order.\n",
      "    if len(image.shape)==3 :\n",
      "        b,g,r = cv2.split(image)       # get b,g,r\n",
      "        image = cv2.merge([r,g,b])     # switch it to rgb\n",
      "    imgplot=plt.imshow(image, ucmap)\n",
      "    imgplot.axes.get_xaxis().set_visible(False)\n",
      "    imgplot.axes.get_yaxis().set_visible(False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Shape Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As mentioned previously, AAMs require a shape model, and this role is played by Active Shape Models (ASMs). In the following section, we have described the method to build it.\n",
      "\n",
      "The shape model is generated through the combination of shape variations and for that a training set of annotated images is required. That is we need to have several images marked with points on key positions of a face which outlines the main features.\n",
      "\n",
      "Few of the training images with the landmark points are shown below. There are 68 landmarks on a face. These are usually marked up by hand and they outline several face features, such as mouth contour, nose, eyes, eyebrows, and face shape since they are easier to track:\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.rcParams['figure.figsize'] = 17, 7\n",
      "\n",
      "fig=plt.figure()\n",
      "for i in xrange(1,4):\n",
      "    image_path=\"../../../data/AAM/images/%d.jpg\"%i\n",
      "    tim_cootes=cv2.imread(image_path)\n",
      "    axes=fig.add_subplot(1,3,i)\n",
      "    data=np.loadtxt('../../../data/AAM/points/%d.pts'%i, usecols=range(2))\n",
      "    x=data[:,0]\n",
      "    y=data[:,1]\n",
      "    axes.plot(x,y,'*')\n",
      "    showfig(tim_cootes, None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The procedures to build an ASM are as follows:\n",
      "\n",
      "1. Aligning shapes to a common frame using Generalized Procrustes Analysis\n",
      "2. Modeling the dataset shape variation using PCA"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Generalized Procrustes Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The alignment of two shapes consists on finding the Similarity parameters (scale, rotation and translation) that best match one shape to another by minimizing a given metric. The classical solution of align two shapes is the Procrustes Analysis method. It align shapes with the same number of landmarks with one-to-one point correspondences, which is sufficient for the AAM standard formulation.\n",
      "\n",
      "The landmark points that we are provided right now are not aligned in a way that helps us to build an ASM. This can be seen in the following figure:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.rcParams['figure.figsize'] = 7, 7 \n",
      "figure, axis=plt.subplots(1,1)\n",
      "plt.xlim(0, 480)\n",
      "plt.ylim(0, 480)\n",
      "plt.title(\"The landmark points for all the face images in the training set\")\n",
      "for i in xrange(1,26):\n",
      "    Y=np.loadtxt('../../../data/AAM/points/%d.pts'%i, usecols=range(2))\n",
      "    axis.plot(640-Y[:,0], 480-Y[:,1],'+', color='blue', markersize=5)\n",
      "    axis.set_xticks([])\n",
      "    axis.set_yticks([])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A Generalized Procrustes Analysis(GPA) consists of sequentially aligning pairs of shapes with Procrustes using the reference shape (the mean shape) and align others to it. \n",
      "\n",
      "To facilitate this iterative process, we define a procrustes function below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def procrustes(X, Y):\n",
      "    \"\"\"\n",
      "    Inputs:\n",
      "    ------------\n",
      "    X, Y    \n",
      "        matrices of target and input coordinates. they must have equal\n",
      "        numbers of  points (rows)\n",
      "\n",
      "    Outputs\n",
      "    ------------\n",
      "    Z\n",
      "        the matrix of transformed Y-values\n",
      "    \"\"\"\n",
      "\n",
      "    n,m = X.shape\n",
      "    ny,my = Y.shape\n",
      "\n",
      "    muX = X.mean(0)\n",
      "    muY = Y.mean(0)\n",
      "\n",
      "    X0 = X - muX\n",
      "    Y0 = Y - muY\n",
      "\n",
      "    ssX = (X0**2.).sum()\n",
      "    ssY = (Y0**2.).sum()\n",
      "\n",
      "    # centred Frobenius norm\n",
      "    normX = np.sqrt(ssX)\n",
      "    normY = np.sqrt(ssY)\n",
      "\n",
      "    # scale to equal (unit) norm\n",
      "    X0 /= normX\n",
      "    Y0 /= normY\n",
      "\n",
      "    # optimum rotation matrix of Y\n",
      "    A = np.dot(X0.T, Y0)\n",
      "    U,s,Vt = np.linalg.svd(A,full_matrices=False)\n",
      "    V = Vt.T\n",
      "    T = np.dot(V, U.T)\n",
      "\n",
      "    traceTA = s.sum()\n",
      "\n",
      "    # transformed coords\n",
      "    Z = normX*traceTA*np.dot(Y0, T) + muX\n",
      "\n",
      "    return Z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At the beginning any shape can be chosen to be the initial mean. After the alignment a new estimate for the mean is recomputed and again the shapes are aligned to this mean. The procedure is performed repeatedly until the mean shape don't change significantly within iterations. Normally this procedure converges in two iterations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ****GPA*****\n",
      "# choose any shape as the intial mean\n",
      "X=np.loadtxt('1.pts', usecols=range(2))\n",
      "\n",
      "# two iterations are enough \n",
      "for j in xrange(2):\n",
      "    Y_new=[]\n",
      "    \n",
      "    #our training set has 25 images numbered 1,2,..,24,25\n",
      "    for i in xrange(1,26):\n",
      "        Y=np.loadtxt('%d.pts'%i, usecols=range(2))\n",
      "        z=procrustes(X, Y)\n",
      "        Y_new.append(z) \n",
      "    #recompute the mean\n",
      "    X=sum(Y_new)/float(len(Y_new)) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets check the output after applying the GPA over the landmark points that we had:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.rcParams['figure.figsize'] = 7, 7 \n",
      "Y_new[0].shape\n",
      "figure, axis=plt.subplots(1,1)\n",
      "plt.title(\"The GPA processed landmark points\")\n",
      "plt.xlim(0, 480)\n",
      "plt.ylim(0, 480)\n",
      "for i in xrange(25):    \n",
      "    axis.plot(640-Y_new[i][:,0], 480-Y_new[i][:,1],'+', color='blue', markersize=5)\n",
      "    axis.set_xticks([])\n",
      "    axis.set_yticks([])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After aligning these landmarks using GPA, we will arrange them in the following format where the $k$ aligned landmarks in the two dimensions are given as:\n",
      "\n",
      "$X = (x_1,y_1,...,x_k,y_k)$\n",
      "\n",
      "It is important to note that each landmark is consistently labelled across all the training images. So, for instance, if the left part of the mouth is landmark number 3 in the first image, it will need to be number 3 in all the other images."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Principal Component Analysis (PCA)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The PCA is a statistical technique that allows data dimension reduction. This process searches for directions in the data that has largest variance and subsequently project the data onto it. For more details checkout this <a href=\"http://www.shogun-toolbox.org/static/notebook/current/pca_notebook.html\">notebook</a> on PCA.\n",
      "\n",
      "Applying PCA on the previously aligned data, the statistical shape variation can be modeled with:\n",
      "\n",
      "$\\mathbf{x}=\\mathbf{\\bar{x}}+\\mathbf{E_s}\\mathbf{y_s}$\n",
      "\n",
      "where new shapes $\\mathbf{x}$, are synthesised by deforming the mean shape, $\\mathbf{\\bar{x}}$, using a weighted linear combination of eigenvectors of the covariance matrix, $\\mathbf{E_s}$. $\\mathbf{y_s}$ is a vector of shape parameters which represent the weights. $\\mathbf{E_s}$ holds $t_s$ most important eigenvectors that explain a user defined variance. It is possible to recover the shape parameters associated with each shape by\n",
      "\n",
      "$\\mathbf{y_s}=\\mathbf{E_s}^T(\\mathbf{x}-\\mathbf{\\bar{x}})$\n",
      "\n",
      "where $\\mathbf{y_s}$ vector defines a set of deformable model parameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "obs_matrix=[]\n",
      "\n",
      "#arrange the data in the format (x1,y1,...,xk,yk)\n",
      "for i in xrange(25):\n",
      "    obs_matrix.append(np.hstack(Y_new[i]))\n",
      "\n",
      "#prepare the observation matrix for PCA    \n",
      "obs_matrix=np.array(obs_matrix).T\n",
      "\n",
      "#convert it into Shogun RealFeatures format\n",
      "train_features=RealFeatures(np.array(obs_matrix))\n",
      "\n",
      "#set the PCA mode to AUTO\n",
      "preprocessor=PCA(AUTO)\n",
      "\n",
      "#set the number of eigenvectors to hold onto, here three\n",
      "preprocessor.set_target_dim(24)\n",
      "\n",
      "#all set. Run it\n",
      "preprocessor.init(train_features)\n",
      "mean=preprocessor.get_mean()\n",
      "\n",
      "#get the eigenvectors of the covariance marix\n",
      "Es=preprocessor.get_transformation_matrix()\n",
      "#get the eigenvalues of the covariance matrix\n",
      "eigenvalues_s=preprocessor.get_eigenvalues()\n",
      "\n",
      "#project the data to their principal components\n",
      "ys=preprocessor.apply_to_feature_matrix(train_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The variance of the parameter $y_{si}: i = 1,..., ts$ over the training set is given by $\u03bb_i$. Limiting $y_{si}$ between $\u00b13 \\sqrt{\u03bb_i} = \u00b13\u03c3_i$ is safe that the shape generated is\n",
      "similar to the ones in the training set.\n",
      "\n",
      "The following images shows the first three shape parameters varied between $[-3\\sigma_i, +3\\sigma_i]$. The first variation mode, as expected displays more information associated, causing a bigger movement between the landmark position. The lower significatives modes cause a more local variation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.rcParams['figure.figsize'] = 17,5\n",
      "\n",
      "shape_container=[]\n",
      "\n",
      "for parameter_no in xrange(3):\n",
      "    std_dev=np.int(eigenvalues_s[parameter_no]**0.5)\n",
      "    figure=plt.figure()\n",
      "    deviation_data=np.array([-3*std_dev, -1.5*std_dev, 0, 1.5*std_dev, 3*std_dev])\n",
      "    title_data=np.array(['-3','-1.5','0','+1.5','+3'])\n",
      "    plot_no=0\n",
      "    for deviation in deviation_data:\n",
      "        \n",
      "        plot_no=plot_no+1\n",
      "        new_ys=np.copy(ys)\n",
      "        \n",
      "        #vary one of the parameter\n",
      "        new_ys[parameter_no,1]=deviation\n",
      "        \n",
      "        # reconstruct the shape landmarks,but with the distorted ys\n",
      "        reconstructed_image=np.hstack(np.dot(Es,new_ys[:,1]))+mean   \n",
      "        image=np.resize(reconstructed_image,[68,2])\n",
      "        shape_container.append(image)\n",
      "        \n",
      "        axis=figure.add_subplot(1,5,plot_no)\n",
      "        axis.plot(600-image[:,0], 600-image[:,1])\n",
      "        plt.title((title_data[plot_no-1])+' $\\sigma %d$'%parameter_no)\n",
      "        axis.set_xticks([])\n",
      "        axis.set_yticks([])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Texture Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The texture is defined as the pixel intensities over the modeled entity. Here we have described the procedures to build a statistical texture model. \n",
      "\n",
      "* Similarly to the shape model, where all the shapes are previously aligned into a common frame, the texture model requires the alignment of all texture samples to a reference texture frame also.\n",
      "\n",
      "\n",
      "* The texture is mapped in a way that the control points from each sample match the control points of a suitable reference frame, the mean shape.\n",
      "\n",
      "\n",
      "* <a href=\"http://en.wikipedia.org/wiki/Delaunay_triangulation\">Delaunay triangulation</a> is used in the mean shape control points to establish triangles that will be used to map pixel intensities by Piece-wise Affine Warping. \n",
      "\n",
      "\n",
      "* A statistical texture model is build using Principal Components Analysis, describing the texture in a condensed model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Texture Mapping - Warping"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The texture mapping is performed by using a piece-wise affine warp, i.e. partitioning the convex hull of the mean shape by a set of triangles using the Delaunay triangulation.\n",
      "\n",
      "In mathematics and computational geometry, a Delaunay triangulation for a set P of points in a plane is a triangulation such that no point in P is inside the circumcircle of any triangle formed. Delaunay triangulation maximize the minimum angle of all the triangles in the triangulation. They tend to avoid skinny triangles.  \n",
      "\n",
      "Below we have shown the Delaunay triangulation on the set of training face images with their annotated landmarks.\n",
      "The concept is very simple. We will create triangles including our annotated points and then map from one triangle to another."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.delaunay as md\n",
      "plt.rcParams['figure.figsize'] = 17,8\n",
      "fig=plt.figure()\n",
      "\n",
      "for i in reversed(xrange(1,7)):\n",
      "    image_path=\"../../../data/AAM/images/%d.jpg\"%i      \n",
      "    tim_cootes=cv2.imread(image_path)\n",
      "    tim_cootes=cv2.cvtColor(tim_cootes, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "    dest_data=np.loadtxt('../../../data/AAM/points/%d.pts'%i, usecols=range(2))\n",
      "    dest_x=dest_data[:,0]\n",
      "    dest_y=dest_data[:,1]\n",
      "    _,_,dest_triangles,_=md.delaunay(dest_x, dest_y)\n",
      "    axes=fig.add_subplot(2,3,i)\n",
      "    for t in dest_triangles:\n",
      "        t_ext=[t[0], t[1], t[2], t[0]] \n",
      "        axes.plot(dest_x[t_ext], dest_y[t_ext],'r')\n",
      "\n",
      "    axes.plot(dest_x,dest_y,'*')\n",
      "    showfig(tim_cootes, plt.get_cmap('gray'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A piecewise Affine Warp is the texture mapping procedure where each pixel from the image to sample, belonging to a specific triangle, is mapped into the respective destination triangle in the mean shape frame.\n",
      "\n",
      "The following shows the result of warping all the mapped triangles in the left image to a mean reference frame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mask_container=[]\n",
      "\n",
      "plt.rcParams['figure.figsize'] = 15,17\n",
      "fig=plt.figure()\n",
      "plot_no=1\n",
      "for n in xrange(1,26):\n",
      "    \n",
      "    image_path=\"../../../data/AAM/images/%d.jpg\"%n\n",
      "    tim_cootes=cv2.imread(image_path)\n",
      "    tim_cootes=cv2.cvtColor(tim_cootes, cv2.COLOR_BGR2GRAY)\n",
      "    \n",
      "    axes=fig.add_subplot(9,6,plot_no)\n",
      "    showfig(tim_cootes,plt.get_cmap('gray'))\n",
      "\n",
      "    s_data=np.loadtxt('../../../data/AAM/points/%d.pts'%n, usecols=range(2))\n",
      "    s_x=s_data[:,0]\n",
      "    s_y=s_data[:,1]\n",
      "    warp_final=np.uint8(np.zeros(tim_cootes.shape))\n",
      "    \n",
      "    for i in xrange(dest_triangles.shape[0]):\n",
      "\n",
      "        # source is the co-ordinates of the triangle from the current training image\n",
      "        source = s_data[dest_triangles[i]]\n",
      "        \n",
      "        # destination is the co-ordinates of the triangle from the mean reference image\n",
      "        destination = X[dest_triangles[i]]\n",
      "        \n",
      "        #calculate an affine transform from three pairs of the corresponding points.\n",
      "        #a 2x3 affine transform matrix is returned.\n",
      "        warp_mat=cv2.getAffineTransform(np.float32(source), np.float32(destination))\n",
      "        \n",
      "        #applies an affine transformation on the training image\n",
      "        output_warp=cv2.warpAffine(tim_cootes, warp_mat, (640,480))\n",
      "        \n",
      "        #we only need the current triangle from the output_warp\n",
      "        warp_mask=np.uint8(np.zeros(tim_cootes.shape))\n",
      "        destination=np.int32(destination)\n",
      "        #fill the warp_mask with white color in the region surrounded by the destination triangle \n",
      "        cv2.fillConvexPoly(warp_mask, destination, 255) \n",
      "        #copy that filled area from the output_warp and paste it in the warp_final\n",
      "        bool_index=(warp_mask==255)\n",
      "        warp_final[bool_index]=output_warp[bool_index]\n",
      "\n",
      "    axes=fig.add_subplot(9,6, plot_no+1)\n",
      "    showfig(warp_final,plt.get_cmap('gray'))\n",
      "    plot_no=plot_no+2\n",
      "    mask_container.append(warp_final)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apply PCA the same way we did previously with the shape models "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in xrange(25):   \n",
      "    a=mask_container[i]\n",
      "    mask_container[i]=a.flatten()\n",
      "mask_vstack=np.double(np.vstack(mask_container))\n",
      "\n",
      "#form the observation matrix    \n",
      "obs_matrix=mask_vstack.T\n",
      "\n",
      "#apply PCA just the way we did in shape models\n",
      "train_features = RealFeatures(obs_matrix)\n",
      "preprocessor=PCA(AUTO)\n",
      "preprocessor.set_target_dim(24)\n",
      "preprocessor.init(train_features)\n",
      "mean=preprocessor.get_mean()\n",
      "Es=preprocessor.get_transformation_matrix()\n",
      "eigenvalues_g=preprocessor.get_eigenvalues()\n",
      "yg=preprocessor.apply_to_feature_matrix(train_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we again vary the PCA weights just like previously did with the shape models. We see subtle variations in texture appearing in the face images."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.rcParams['figure.figsize'] = 17,5\n",
      "texture_container=[]\n",
      "\n",
      "for parameter_no in xrange(3):\n",
      "    std_dev=np.int(eigenvalues_g[parameter_no]**0.5)\n",
      "    figure=plt.figure()\n",
      "    deviation_data=np.array([-3*std_dev, -1.5*std_dev, 0, 1.5*std_dev, 3*std_dev])\n",
      "    title_data=np.array(['-3','-1.5','0','+1.5','+3'])\n",
      "    plot_no=0\n",
      "    for deviation in deviation_data:\n",
      "        plot_no=plot_no+1\n",
      "        new_yg=np.copy(yg)\n",
      "        new_yg[parameter_no,4]=deviation\n",
      "        reconstructed_image=np.hstack(np.dot(Es, new_yg[:,4]))+mean    \n",
      "        image=np.resize(reconstructed_image, [480,640])\n",
      "        texture_container.append(image)\n",
      "        axis=figure.add_subplot(1,5,plot_no)\n",
      "        plt.title((title_data[plot_no-1])+' $\\sigma %d$'%parameter_no)\n",
      "        showfig(image, plt.get_cmap('gray'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Combining the two models together"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Building an AAM instance together is based on the combined statistical model of shape and texture. An AAM instance is built by generating the texture in the mean reference frame and warping it to the control points given by the different shape models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.rcParams['figure.figsize'] = 17,7\n",
      "fig=plt.figure()\n",
      "for plot_no in xrange(15):\n",
      "    \n",
      "    texture=texture_container[5*int(np.floor(plot_no/5))]\n",
      "    dest_image_points=shape_container[plot_no%5]\n",
      "\n",
      "    dest_image_x=dest_image_points[:,0]\n",
      "    dest_image_y=dest_image_points[:,1]\n",
      "    _,_,new_dest_triangles,_=md.delaunay(dest_image_x, dest_image_y)\n",
      "    \n",
      "    axes=fig.add_subplot(3,5,plot_no+1)\n",
      "    for t in new_dest_triangles:\n",
      "        t_ext=[t[0], t[1], t[2], t[0]] \n",
      "    \n",
      "    warp_final=np.uint8(np.zeros(texture.shape))\n",
      "    \n",
      "    for i in xrange(new_dest_triangles.shape[0]):\n",
      "\n",
      "        # source is the co-ordinates of the triangle from the current training image.\n",
      "        # since the current training image is bounded by the mean reference frame, we use X\n",
      "        source = X[new_dest_triangles[i]]\n",
      "\n",
      "        # destination is the co-ordinates of the triangle from the current choosen shape model\n",
      "        destination = dest_image_points[new_dest_triangles[i]]\n",
      "\n",
      "        # calculate an affine transform from three pairs of the corresponding points.\n",
      "        # a 2x3 affine transform matrix is returned.\n",
      "        warp_mat=cv2.getAffineTransform(np.float32(source), np.float32(destination))\n",
      "\n",
      "        # applies an affine transformation on the current texture image\n",
      "        output_warp=cv2.warpAffine(texture, warp_mat, (640,480))\n",
      "\n",
      "        # we only need the current triangle from the output_warp\n",
      "        warp_mask=np.uint8(np.zeros(texture.shape))\n",
      "        destination=np.int32(destination)\n",
      "        # fill the warp_mask with white color in the region surrounded by the destination triangle \n",
      "        cv2.fillConvexPoly(warp_mask, destination, 255) \n",
      "        # copy that filled area from the output_warp and paste it in the warp_final\n",
      "        bool_index=(warp_mask==255)\n",
      "        warp_final[bool_index]=output_warp[bool_index]\n",
      "\n",
      "    showfig(warp_final,plt.get_cmap('gray'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "References:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[1]. Active Appearance Models, T.F. Cootes, G. J. Edwards, and C. J. Taylor, ECCV, 2:484\u2013498, 1998\n",
      "\n",
      "[2]. Active Shape Models \u2013 Their Training and Application, T.F. Cootes, C.J. Taylor, D.H. Cooper, and J. Graham, Computer Vision and Image Understanding, (61):38\u201359, 1995\n",
      "\n",
      "[3]. Active Appearance Models for Facial Expression Recognition and Monocular Head Pose Estimation Master Thesis, P. Martins, 2008\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}