{
 "metadata": {
  "name": "Introduction"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Machine Learning with Shogun"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "By Saurabh Mahindre - <a href=\"https://github.com/Saurabh7\">github.com/Saurabh7</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we will see how a general machine learning problem is represented and solved in Shogun. As a primer to Shoguns's many capabilities, we will see how various types of data and its attributes are handled and also how prediction is done. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Machine learning concerns the construction and study of systems that can learn from data. A major part of Machine Learning is exploiting structure in existing data. That is to predict certain attributes on yet unseen data from the same or a similar generative process. The prediction can be of different types:    [Supervised](http://en.wikipedia.org/wiki/Supervised_learning), [Unsupervised](http://en.wikipedia.org/wiki/Unsupervised_learning), etc. Shogun provides a host of functionality to do most of these. To get off the mark, let us see how shogun handles the attributes of the data using [Features](http://www.shogun-toolbox.org/doc/en/current/classshogun_1_1CFeatures.html). A feature is generally an individual measurable property of some data being observed."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Feature representation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Shogun supports wide range of feature representations. Among these are [String features](http://www.shogun-toolbox.org/doc/en/current/classshogun_1_1CStringFeatures.html), [Dense features](http://www.shogun-toolbox.org/doc/en/current/classshogun_1_1CDenseFeatures.html), [Sparse features](http://www.shogun-toolbox.org/doc/en/current/classshogun_1_1CSparseFeatures.html), etc. To start with let's see how we can define some easy data using float type values."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us consider an example, instead of just abstract concepts. We have a dataset about various attributes of individuals and we know whether or not they are diabetic. Now this is a classic machine learning problem.The Data reveals certain configurations of attributes that correspond to diabetic patients and others that correspond to non-diabetic patients. When given a set of attributes for a new patient, the goal is to predict whether the patient is diabetic or not. This type of learning problem falls under [Supervised learning](http://en.wikipedia.org/wiki/Supervised_learning)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case it is interesting to play with two attributes, Plasma glucose concentration and Body Mass Index (BMI) and try to learn something about their relationship with the disease. In the first example, random values from a [Gaussian distribution](http://en.wikipedia.org/wiki/Normal_distribution) are used as data but the later section includes prediction on a real world dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#To import all shogun classes\n",
      "from modshogun import *\n",
      "\n",
      "#Generate some random data\n",
      "X = 2 * random.randn(20,2)\n",
      "traindata=r_[X + 5, X + 8].T\n",
      "print traindata"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a matrix with 2 rows. It should be noted that each data sample is stored in a column-major fashion, meaning each column here corresponds to an individual and each row in it to an atribute like BMI, Glucose concentration etc. Let's call this the feature matrix.</br> To convert these features in shogun format `RealFeatures` are used which are nothing but the above mentioned [Dense features](http://www.shogun-toolbox.org/doc/en/current/classshogun_1_1CDenseFeatures.html) of `64bit Float` type. To do this call `RealFeatures` with the feature matrix as the argument. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feats_train=RealFeatures(traindata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Assigning labels"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To be able to differentiate between data one has to label them. Shogun provides various types of labels to do this through [Clabels](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CLabels.html).</br> In this particular problem, our data can be of two types: diabetic or non-diabetic, so we need [binary labels](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CBinaryLabels.html). This makes it a [Binary Classification problem](http://en.wikipedia.org/wiki/Binary_classification), where the data has to be classified in two groups."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create array of labels with 1 and -1\n",
      "trainlab=concatenate((ones(20),-ones(20)))\n",
      "print trainlab\n",
      "#convert to shogun format labels\n",
      "labels=BinaryLabels(trainlab)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Prediction with machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Shogun provides tools for classification, regression, etc. through [CMachine](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CMachine.html). Basically one has to $\\it train$  the machine on some training data to be able to learn from it. Then we $\\it apply$ it to test data to get predictions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the training data\n",
      "figure(figsize=(6,4))\n",
      "gray()\n",
      "_=scatter(traindata[0, :], traindata[1,:], c=labels, s=50)\n",
      "title(\"Training Data\")\n",
      "gray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Its is easy to intuitively understand how the data can be separated in the plot. Now let us see if our classifer is up to the task. It is unlikely we will get such an easy separation with real data though.</br> Moving on to the prediction part, [Liblinear](http://www.shogun-toolbox.org/doc/en/current/classshogun_1_1CLibLinear.html), a linear SVM is used to do the classification (more on SVMs in [this notebook](http://www.shogun-toolbox.org/static/notebook/current/SupportVectorMachines.html))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#prameters to svm\n",
      "C=0.9\n",
      "epsilon=1e-3\n",
      "\n",
      "svm=LibLinear(C, feats_train, labels)\n",
      "svm.set_liblinear_solver_type(L2R_L2LOSS_SVC)\n",
      "svm.set_epsilon(epsilon)\n",
      "\n",
      "#train\n",
      "svm.train()\n",
      "\n",
      "size=100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will now `apply` on test features to get predictions. The whole XY grid is used as test data, i.e data to predict on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "x1=linspace(0, 15, size)\n",
      "x2=linspace(0, 15, size)\n",
      "x, y=meshgrid(x1, x2)\n",
      "#Generate X-Y grid test data\n",
      "grid=RealFeatures(array((ravel(x), ravel(y))))\n",
      "\n",
      "#apply on test grid\n",
      "predictions = svm.apply(grid)\n",
      "\n",
      "z=predictions.get_values().reshape((size, size))\n",
      "\n",
      "#plot\n",
      "jet()\n",
      "figure(figsize=(8,6))\n",
      "title(\"Classification\")\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n",
      "gray()\n",
      "_=scatter(traindata[0, :], traindata[1,:], c=labels, s=50)\n",
      "gray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A nice boundary is predicted to classify the data and this is surely close to what our human machine reasoned!  To play with this interactively have a look at this: [web demo](http://demos.shogun-toolbox.org/classifier/binary/) "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Using datasets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Shogun provides the capability to load datasets of different formats using [CFile](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CFile.html).</br> A real world dataset regarding the previous example:[Pima Indians Diabetes data set](http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) is used now. We load the `LibSVM` format file using shogun's [LibSVMFile](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CLibSVMFile.html) class. Since `LibSVM` format files have labels included in the file, one can get them with `load_with_labels`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f=SparseRealFeatures()\n",
      "#Load the file and generate labels.\n",
      "trainlab=f.load_with_labels(LibSVMFile('../../../data/toy/diabetes_scale.svm'))\n",
      "labels=BinaryLabels(trainlab)\n",
      "#Get the feature matrix\n",
      "mat=f.get_full_feature_matrix()\n",
      "feats=array(mat[1])\n",
      "feats=vstack((feats, array(mat[5])))\n",
      "print feats, feats.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we get hold of the feature matrix, vectors 1 and 5 are extracted. These are the attributes we are interested in: Plasma glucose concentration and Body Mass Index (BMI). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#convert to shogun format\n",
      "feats_train=RealFeatures(feats)\n",
      "#plot the training data\n",
      "figure(figsize=(6,5))\n",
      "_=scatter(feats[0, :], feats[1,:], c=trainlab, s=50)\n",
      "_=xlabel('Plasma glucose concentration')\n",
      "_=ylabel('Body mass index')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What follows next is the now familiar routine of training and applying, similar to the previous section. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C=0.9\n",
      "epsilon=1e-3\n",
      "\n",
      "svm=LibLinear(C, feats_train, labels)\n",
      "svm.set_liblinear_solver_type(L2R_L2LOSS_SVC)\n",
      "svm.set_epsilon(epsilon)\n",
      "\n",
      "#train\n",
      "svm.train()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "size=100\n",
      "x1=linspace(-1.2, 1.2, size)\n",
      "x2=linspace(-1.2, 1.2, size)\n",
      "x, y=meshgrid(x1, x2)\n",
      "#Generate X-Y grid test data\n",
      "grid=RealFeatures(array((ravel(x), ravel(y))))\n",
      "\n",
      "#apply on test grid\n",
      "predictions = svm.apply(grid)\n",
      "\n",
      "z=predictions.get_values().reshape((size, size))\n",
      "\n",
      "#plot\n",
      "jet()\n",
      "figure(figsize=(8,6))\n",
      "title(\"Classification\")\n",
      "c=pcolor(x, y, z)\n",
      "_=contour(x, y, z, linewidths=1, colors='black', hold=True)\n",
      "_=colorbar(c)\n",
      "\n",
      "gray()\n",
      "_=scatter(feats[0, :], feats[1,:], c=trainlab, s=50)\n",
      "_=xlabel('Plasma glucose concentration')\n",
      "_=ylabel('Body mass index')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This seems like a decent enough prediction. We can thus infer that individuals below a ceratin level of BMI and glucose are most certainly safe. To have more strict boundaries explore many more of shogun's classifiers including [Kernel machines](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CKernelMachine.html), etc."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Evaluating performance and Model selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How do you assess the quality of a prediction? Shogun provides various ways to do this using [CEvaluation](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CEvaluation.html). To keep things simple let us split the dataset, train on one part and evaluate performance on other using [ROCEvaluation](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CROCEvaluation.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#split features for training and evaluation\n",
      "feats=array(mat[1])\n",
      "feats_t=feats[0:700]\n",
      "feats_e=feats[700:785]\n",
      "feats=array(mat[5])\n",
      "feats_t1=feats[0:700]\n",
      "feats_e1=feats[700:785]\n",
      "feats_t=vstack((feats_t, feats_t1))\n",
      "feats_e=vstack((feats_e, feats_e1))\n",
      "\n",
      "feats_train=RealFeatures(feats_t)\n",
      "feats_evaluate=RealFeatures(feats_e)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's see the accuracy by applying on test features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "label_t=trainlab[0:700]\n",
      "labels=BinaryLabels(label_t)\n",
      "label_e=trainlab[700:785]\n",
      "labels_true=BinaryLabels(label_e)\n",
      "\n",
      "svm=LibLinear(C, feats_train, labels)\n",
      "svm.set_liblinear_solver_type(L2R_L2LOSS_SVC)\n",
      "svm.set_epsilon(epsilon)\n",
      "\n",
      "#train and evaluate\n",
      "svm.train()\n",
      "output=svm.apply(feats_evaluate)\n",
      "\n",
      "#use ROCEvaluation to get accuracy\n",
      "evaluator=ROCEvaluation()\n",
      "print 'Accuracy(%):'\n",
      "print evaluator.evaluate(output,labels_true)*100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To evaluate more efficiently [cross-validation](http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29) is used. As you might have wondered how are the parameters of the classifier selected? Shogun has a model selection framework to select the best parameters. More description of these things in [this notebook](http://www.shogun-toolbox.org/static/notebook/current/xval_modelselection.html)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "More predictions: Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section will demonstrate another type of machine learning problem on real world data.</br> The task is to estimate price of houses in Boston using the [Housing Dataset](https://archive.ics.uci.edu/ml/datasets/Housing) provided by StatLib library. The attributes are : Weighted distances to employment centres and % lower status of the population. Let us see if we can predict a good relationship between the pricing of houses and the attributes. This type of problems are solved using [Regression analysis](http://en.wikipedia.org/wiki/Regression_analysis)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f=SparseRealFeatures()\n",
      "#Load the file and generate labels.\n",
      "trainlab=f.load_with_labels(LibSVMFile('../../../data/toy/housing_scale.svm'))\n",
      "labels=RegressionLabels(trainlab)\n",
      "#Get the feature matrix\n",
      "mat=f.get_full_feature_matrix()\n",
      "feats=array(mat[7])\n",
      "feats=vstack((feats, array(mat[12])))\n",
      "print feats, feats.shape\n",
      "#convert to shogun format features\n",
      "feats_train=RealFeatures(feats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data set is now loaded and the attributes required (7th and 12th vector) have been converted to Shogun format features."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The tool we will use here to perform regression is [Kernel ridge regression](http://shogun-toolbox.org/doc/en/latest/classshogun_1_1CKernelRidgeRegression.html). Again we train on the data and apply on the XY grid to get predicitions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "size=100\n",
      "x1=linspace(-1.0, 1.0, size)\n",
      "x2=linspace(-1.0, 1.0, size)\n",
      "x, y=meshgrid(x1, x2)\n",
      "#Generate X-Y grid test data\n",
      "grid=RealFeatures(array((ravel(x), ravel(y))))\n",
      "\n",
      "#Train on data(both attributes) and predict\n",
      "width=1.0\n",
      "tau=0.5\n",
      "kernel=GaussianKernel(feats_train, feats_train, width)\n",
      "krr=KernelRidgeRegression(tau, kernel, labels)\n",
      "krr.train(feats_train)\n",
      "kernel.init(feats_train, grid)\n",
      "out = krr.apply().get_labels()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thus we have a relationship between the attributes we wanted. Below is an attempt to establish such relationship between the attributes individually. You could skip the code and have a look at the plots directly if you just want the essence.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Visualization of regression\n",
      "feats_test=RealFeatures(x1.reshape(1,len(x1)))\n",
      "feats_t0=array(mat[7])\n",
      "feats_train0=RealFeatures(feats_t0.reshape(1,len(feats_t0)))\n",
      "feats_t1=array(mat[12])\n",
      "feats_train1=RealFeatures(feats_t1.reshape(1,len(feats_t1)))\n",
      "\n",
      "#Regression with first attribute\n",
      "kernel=GaussianKernel(feats_train0, feats_train0, width)\n",
      "krr=KernelRidgeRegression(tau, kernel, labels)\n",
      "krr.train(feats_train0)\n",
      "kernel.init(feats_train0, feats_test)\n",
      "out0 = krr.apply().get_labels()\n",
      "\n",
      "#Regression with second attribute \n",
      "kernel=GaussianKernel(feats_train1, feats_train1, width)\n",
      "krr=KernelRidgeRegression(tau, kernel, labels)\n",
      "krr.train(feats_train1)\n",
      "kernel.init(feats_train1, feats_test)\n",
      "out1 = krr.apply().get_labels()\n",
      "\n",
      "fig=figure(figsize(20,6))\n",
      "#first plot with only one attribute\n",
      "fig.add_subplot(131)\n",
      "title(\"Regression with 1st attribute\")\n",
      "_=scatter(feats[0, :], trainlab,c=ones(560), cmap=gray(), s=20)\n",
      "_=xlabel('Weighted distances to employment centres ')\n",
      "_=ylabel('Median value of homes')\n",
      "\n",
      "_=plot(x1,out0, linewidth=3)\n",
      "\n",
      "#second plot with only one attribute\n",
      "fig.add_subplot(132)\n",
      "title(\"Regression with 2nd attribute\")\n",
      "_=scatter(feats[1, :], trainlab,c=ones(560), cmap=gray(), s=20)\n",
      "_=xlabel('% lower status of the population')\n",
      "_=ylabel('Median value of homes')\n",
      "_=plot(x1,out1, linewidth=3)\n",
      "\n",
      "#Both attributes and regression output\n",
      "ax=fig.add_subplot(133, projection='3d')\n",
      "z=out.reshape((size, size))\n",
      "gray()\n",
      "title(\"Regression\")\n",
      "ax.plot_wireframe(y, x, z, linewidths=2, alpha=0.4)\n",
      "ax.view_init(25, 40)\n",
      "ax.set_xlabel('% lower status of the population')\n",
      "ax.set_ylabel('Distances to employment centres ')\n",
      "ax.set_zlabel('Median value of homes')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}