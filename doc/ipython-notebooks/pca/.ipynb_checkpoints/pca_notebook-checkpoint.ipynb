{
 "metadata": {
  "name": "",
  "signature": "sha256:224105e9fb538267eabc12f3e834d563e5c3d05558c027c295c6b4b14e4e90f4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Principal Component Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "nbviewer:  http://nbviewer.ipython.org/gist/kislayabhi/9431770"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PCA finds a linear projection of high dimensional data into a lower dimensional subspace such that the variance is retained and least square reconstruction error is minimized."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we concentrate on linear dimension reduction techniques. In this approach a high dimensional datapoint $x$ is 'projected down' to a lower dimensional vector $y$ by using $y=F*x+const$ where the non-square matrix $F$ has dimensions $dim(y)*dim(x)$, with $dim(y) < dim(x)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Effectively, we are trying to discover a low dimensional co-ordinate system in which we can approximately represent the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's already make it working!!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lets generate the toy data.\n",
      "import numpy as np\n",
      "from modshogun import *\n",
      "\n",
      "def randrange(n, vmin, vmax):\n",
      "    return (vmax-vmin)*np.random.rand(n)+vmin\n",
      "\n",
      "# number of data points:\n",
      "n=100\n",
      "\n",
      "#generate random 2d data\n",
      "xs = randrange(n,-20,+20)\n",
      "ys = randrange(n,-20,+20)\n",
      "\n",
      "#subtract the mean from this\n",
      "mxs=mean(xs)\n",
      "xs = xs - np.tile(mxs,[n,1]).T[0]\n",
      "mys=mean(ys)\n",
      "ys = ys - np.tile(mys,[n,1]).T[0]\n",
      "\n",
      "#form the data matrix\n",
      "twoD_obsmatrix=np.array([xs, ys])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lets visualise the given data.\n",
      "from matplotlib import pyplot\n",
      "pyplot.ion() #to set the matplotlib's interactive mode on!\n",
      "figure, axis = pyplot.subplots(1,1)\n",
      "axis.plot(twoD_obsmatrix[0,:], twoD_obsmatrix[1,:],'o',color='green', markersize=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lets get our pca preprocessor ready to cut down this data's dimensions from 2 to 1\n",
      "\n",
      "def apply_pca_to_data(target_dims, data_matrix):\n",
      "    train_features = RealFeatures(data_matrix)\n",
      "    submean = PruneVarSubMean(False)\n",
      "    submean.init(train_features)\n",
      "    submean.apply_to_feature_matrix(train_features)\n",
      "    preprocessor = PCA()\n",
      "    preprocessor.set_target_dim(target_dims)\n",
      "    preprocessor.init(train_features)\n",
      "    eigenvectors = preprocessor.get_transformation_matrix()\n",
      "    preprocessor.apply_to_feature_matrix(train_features)\n",
      "    return train_features,eigenvectors\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#apply pca\n",
      "#the target_dims is made 1 for this 2d data.\n",
      "y,eig = apply_pca_to_data(1, twoD_obsmatrix)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Automatically we are returned with that eigenvector which corresponds to higher eigenvalue\n",
      "eig1=eig\n",
      "\n",
      "#hence we need to take only that set of weights which corresponds to eig1.\n",
      "y1=y[0,:]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# y is just the weights that each eigenvector carries. i.e\n",
      "# here we have only 1 eigenvector at our disposal(we are removing the other one corresponding to lesser eigenvalue to achieve\n",
      "# dimension reduction).\n",
      "# so for       datapoint1 (x,y) in 2d is approximated by y1[0]*eig1 in 1d\n",
      "# similarly    datapoint2 (x,y) in 2d is approximated by y1[1]*eig1 in 1d ...\n",
      "\n",
      "\n",
      "\n",
      "#we visualise this:\n",
      "figure, axis = pyplot.subplots(1,1)\n",
      "pyplot.xlim(-20, 20)\n",
      "pyplot.ylim(-20, 20)\n",
      "a1=axis.plot(twoD_obsmatrix[0,:], twoD_obsmatrix[1,:],'o',color='green', markersize=5, label=\"green\")\n",
      "a2=axis.plot((y1 * eig1[0]), (y[0,:] * eig1[1]), 'o', color='blue', markersize=5, label=\"red\")\n",
      "pyplot.title('the projection of 2d data onto 1d maintaining the best variance between the datapoints!')\n",
      "p1 = Rectangle((0, 0), 1, 1, fc=\"r\")\n",
      "p2 = Rectangle((0, 0), 1, 1, fc=\"g\")\n",
      "p3 = Rectangle((0, 0), 1, 1, fc=\"b\")\n",
      "legend([p1,p2,p3],[\"normal projection\",\"2d data\",\"1d projection\"])\n",
      "\n",
      "\n",
      "#we are plotting the projection here:\n",
      "for i in range(n):\n",
      "    axis.plot([xs[i],(y[0,i]*eig1[0])],[ys[i],(y[0,i]*eig1[1])] , color='red')\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lets take one more example to make things more clearer. here we will convert a 3d data to 2d.\n",
      "\n",
      "#we generate the height of the previous data.\n",
      "zs=randrange(n, -5, +5)\n",
      "mzs=mean(zs)\n",
      "zs = zs - np.tile(mzs,[n,1]).T[0]\n",
      "threeD_obsmatrix=array([xs, ys, zs])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for plotting the 3d data, we import Axes3D from matplotlib module\n",
      "from mpl_toolkits.mplot3d import Axes3D"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#plot the 3d data\n",
      "fig = pyplot.figure()\n",
      "ax=fig.add_subplot(111, projection='3d')\n",
      "ax.scatter(xs, ys, zs,marker='o', color='g')\n",
      "ax.set_xlabel('x label')\n",
      "ax.set_ylabel('y label')\n",
      "ax.set_zlabel('z label')\n",
      "legend([p2],[\"3d data\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#again applying the previous methadology, we proceed by applying the pca\n",
      "y,eig= apply_pca_to_data(2, threeD_obsmatrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#here we are trying to reduce dimensionality to two, hence only two eigenvectors\n",
      "#are to be taken according to their eigenvalues.\n",
      "\n",
      "#1st eigenvactor\n",
      "eig1=eig[:,0]\n",
      "\n",
      "#2nd eigenvactor\n",
      "eig2=eig[:,1]\n",
      "\n",
      "#similarly, we need to have two sets of weights. one corresponding to eig1 and the other corresponding to eig2\n",
      "\n",
      "#1st set of weights\n",
      "y1=y[0,:]\n",
      "\n",
      "#2nd set of weights\n",
      "y2=y[1,:]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lets visualise the output:\n",
      "fig=pyplot.figure()\n",
      "ax1=fig.add_subplot(111, projection='3d')\n",
      "legend([p3],[\"2d projected data points\"])\n",
      "for i in range(100):\n",
      "    points=y1[i]*eig1+y2[i]*eig2\n",
      "    ax1.scatter(points[0], points[1], points[2],marker='o', color='b')\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#all the above points lie on the same plane. to make it more clear we will plot the projection also.\n",
      "fig=pyplot.figure()\n",
      "ax2=fig.add_subplot(111, projection='3d')\n",
      "ax2.scatter(xs, ys, zs,marker='o', color='g')\n",
      "ax2.set_xlabel('x label')\n",
      "ax2.set_ylabel('y label')\n",
      "ax2.set_zlabel('z label')\n",
      "legend([p1,p2,p3],[\"normal projection\",\"3d data\",\"2d projection\"])\n",
      "for i in range(100):\n",
      "    points=y1[i]*eig1+y2[i]*eig2\n",
      "    ax2.scatter(points[0], points[1], points[2],marker='o', color='b')\n",
      "    ax2.plot([xs[i],points[0]],[ys[i],points[1]],[zs[i],points[2]],color='r')\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Enough of the toy data! Lets work on something real.\n",
      "\n",
      "Here we will show the implementation of PCA for data compression and basic face recognition.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Eigenfaces"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "\n",
      "Eigenfaces refers to an appearance-based approach to face recognition that seeks to capture the variation in a collection of face images and use this information to encode and compare images of individual faces in a holistic manner.\n",
      "\n",
      "Specifically, the eigenfaces are the principal components of a distribution of faces, or equivalently, the eigenvectors of the covariance matrix of the set of face images, where an image with $N$ pixels is considered a point(or vector) in N-dimension space.\n",
      "\n",
      "The eigenfaces may be considered as a set of features which characterize the global variation among face images. Then each face image is approximated using a subset of the eigenfaces, those associated with the largest eigenvalues. These features account for the most variance in the training set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "\n",
      "#lets start with data preparation.\n",
      "#Download the att_data set from http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n",
      "#Then make sure to put all images (in personwise different folders) in a single folder(lots of renaming may be required!!)\n",
      "\n",
      "def get_imlist(path):\n",
      "    \"\"\" Returns a list of filenames for all jpg images in a directory\"\"\"\n",
      "    return [os.path.join(path,f) for f in os.listdir(path) if f.endswith('.pgm')]\n",
      "\n",
      "#set path to the required folder.\n",
      "path='../../../data/att_dataset/training/'\n",
      "\n",
      "#set no. of rows that the images will be resized.\n",
      "k1=100\n",
      "#set no. of columns that the images will be resized.\n",
      "k2=100\n",
      "\n",
      "filenames = get_imlist(path)\n",
      "filenames = np.array(filenames)\n",
      "# n is total number of images that has to be analysed.\n",
      "n=len(filenames)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we will be using this often to visualise the images out there.\n",
      "def showfig(image):\n",
      "    imgplot=plt.imshow(image, cmap='gray')\n",
      "    imgplot.axes.get_xaxis().set_visible(False)\n",
      "    imgplot.axes.get_yaxis().set_visible(False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import Image\n",
      "from scipy import misc\n",
      "\n",
      "# to get a hang of the data, lets see some part of the dataset images.\n",
      "fig = plt.figure()\n",
      "\n",
      "for i in range(49):\n",
      "    fig.add_subplot(7,7,i)\n",
      "    train_img=np.array(Image.open(filenames[i]).convert('L'))\n",
      "    train_img=misc.imresize(train_img, [k1,k2])\n",
      "    showfig(train_img)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#read each and every image. flatten them to make row vectors and stack them together \n",
      "#to form the observation matrix obs_matrix.\n",
      "\n",
      "train_img = np.array(Image.open(filenames[0]).convert('L'))\n",
      "train_img=misc.imresize(train_img, [k1,k2])\n",
      "train_img=np.array(train_img, dtype='double')\n",
      "train_img=train_img.flatten()\n",
      "\n",
      "for i in range(1,n):\n",
      "    temp=np.array(Image.open(filenames[i]).convert('L'))    \n",
      "    temp=misc.imresize(temp, [k1,k2])\n",
      "    temp=np.array(temp, dtype='double')\n",
      "    temp=temp.flatten()\n",
      "    train_img=np.vstack([train_img,temp])\n",
      "    \n",
      "obs_matrix=train_img.T\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Again applying the PCA on the data, the same way we were doing for the last \n",
      "# two times.\n",
      "# here we are setting the target dimension as 100. Hence we are trying to represent n*10000 dim. data to 100*10000 dim.\n",
      "\n",
      "y,eig= apply_pca_to_data(100,obs_matrix)\n",
      "res=y.get_feature_matrix()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lets see how these eigenfaces/eigenvectors look like:\n",
      "fig1 = plt.figure()\n",
      "plt.title('top 20 eigenfaces')\n",
      "\n",
      "\n",
      "for i in range(20):\n",
      "    a = fig1.add_subplot(5,4,i+1)\n",
      "    eigen_faces=eig[:,i].reshape([k1,k2])\n",
      "    showfig(eigen_faces)\n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#to see the reconstructed image from 100 eigenvectors/eigenfaces, we multiply each eigenfaces with their respective weights,\n",
      "# which when added provides us with a reconstruction of the original image.\n",
      "\n",
      "reconstructed_vector=np.mat(eig)*np.mat(res)\n",
      "\n",
      "#plot the reconstructed images to visualise it. \n",
      "#We are here able to reconstruct all the images by shedding (n-100) * 10000 dimensions!! Thats data compression !!\n",
      "fig2=plt.figure()\n",
      "for i in range(1,50):\n",
      "    reconstructed_image = reconstructed_vector[:,i].reshape([k1,k2])\n",
      "    fig2.add_subplot(7,7,i)\n",
      "    showfig(reconstructed_image)\n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Face Recognition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets use this technique of eigenfaces to perform some basic face recognition.\n",
      "The eigenfaces span an m-dimensional subspace of the original image space by choosing the subset of eigenvectors $U\u02c6={u1,\u22ef,um}$ associated with the m largest eigenvalues. This results in the so-called face space, whose origin is the average face, and whose axes are the eigenfaces (see Figure 3). To perform face detection or recognition, one may compute the distance within or from the face space.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A new face test_img is projected into the face space by $eig^T * testimg$ , where $eig^T$ is the set of significant eigenvectors. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the test image in the list test_file.\n",
      "test_files= get_imlist('../../../data/att_dataset/testing/')\n",
      "test_img=np.array(Image.open(test_files[0]).convert('L'))\n",
      "\n",
      "#we plot the test image , for which we have to identify a good match from the training images we already have\n",
      "fig = plt.figure()\n",
      "t_img=plt.imshow(test_img, cmap='gray')\n",
      "plt.title('the test image for which a match is to be identified')\n",
      "t_img.axes.get_xaxis().set_visible(False)\n",
      "t_img.axes.get_yaxis().set_visible(False)\n",
      "\n",
      "# we flatten out or test image just the way we have done for the other images\n",
      "test_image=misc.imresize(test_img, [k1,k2])\n",
      "test_image=test_image.flatten()\n",
      "test_image=np.array(test_image, dtype='double')\n",
      "test_image=np.mat(test_image).transpose()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we have to project our training images as well as the test image on the PCA subspace."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#projecting our training images into pca subspace\n",
      "train_proj=np.dot(eig.T , obs_matrix)\n",
      "\n",
      "#projecting our test image into pca subspace\n",
      "test_proj=np.dot(eig.T , test_image)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#here we are using the Eucledian Diatance Measure for finding out the similarity\n",
      "#between test image and all the training images.\n",
      "#The training image which produces the least distance measure with our test image \n",
      "#is said to be the best match.\n",
      "testfeat = RealFeatures(np.array(test_proj))\n",
      "workfeat = RealFeatures(np.array(train_proj))\n",
      "RaRb=EuclideanDistance(testfeat, workfeat)\n",
      "\n",
      "d=np.empty([n,1])\n",
      "for i in range(n):\n",
      "    d[i]= RaRb.distance(0,i)\n",
      "    \n",
      "\n",
      "iden=np.array(Image.open(filenames[d.argmin()]))\n",
      "i_img=plt.imshow(iden, cmap='gray')\n",
      "plt.title('identified image from our set of training images')\n",
      "i_img.axes.get_xaxis().set_visible(False)\n",
      "i_img.axes.get_yaxis().set_visible(False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}