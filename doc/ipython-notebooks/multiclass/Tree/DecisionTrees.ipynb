{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Decision Trees"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "*By Parijat Mazumdar (GitHub ID: [mazumdarparijat](https://github.com/mazumdarparijat))*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook illustrates the use of [decision trees](http://en.wikipedia.org/wiki/Decision_tree) in Shogun for classification and regression. Various [decision tree learning](http://en.wikipedia.org/wiki/Decision_tree_learning) algorithms like [ID3](http://en.wikipedia.org/wiki/ID3_algorithm), [C4.5](http://en.wikipedia.org/wiki/C4.5_algorithm), [CART](http://themainstreamseer.blogspot.in/2013/01/introduction-to-classification.html), [CHAID](http://en.wikipedia.org/wiki/CHAID), [MARS](http://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines) have been discussed in detail using both intuitive toy datasets as well as real-world datasets."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Decision Tree Basics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Decision Trees are a non-parametric supervised learning method that can be used for both classification and regression. Decision trees essentially encode a set of if-then-else rules which can be used to predict target variable given data features. These if-then-else rules are formed using the training dataset with the aim to satisfy as much training data instances as possible. The formation of these rules (aka. decision tree) from training data is called decision tree learning. Various decision tree learning algorithms have been developed and they work best in different situations. An advantage of decision trees is that they can model any type of function for classification or regression which other techniques cannot. But a decision tree is highly prone to overfitting and bias towards training data. So, decision trees are used for very large datasets which are assumed to be represent the ground truth well. Additionally, certain [tree pruning algorithms](http://en.wikipedia.org/wiki/Pruning_%28decision_trees%29) are also used to tackle overfitting.   "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "ID3 (Iterative Dichotomiser 3)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ID3 is a straightforward decision tree learning algorithm developed by [Ross Quinlan](http://en.wikipedia.org/wiki/Ross_Quinlan). ID3 is applicable only in cases where the attributes (or features) defining data examples are categorical in nature and the data examples belong to pre-defined, clearly distinguishable (ie. well defined) classes. ID3 is an iterative greedy algorithm which starts with the root node and eventually builds the entire tree. At each node, the \"best\" attribute to classify data is chosen. The \"best\" attribute is chosen using the [information gain metric](http://en.wikipedia.org/wiki/Information_gain_in_decision_trees). Once an attribute is chosen in a node, the data examples in the node are categorized into sub-groups based on the attribute values that they have. Basically, all data examples having the same attribute value are put together in the same sub-group. These sub-groups form the children of the present node and the algorithm is repeated for each of the newly formed children nodes. This goes on until all the data members of a node belong to the same class or all the attributes are exhausted. In the latter case, the class predicted may be erroneous and generally the mode of the classes appearing in the node is chosen as the predictive class. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Pseudocode for ID3 Algorithm"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\tInputs: (D, A, C), where:\n",
      "\t\tD is a dataset with only nominal instance attributes A \n",
      "\t\tC is the class attribute\n",
      "\n",
      "\tOutput: a decision tree T representing a sequential decision process for\n",
      "\tclassifying instances (predicting the values of the class attribute C);\n",
      "\teach node of T is labeled with a non-class attribute of A\n",
      "\n",
      "\tInformal Inductive Bias: minimize the average height of the tree\n",
      "\n",
      "\tProcedure:\n",
      "\n",
      "\tif the set of remaining non-class attributes is empty \n",
      "\tor if all of the instances in D are in the same class\n",
      "\n",
      "\t\treturn an empty tree\n",
      "\n",
      "\telse \n",
      "    {\n",
      "\t\tcompute the Information Gain for each attribute over the dataset D\n",
      "\t\t\tlet a* be an attribute with maximum Information Gain\t\n",
      "\n",
      "\t\tcreate a root node for a tree T; label it with a* \n",
      "\n",
      "\t\tfor each value b of attribute a* \n",
      "        {\n",
      "\t\t\tlet T(a*=b) be the tree computed recursively by ID3 on input (D|a*=b, A-a*, C), where:\n",
      "\t\t\t\tD|a*=b contains all instances of D for which a* has the value b\n",
      "\t\t\t\tA-a* consists of all attributes of A except a*\n",
      "\n",
      "\t\t\tattach T(a*=b) to the root of T as a subtree\n",
      "\t\t}\n",
      "\t\t\n",
      "\t\treturn the resulting decision tree T\n",
      "\t}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example using a Simple dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section, we create a simple example where we try to predict the usage of mobile phones by individuals based on their income, age, education and marital status. Each of the attributes have been categorized into 2 or 3 types. Let us create the training dataset and tabulate it first."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# training data\n",
      "train_income=['Low','Medium','Low','High','Low','High','Medium','Medium','High','Low','Medium',\n",
      "'Medium','High','Low','Medium']\n",
      "\n",
      "train_age = ['Old','Young','Old','Young','Old','Young','Young','Old','Old','Old','Young','Old',\n",
      "'Old','Old','Young']\n",
      "\n",
      "train_education = ['University','College','University','University','University','College','College',\n",
      "'High School','University','High School','College','High School','University','High School','College']\n",
      "\n",
      "train_marital = ['Married','Single','Married','Single','Married','Single','Married','Single','Single',\n",
      "'Married','Married','Single','Single','Married','Married']\n",
      "\n",
      "train_usage = ['Low','Medium','Low','High','Low','Medium','Medium','Low','High','Low','Medium','Low',\n",
      "'High','Low','Medium']\n",
      "\n",
      "# print data\n",
      "print 'Training Data Table : \\n'\n",
      "print 'Income \\t\\t Age \\t\\t Education \\t\\t Marital Status \\t Usage'\n",
      "for i in xrange(len(train_income)):\n",
      "\tprint train_income[i]+' \\t\\t '+train_age[i]+' \\t\\t '+train_education[i]+' \\t\\t '+train_marital[i]+' \\t\\t '+train_usage[i]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to create a decision tree from the above training dataset. The first step for that is to encode the data into numeric values and bind them to Shogun's features and multiclass labels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import ID3ClassifierTree, RealFeatures, MulticlassLabels\n",
      "from numpy import array, concatenate\n",
      "\n",
      "# encoding dictionary\n",
      "income = {'Low' : 1.0, 'Medium' : 2.0, 'High' : 3.0}\n",
      "age = {'Young' : 1.0, 'Old' : 2.0}\n",
      "education = {'High School' : 1.0, 'College' : 2.0, 'University' : 3.0}\n",
      "marital_status = {'Married' : 1.0, 'Single' : 2.0}\n",
      "usage = {'Low' : 1.0, 'Medium' : 2.0, 'High' : 3.0}\n",
      "\n",
      "\n",
      "# encode training data\n",
      "for i in xrange(len(train_income)):\n",
      "\ttrain_income[i] = income[train_income[i]]\n",
      "\ttrain_age[i] = age[train_age[i]]\n",
      "\ttrain_education[i] = education[train_education[i]]\n",
      "\ttrain_marital[i] = marital_status[train_marital[i]]\n",
      "\ttrain_usage[i] = usage[train_usage[i]]\n",
      "    \n",
      "# form Shogun feature matrix\n",
      "train_data = array([train_income, train_age, train_education, train_marital])\n",
      "train_feats = RealFeatures(train_data);\n",
      "\n",
      "# form Shogun multiclass labels\n",
      "labels = MulticlassLabels(array(train_usage));"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we learn our decision tree using the features and labels created."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create ID3ClassifierTree object\n",
      "id3 = ID3ClassifierTree()\n",
      "\n",
      "# set labels\n",
      "id3.set_labels(labels)\n",
      "\n",
      "# learn the tree from training features\n",
      "is_successful = id3.train(train_feats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our decision tree is ready now and we want to use it to make some predictions over test data. So, let us create some test data examples first."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test data\n",
      "test_income = ['Medium','Medium','Low','High','High']\n",
      "test_age = ['Old','Young','Old','Young','Old']\n",
      "test_education = ['University','College','High School','University','College']\n",
      "test_marital = ['Married','Single','Married','Single','Married']\n",
      "test_usage = ['Low','Medium','Low','High','High']\n",
      "\n",
      "# tabulate test data\n",
      "print 'Test Data Table : \\n'\n",
      "print 'Income \\t\\t Age \\t\\t Education \\t\\t Marital Status \\t Usage'\n",
      "for i in xrange(len(test_income)):\n",
      "\tprint test_income[i]+' \\t\\t '+test_age[i]+' \\t\\t '+test_education[i]+' \\t\\t '+test_marital[i]+' \\t\\t ?'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, as with training data, we encode our test dataset and bind it to Shogun features. Then, we apply our decision tree to the test examples to obtain the predicted labels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# encode test data\n",
      "for i in xrange(len(test_income)):\n",
      "\ttest_income[i] = income[test_income[i]]\n",
      "\ttest_age[i] = age[test_age[i]]\n",
      "\ttest_education[i] = education[test_education[i]]\n",
      "\ttest_marital[i] = marital_status[test_marital[i]]\n",
      "\n",
      "# bind to shogun features    \n",
      "test_data = array([test_income, test_age, test_education, test_marital])\n",
      "test_feats = RealFeatures(test_data)\n",
      "\n",
      "# apply decision tree classification\n",
      "test_labels = id3.apply_multiclass(test_feats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally let us tabulate the results obtained and compare them with our intuitive predictions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output = test_labels.get_labels();\n",
      "output_labels=[0]*len(output)\n",
      "\n",
      "# decode back test data for printing\n",
      "for i in xrange(len(test_income)):\n",
      "\ttest_income[i]=income.keys()[income.values().index(test_income[i])]\n",
      "\ttest_age[i]=age.keys()[age.values().index(test_age[i])]\n",
      "\ttest_education[i]=education.keys()[education.values().index(test_education[i])]\n",
      "\ttest_marital[i]=marital_status.keys()[marital_status.values().index(test_marital[i])]\n",
      "\toutput_labels[i]=usage.keys()[usage.values().index(output[i])]\n",
      "\n",
      "# print output data\n",
      "print 'Final Test Data Table : \\n'\n",
      "print 'Income \\t Age \\t Education \\t Marital Status \\t Usage(predicted)'\n",
      "for i in xrange(len(test_income)):\n",
      "\tprint test_income[i]+' \\t '+test_age[i]+' \\t '+test_education[i]+' \\t '+test_marital[i]+' \\t\\t '+output_labels[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, do the predictions made by our decision tree match our inferences from training set? Yes! For example, from the training set we infer that all individuals having low income have low usage and also all individuals going to college have medium usage. The decision tree predicts the same for both cases. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example using a real dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We choose the [car evaluation dataset](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.html) as our real-world dataset. The [car.names](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.c45-names) file of the dataset enumerates the class categories as well as the non-class attributes. Each car categorized into one of 4 classes : unacc, acc, good, vgood. Each car is judged using 6 attributes : buying, maint, doors, persons, lug_boot, safety. Each of these attributes can take 3-4 values. Let us first make a dictionary to encode strings to numeric values using information from cars.names file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# class attribute\n",
      "evaluation = {'unacc' : 1.0, 'acc' : 2.0, 'good' : 3.0, 'vgood' : 4.0}\n",
      "\n",
      "# non-class attributes\n",
      "buying = {'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0}\n",
      "maint = {'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0}\n",
      "doors = {'2' : 1.0, '3' : 2.0, '4' : 3.0, '5more' : 4.0}\n",
      "persons = {'2' : 1.0, '4' : 2.0, 'more' : 3.0}\n",
      "lug_boot = {'small' : 1.0, 'med' : 2.0, 'big' : 3.0}\n",
      "safety = {'low' : 1.0, 'med' : 2.0, 'high' : 3.0}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, let us read the file and form Shogun features and labels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('../../../../data/multiclass/categorical_dataset/car.data', 'r')\n",
      "\n",
      "features = []\n",
      "labels = []\n",
      "\n",
      "# read data from file and encode\n",
      "for line in f:\n",
      "    words = line.rstrip().split(',')\n",
      "    words[0] = buying[words[0]]\n",
      "    words[1] = maint[words[1]]\n",
      "    words[2] = doors[words[2]]\n",
      "    words[3] = persons[words[3]]\n",
      "    words[4] = lug_boot[words[4]]\n",
      "    words[5] = safety[words[5]]\n",
      "    words[6] = evaluation[words[6]]\n",
      "    features.append(words[0:6])\n",
      "    labels.append(words[6])\n",
      "\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the entire dataset, let us choose some test vectors to form our test dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import random, delete\n",
      "\n",
      "features = array(features)\n",
      "labels = array(labels)\n",
      "\n",
      "# number of test vectors\n",
      "num_test_vectors = 200;\n",
      "\n",
      "test_indices = random.randint(features.shape[0], size = num_test_vectors)\n",
      "test_features = features[test_indices]\n",
      "test_labels = labels[test_indices]\n",
      "\n",
      "# remove test vectors from training set\n",
      "features = delete(features,test_indices,0)\n",
      "labels = delete(labels,test_indices,0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next step is to train our decision tree using the training features and applying it to our test dataset to get predicted output classes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# shogun test features and labels\n",
      "test_feats = RealFeatures(test_features.T)\n",
      "test_labels = MulticlassLabels(test_labels)\n",
      "\n",
      "# method for id3 training and\n",
      "def ID3_routine(features, labels):\n",
      "\n",
      "    # Shogun train features and labels\n",
      "    train_feats = RealFeatures(features.T)\n",
      "    train_lab = MulticlassLabels(labels)\n",
      "\n",
      "    # create ID3ClassifierTree object\n",
      "    id3 = ID3ClassifierTree()\n",
      "\n",
      "    # set labels\n",
      "    id3.set_labels(train_lab)\n",
      "\n",
      "    # learn the tree from training features\n",
      "    id3.train(train_feats)\n",
      "\n",
      "    # apply to test dataset\n",
      "    output = id3.apply_multiclass(test_feats)\n",
      "    \n",
      "    return output\n",
      "\n",
      "output = ID3_routine(features, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, let us compare the our predicted labels with test labels to find out the percentage error of our classification model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import MulticlassAccuracy\n",
      "\n",
      "# Shogun object for calculating multiclass accuracy\n",
      "accuracy = MulticlassAccuracy()\n",
      "print 'Accuracy : ' + str(accuracy.evaluate(output, test_labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the accuracy is moderately high. Thus our decision tree can evaluate any car given its features with a very high success rate. As a final exercise, let us examine the effect of training dataset size on the accuracy of decision tree."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list of error rates for all training dataset sizes\n",
      "error_rate = []\n",
      "\n",
      "# loop over training dataset size\n",
      "for i in range(500,1600,200):\n",
      "    indices = random.randint(features.shape[0], size = i)\n",
      "    train_features = features[indices]\n",
      "    train_labels = labels[indices]\n",
      "    \n",
      "    average_error = 0\n",
      "    for i in xrange(3):\n",
      "        output = ID3_routine(train_features, train_labels)\n",
      "        average_error = average_error + (1-accuracy.evaluate(output, test_labels))\n",
      "        \n",
      "    error_rate.append(average_error/3)\n",
      "\n",
      "# plot the error rates    \n",
      "import matplotlib.pyplot as pyplot\n",
      "% matplotlib inline\n",
      "from scipy.interpolate import interp1d\n",
      "from numpy import linspace, arange\n",
      "\n",
      "fig,axis = pyplot.subplots(1,1)\n",
      "x = arange(500,1600,200)\n",
      "f = interp1d(x, error_rate)\n",
      "\n",
      "xnew = linspace(500,1500,100)\n",
      "pyplot.plot(x,error_rate,'o',xnew,f(xnew),'-')\n",
      "pyplot.xlim([400,1600])\n",
      "pyplot.xlabel('training dataset size')\n",
      "pyplot.ylabel('Classification Error')\n",
      "pyplot.title('Decision Tree Performance')\n",
      "pyplot.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "NOTE : The above code snippet takes about half a minute to execute. Please wait patiently."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the above plot, we see that error rate decreases steadily as we increase the training dataset size. Although in this case, the decrease in error rate is not very significant, in many datasets this decrease in error rate can be substantial."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[1] Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science\n",
      "\n",
      "[2] Quinlan, J. R. 1986. Induction of Decision Trees. Mach. Learn. 1: 1 (Mar. 1986), 81-106"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}