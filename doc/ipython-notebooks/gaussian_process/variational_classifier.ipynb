{
 "metadata": {
  "name": "",
  "signature": "sha256:71842193db95866bd9082d4ea7921d5e511e9499bd009d11a224842b8a88d57b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Variational Inference in the GP modular of Shogun"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "By Wu Lin - <a href=\"mailto:yorker.lin@gmail.com\">yorker.lin@gmail.com</a> - <a href=\"https://github.com/yorkerlin\">https://github.com/yorkerlin</a> Suggested by Heiko Strathmann and Mohammad Emtiyaz Khan\n",
      "\n",
      "Based on the notebook of Gaussian Processes by Heiko Strathmann - <a href=\"mailto:heiko.strathmann@gmail.com\">heiko.strathmann@gmail.com</a> - <a href=\"https://github.com/karlnapf\">github.com/karlnapf</a> - <a href=\"http://herrstrathmann.de\">herrstrathmann.de</a>, and the GP framework of the Google summer of code 2014 project of Wu Lin,  - Google summer of code 2013 projec of Roman Votyakov - <a href=\"mailto:votjakovr@gmail.com\">votjakovr@gmail.com</a> - <a href=\"https://github.com/votjakovr\">github.com/votjakovr</a>, and the Google summer of code 2012 project of Jacob Walker - <a href=\"mailto:walke434@gmail.com\">walke434@gmail.com</a> - <a href=\"https://github.com/puffin444\">github.com/puffin444</a> "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook describes how to do v<a href=\"http://en.wikipedia.org/wiki/Variational_Bayesian_methods\">variational inference</a> for <a href=\"http://en.wikipedia.org/wiki/Gaussian_process\">Gaussian Process (GP) <a href=\"http://en.wikipedia.org/wiki/Statistical_classification\">classification</a> models in Shogun. \n",
      "\n",
      "\n",
      "We assume reader has some background in <a href=\"http://en.wikipedia.org/wiki/Bayesian_statistics\">Bayesian statistics</a>. For background in Bayesian statistics, please see the <a href=\"http://www.shogun-toolbox.org/static/notebook/current/gaussian_processes.html\">notebook</a> about Gaussian Process.\n",
      "\n",
      "After providing a semi-formal introduction, we illustrate how to do training, make predictions, and automatically learn hyper-parameters for GPs in Shogun"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "# import all shogun classes\n",
      "from modshogun import *\n",
      "\n",
      "# import all required libraries\n",
      "import scipy\n",
      "import scipy.io\n",
      "import numpy as np\n",
      "from math import exp,sqrt\n",
      "import time\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Brief Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In binary classification, we get binary labels in form of a column vector, $\\mathbf{y}\\in\\mathcal{Y}^n=\\{-1,+1\\}^n$, and features as a matrix, $\\mathbf{X}\\in\\mathcal{R}^{n\\times d}$, where $n$ is the number of data points and $d$ is the number of features.  A likelihood function $p(\\mathbf{y}|\\text{f},\\mathbf{X})=p(\\mathbf{y}|\\mathbf{f})$ is used to model data with labels, where a function $\\text{f}:\\mathcal{R}^{d}\\to \\mathcal{R} $ is drawn from a Gaussian Process prior and $\\text{f}(\\mathbf{X})=\\mathbf{f} \\in \\mathcal{R}^{n}$ is a column vector by applying f to each row of $\\mathbf{X}$. Note that the difference between f and $\\mathbf{f}$ is that f is a function while $\\mathbf{f}$ is an n-dimensional vector. $p(\\text{f})$ and $p(\\mathbf{f})$ are also different because $p(\\text{f})$ is a distribution in an infinite dimensional (function) space while $p(\\mathbf{f})$ is a distribution in an finite dimensional space.\n",
      "\n",
      "Given data with labels, our goal is to train a Gaussian Process classifier to fit the data well.\n",
      "In other words, we want to learn posterior, $p(\\text{f}|\\mathbf{y},\\mathbf{X}) \\propto p(\\mathbf{y}| \\text{f},\\mathbf{X})*p(\\text{f}|GP) $, given training data points. In fact, we will see in the next session that all we need is to learn $p(\\mathbf{f}|\\mathbf{y})$ (Note that we use $\\mathbf{f}$ as $\\text{f}(\\mathbf{X})$ for short).\n",
      "\n",
      "The key intuition of variational inference is to approximate the distribution of interest (here $p(\\mathbf{f}|\\mathbf{y})$, which is a non-Gaussian distribution) with a more tractable distribution (here a Gaussian $q(\\mathbf{f}|\\mathbf{y}))$, via minimizing the <a href=\"http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\">Kullback\u2013Leibler divergence</a> (KL divergence) \n",
      "\n",
      "$${\\mathrm{KL}}(Q\\|P) = \\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{p(\\mathbf{f}|\\mathbf{y})}\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f}$$\n",
      "\n",
      "between the true distribution and the approximation.\n",
      "\n",
      "Please see the next session if you want to know the reason why non-Gaussian $p(\\mathbf{f}|\\mathbf{y})$ cannot be used in inference process of GPC.\n",
      "\n",
      "Throughout this notebook, we deal with binary classification using <a href=\"http://en.wikipedia.org/wiki/Logit\">inverse-logit</a>, (also known as Bernoulli-logistic function)\n",
      "likelihood to model labels, given by \n",
      "\n",
      "$p(\\mathbf{y}|\\mathbf{f})=\\prod_{i=1}^n p(y_\\text{i}|\\text{f}(\\mathbf{X}_\\text{i}))=\\prod_{i=1}^n \\frac{1}{1-\\exp(-y_\\text{i} \\mathbf{f}_\\text{i})}$,\n",
      "where $\\mathbf{f}_\\text{i}$ is the i-th row of $\\mathbf{X}$ and $\\text{f}(\\mathbf{X}_\\text{i})=\\mathbf{f}_\\text{i}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "More detailed information about GPC (Skip if you just want code examples)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In GPC models, our goal is to predict the label ($y_\\text{new}$) of a new data point, a column vector, $\\mathbf{x}_\\text{new}\\in\\mathcal{R}^{d}$ based on $p(y_\\text{new}|\\mathbf{y},\\mathbf{X},\\mathbf{x}_\\text{new})$. \n",
      "According to <a href=\"http://en.wikipedia.org/wiki/Bayes%27_theorem\">Bayes' theorem</a>, we know that <a href=\"http://en.wikipedia.org/wiki/Posterior_predictive_distribution\"> the Posterior predictive distribution</a> is    $$p(y_\\text{new}|\\mathbf{y},\\mathbf{X},\\mathbf{x}_\\text{new})= \\int {p(y_\\text{new}|f,\\mathbf{x}_\\text{new})p(f|\\mathbf{y},\\mathbf{X}) df}$$ \n",
      "Informally, according to <a href=\"http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Marginal_distributions\">the property</a> of GP about marginalization,\n",
      "\n",
      "$$\\int {p(\\mathbf{y}_\\text{new}|\\text{f},\\mathbf{x}_\\text{new})p(\\text{f}|\\mathbf{y},\\mathbf{X}) d\\text{f}}= \\int {p(\\mathbf{y}_\\text{new}|\\mathbf{f}_\\text{new})p(\\mathbf{f}_\\text{new}|\\mathbf{y},\\mathbf{X}) d\\mathbf{f}_\\text{new}}$$.\n",
      "\n",
      "where $\\text{f}(\\mathbf{x}_\\text{new})=\\mathbf{f}_\\text{new}$, $p(\\mathbf{y}_\\text{new}|\\mathbf{f}_\\text{new})=p(\\mathbf{y}_\\text{new}|\\text{f},\\mathbf{x}_\\text{new})$ and $p(\\mathbf{f}_\\text{new}|\\mathbf{y},\\mathbf{X})=p(\\mathbf{f}_\\text{new}|\\mathbf{y},\\mathbf{X}, \\mathbf{x}_\\text{new})$ .\n",
      "\n",
      "The key difference here is that $p(f|\\mathbf{y}, \\mathbf{X})$ is a distribution in infinite dimensional space while $p(\\mathbf{f}_{new}|\\mathbf{y},\\mathbf{X})$ is a distribution in one-dimensional space.\n",
      "\n",
      "Note that $p(\\mathbf{f}_\\text{new}|\\mathbf{y},\\mathbf{X})=\\int {p(\\mathbf{f}_\\text{new}|\\text{f}) p(\\text{f}|\\mathbf{y},\\mathbf{X}) d\\text{f}}$.\n",
      "Similarly, $\\int {p(\\mathbf{f}_\\text{new}|\\text{f}) p(\\text{f}|\\mathbf{y},\\mathbf{X}) d\\text{f}}=\\int {p(\\mathbf{f}_\\text{new}|\\mathbf{f}) p(\\mathbf{f}|\\mathbf{y}) d\\mathbf{f}}$.\n",
      "Again, $p(\\mathbf{f}|\\mathbf{y})=p(\\mathbf{f}|\\mathbf{y}, \\mathbf{X})$ is a distribution in n-dimensional space while $p(\\text{f}|\\mathbf{y},\\mathbf{X})$ is distribution in infinite dimensional space.\n",
      "Informally, according to GP the following holds:\n",
      "\n",
      "$p(\\mathbf{f}_\\text{new}|\\mathbf{f})=\\frac{p(\\text{f}(\\mathbf{[\\mathbf{X};\\mathbf{x}_\\text{new}^T]}))}{p(\\text{f}(\\mathbf{\\mathbf{X}}))}$ is followed by finite-dimensional Gaussian distribution, where $[\\mathbf{X};\\mathbf{x}_\\text{new}^T]\\in \\mathcal{R}^{(n+1)\\times d}$ and $\\text{f}([\\mathbf{X};\\mathbf{x}_\\text{new}^T]) \\in \\mathcal{R}^{n+1} $\n",
      "\n",
      "Note that If $p(\\mathbf{f}|\\mathbf{y})$ is a Gaussian distribution, $p(\\mathbf{f}_{new}|\\mathbf{y},\\mathbf{X})$ has a close form. However,in classification $p(\\mathbf{f}|\\mathbf{y})$ usually is NOT a Gaussian distribution since $p(\\mathbf{f}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})$, where $p(\\mathbf{f})$ is a Gaussian distribution but $p(\\mathbf{y}|\\mathbf{f}))$ (the likelihood) is a non-Gaussian distribution.\n",
      "\n",
      "Variational inference in GPC is to approximate $p(\\mathbf{f}|\\mathbf{y})$ using a Gaussian distribution, $q(\\mathbf{f}|\\mathbf{y})$, via minimizing the KL divergence, ${\\mathrm{KL}}(Q\\|P)$. Reader may note that KL divergence is asymmetric. If we minimizing ${\\mathrm{KL}}(P\\|Q)$ instead of ${\\mathrm{KL}}(Q\\|P)$, it is about inference using <a href=\"http://en.wikipedia.org/wiki/Expectation_propagation\">expectation propagation</a> (EP) in GPC. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Variational Inference in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As mentioned in the first section, variaitonal inference in GPC is about **minimizing** the KL divergence given as below:\n",
      "\n",
      "${\\mathrm{KL}}(Q\\|P) = \\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{p(\\mathbf{f}|y)}\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f} = \\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{  \\frac{p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})}{p(\\mathbf{y})}  }\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f}=\\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{ p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f}|\\mathbf{y})  }\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f} - \\text{const}$. \n",
      "\n",
      "\n",
      "Another way to explain variational inference in GPC is to **maximize** a lower bound of the log of marginal likelihood, $\\ln (p(\\mathbf{y}|\\mathbf{X})) $.\n",
      "\n",
      "$\\ln (p(\\mathbf{y}|\\mathbf{X})) = \\ln (\\int_{-\\infty}^\\infty {p(\\mathbf{y}|\\text{f})p(\\text{f}|\\mathbf{X})} d\\text{f}) =  \\ln (\\int_{-\\infty}^\\infty {p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})} d\\mathbf{f})= \\ln (\\int_{-\\infty}^\\infty { q(\\mathbf{f}) \\frac{ p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})} {q(\\mathbf{f}|\\mathbf{y})}} d\\mathbf{f}) \\geq  (\\int_{-\\infty}^\\infty { q(\\mathbf{f}|\\mathbf{y}) \\ln ( \\frac{ p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})} {q(\\mathbf{f}|\\mathbf{y})}} ) d\\mathbf{f})$\n",
      "\n",
      "\n",
      "where the inequality is based on <a href=\"http://en.wikipedia.org/wiki/Jensen%27s_inequality\">Jensen\u2019s inequality</a>.\n",
      "\n",
      "$\\int_{-\\infty}^\\infty { q(\\mathbf{f}|\\mathbf{y}) \\ln ( \\frac{ p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})} {q(\\mathbf{f}|\\mathbf{y})}} ) d\\mathbf{f}=- \\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{ p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})  }\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f} = -E_q[\\ln(q(\\mathbf{f}|\\mathbf{y}))] + E_q[\\ln(p(\\mathbf{f}))] + E_q[\\ln(p(\\mathbf{y}|\\mathbf{f}))]$, \n",
      "\n",
      "where $E_q(\\cdot)$ is the expectation with respect to $\\mathbf{f}$.\n",
      "\n",
      "Note that the last term, $E_q[\\ln(p(\\mathbf{y}|\\mathbf{f}))]$, in GPC usually does NOT have a close form.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dealing with the non-closed form issue in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can show that $E_q[\\ln(p(\\mathbf{y}|\\mathbf{f}))]$ can be expressed in term of summation of one-dimensional integrations via the similar <a href=\"http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Marginal_distributions\">marginalization property</a> of multivariate Gaussian distribution. \n",
      "$$E_q[\\ln(p(\\mathbf{y}|\\mathbf{f}))]=E_q[\\sum_{i=1}^n {\\ln(p(\\mathbf{y}_\\text{i}|\\mathbf{f}_\\text{i})}]=\\sum_{i=1}^n {E_q[\\ln(p(\\mathbf{y}_\\text{i}|\\mathbf{f}_\\text{i})]}=\\sum_{i=1}^n {E_{q_\\text{i}}[\\ln(p(\\mathbf{y}_\\text{i}|\\mathbf{f}_\\text{i})]}$$\n",
      "where $q$ denotes $q(\\mathbf{f}|\\mathbf{y})$ is a multivariable $N(\\mu, \\Sigma)$, $q_i$ denotes $q_i(\\mathbf{f}_\\text{i}|\\mathbf{y}_\\text{i})$ is a univariate $N(\\mu_\\text{i}, \\Sigma_\\text{i,i})$, and $\\mu_\\text{i}$ and $\\Sigma_\\text{i,i}$ are the i-th element of the mean $\\mu$ and the i-th diagonal element of the covariance matrix $\\Sigma$ of $q(\\mathbf{f}|\\mathbf{y})$ respectively.\n",
      "\n",
      "Of course, $E_{q_\\text{i}}[\\ln(p(\\mathbf{y}_\\text{i}|\\mathbf{f}_\\text{i})]$ usually does not have a close form in GPC.\n",
      "One way to deal with this issue is one dimensional numerical integration.\n",
      "Another way is using some variational bounds. For now, we assume this expectation is given and we will briefly discuss how to approximate it in later session."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A toy example of variational inference"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We present a 2-D example for GP classification using variational inference. The gaussian prior and the likelihood are shown in Fig. (a) and (b). For this simple example, we can numerically compute the true posterior, shown in Fig. (c), using the Bayes rule: $p(\\mathbf{f}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})$, where $p(\\mathbf{f})$ is the prior and $p(\\mathbf{y}|\\mathbf{f}))$ is the likelihood. The approximated posterior obtained using various methods are shown in Fig. (g)-(i), along with the true distribution shown in dashed grey lines.\n",
      "\n",
      "\n",
      "\n",
      "These figures can be obtained by using the following code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Gaussian_points(Sigma,mu,xmin,xmax,ymin,ymax,delta=0.01):\n",
      "    #This function is used to generate points drawn from a Gaussian distribution\n",
      "    \n",
      "    #get a vector of Z = log(p(X,Y)), \n",
      "    #where p is a bivariate gaussian (mu, Sigma)\n",
      "    #X, Y are vectors which come from meshgrid\n",
      "\n",
      "    xlist = np.arange(xmin, xmax, delta)\n",
      "    ylist = np.arange(ymin, ymax, delta)\n",
      "    X, Y = np.meshgrid(xlist, ylist)\n",
      "    model = GaussianDistribution(mu, Sigma)\n",
      "    Z = np.zeros(len(X)*len(Y))\n",
      "    idx = 0\n",
      "    for items in zip(X,Y):\n",
      "        for sample in zip(items[0],items[1]):\n",
      "            sample = np.asarray(sample)\n",
      "            Z[idx] = model.log_pdf(sample)\n",
      "            idx += 1\n",
      "    Z = np.asarray(Z).reshape(len(X),len(Y))\n",
      "    return (X,Y,Z)\n",
      "\n",
      "def likelihood_points(X,Y,labels,likelihood):\n",
      "    #This function is used to generate points drawn from data likelihood \n",
      "    #(say, Bernoulli-logistic in this example)\n",
      "    \n",
      "    #get a vector of Z = log(p(X,Y,labels)), \n",
      "    #where p comes from likelihood\n",
      "    Z = np.zeros(len(X)*len(Y))\n",
      "    idx = 0\n",
      "    for items in zip(X,Y):\n",
      "        for sample in zip(items[0],items[1]):\n",
      "            sample = np.asarray(sample)\n",
      "            lpdf = likelihood.get_log_probability_f(labels, sample).sum()\n",
      "            Z[idx] = lpdf\n",
      "            idx += 1\n",
      "    Z = np.asarray(Z).reshape(len(X),len(Y))\n",
      "    return Z\n",
      "\n",
      "def approx_posterior_plot(methods, kernel_func, features, mean_func, \n",
      "                          labels, likelihoods, kernel_log_scale, \n",
      "                          xmin, xmax, ymin, ymax, delta, plots):\n",
      "    #This function is used to generate points drawn from approximated posterior and plot them\n",
      "        \n",
      "    #@methods a list of methods used to approximate the posterior\n",
      "    #@kernel_func a covariance function for GP\n",
      "    #@features X\n",
      "    #@mean_func a mean function for GP\n",
      "    #@labels Y\n",
      "    #likelihood a data likelihood to model labels\n",
      "    #@kernel_log_scale a hyper-parameter of covariance function\n",
      "    #@xmin, @ymin @xmax, @ymax @delta are used in linespace function\n",
      "    #maximum and minimum values used to generate points from an approximated posterior\n",
      "    #@plots\n",
      "    (rows, cols) = plots.shape\n",
      "    methods = np.asarray(methods).reshape(rows, cols)\n",
      "    likelihoods = np.asarray(likelihoods).reshape(rows, cols)\n",
      "    for r in xrange(rows):\n",
      "        for c in xrange(cols):\n",
      "            inference = methods[r][c]\n",
      "            likelihood = likelihoods[r][c]\n",
      "            inf = inference(kernel_func, features, mean_func, labels, likelihood())\n",
      "            inf.set_scale(exp(kernel_log_scale))\n",
      "            #get the approximated Gaussian distribution\n",
      "            mu = inf.get_posterior_mean()\n",
      "            Sigma = inf.get_posterior_covariance()\n",
      "            #normalized approximated posterior\n",
      "            (X,Y,Z) = Gaussian_points(Sigma, mu, xmin, xmax, ymin, ymax, delta)\n",
      "            plots[r][c].contour(X, Y, np.exp(Z))\n",
      "            plots[r][c].set_title('posterior via %s'%inf.get_name())\n",
      "            plots[r][c].axis('equal')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#a toy 2D example (data)\n",
      "x=np.asarray([sqrt(2),-sqrt(2)]).reshape(1,2)\n",
      "y=np.asarray([1,-1])\n",
      "\n",
      "features = RealFeatures(x)\n",
      "labels = BinaryLabels(y)\n",
      "kernel_log_sigma = 1.0\n",
      "kernel_log_scale = 1.5\n",
      "\n",
      "#a mean function and a covariance function for GP\n",
      "mean_func = ConstMean()\n",
      "#using log_sigma as a hyper-parameter of GP instead of sigma\n",
      "kernel_sigma = 2*exp(2*kernel_log_sigma)\n",
      "kernel_func = GaussianKernel(10, kernel_sigma)\n",
      "kernel_func.init(features, features)\n",
      "#a prior distribution derived from GP via applying the mean function and the covariance function to data\n",
      "Sigma = kernel_func.get_kernel_matrix()\n",
      "Sigma = Sigma * exp(2.0*kernel_log_scale)\n",
      "mu = mean_func.get_mean_vector(features)\n",
      "\n",
      "delta = 0.1\n",
      "xmin = -4\n",
      "xmax = 6\n",
      "ymin = -6\n",
      "ymax = 4\n",
      "\n",
      "#a prior (Gaussian) derived from GP\n",
      "(X,Y,Z1) = Gaussian_points(Sigma, mu, xmin, xmax, ymin, ymax, delta)\n",
      "\n",
      "col_size = 6\n",
      "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(col_size*3,col_size))\n",
      "\n",
      "ax1.contour(X, Y, np.exp(Z1))\n",
      "ax1.set_title('prior')\n",
      "ax1.axis('equal')\n",
      "\n",
      "#likelihood (inverse logit, A.K.A. Bernoulli-logistic)\n",
      "#likelihoods class for inference methods \n",
      "likelihoods=[\n",
      "LogitLikelihood,\n",
      "LogitLikelihood,\n",
      "LogitVGLikelihood,\n",
      "LogitVGLikelihood,\n",
      "LogitVGLikelihood,\n",
      "LogitDVGLikelihood\n",
      "]\n",
      "\n",
      "#using LogitLikelihood as the likelihood\n",
      "#Note that likelihoods in the likelihoods list are extensions of LogitLikelihood\n",
      "#Aslo see http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CVariationalLikelihood.html\n",
      "Z2 = likelihood_points(X,Y,labels,LogitLikelihood())\n",
      "ax2.contour(X, Y, np.exp(Z2))\n",
      "ax2.set_title('likelihood')\n",
      "ax2.axis('equal')\n",
      "\n",
      "#a unnormalized true posterior (non-Gaussian)\n",
      "Z3 = Z1+Z2\n",
      "ax3.contour(X, Y, np.exp(Z3))\n",
      "ax3.set_title('true posterior')\n",
      "ax3.axis('equal')\n",
      "\n",
      "f, plots =plt.subplots(2, 3, figsize=(col_size*3,col_size*2))\n",
      "\n",
      "#Inference methods used to approximate a posterior\n",
      "#Also see http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CInferenceMethod.html\n",
      "methods=[\n",
      "SingleLaplacianInferenceMethod,\n",
      "SingleLaplacianInferenceMethodWithLBFGS,\n",
      "KLApproxDiagonalInferenceMethod,\n",
      "KLCovarianceInferenceMethod,\n",
      "KLCholeskyInferenceMethod,\n",
      "KLDualInferenceMethod\n",
      "]\n",
      "\n",
      "approx_posterior_plot(methods, kernel_func, features, mean_func, labels, likelihoods, kernel_log_scale, \n",
      "                      xmin, xmax, ymin, ymax, delta, plots)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remark:\n",
      "The true posterior is non-Gaussian and the normalized constant usually is difficult to compute.\n",
      "All approximated posteriors are Gaussian. A posterior Gaussian distribution is required to perform inference and predictions.\n",
      "Laplace approximation is not ideal in term of approximating a posterior and making predictions although it is fast.\n",
      "With the price of speed, variational methods such as KLCovariance and KLCholesky offer more accurate approximations compared against Laplace method. KLDual method also is used to get a good approximation and in most cases is better than Laplace method.\n",
      "On the other hand, KLApproxDiagonal method is as fast as Laplace method and for certain datasets KLApproxDiagonal method is more accurate than Laplace method in term of making predictions. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A real-world example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We apply variational methods to the sonar data set, which can be found at <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/\">here</a>. This is a binary classification problem. \n",
      "\n",
      "The class \"sonar.mines\" contains 111 data points  obtained by bouncing sonar\n",
      "signals off a metal cylinder at various angles and under various\n",
      "conditions.  The class \"sonar.rocks\" contains 97 data points obtained from\n",
      "rocks under similar conditions.  The transmitted sonar signal is a\n",
      "frequency-modulated chirp, rising in frequency.  The data set contains\n",
      "signals obtained from a variety of different aspect angles, spanning 90\n",
      "degrees for the cylinder and 180 degrees for the rock.\n",
      "\n",
      "Each data point is a set of 60 features in the range 0.0 to 1.0.  Each feature\n",
      "represents the energy within a particular frequency band, integrated over\n",
      "a certain period of time.  The integration aperture for higher frequencies\n",
      "occur later in time, since these frequencies are transmitted later during\n",
      "the chirp.\n",
      "\n",
      "The label associated with each record contains the letter \"R\" if the object\n",
      "is a rock and \"M\" if it is a mine (metal cylinder).  The features in the\n",
      "labels are in increasing order of aspect angle, but they do not encode the\n",
      "angle directly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learning(inference, linesearch, likelihood, x_train, x_test, y_train, y_test, kernel_log_sigma, kernel_log_scale):\n",
      "    #using an inference method to train a GP classifer \n",
      "    mean_func = ZeroMean()\n",
      "    kernel_sigma = 2*exp(2*kernel_log_sigma);\n",
      "    kernel_func = GaussianKernel(10, kernel_sigma)\n",
      "\n",
      "    #Y is a sample-by-1 vector\n",
      "    labels_train = BinaryLabels(y_train)\n",
      "    labels_test = BinaryLabels(y_test)\n",
      "    #X is a feature-by-sample matrix\n",
      "    features_train=RealFeatures(x_train)\n",
      "    features_test=RealFeatures(x_test)\n",
      "\n",
      "    inf = inference(kernel_func, features_train, mean_func, labels_train, likelihood)\n",
      "    inf.set_scale(exp(kernel_log_scale))\n",
      "    try:\n",
      "        #setting lbfgs parameters\n",
      "        inf.set_lbfgs_parameters(100,2000,int(linesearch),2000)\n",
      "        #used to make sure the kernel matrix is positive definite\n",
      "        inf.set_noise_factor(1e-6)\n",
      "        inf.set_min_coeff_kernel(1e-5)\n",
      "        inf.set_max_attempt(10)\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    gp = GaussianProcessClassification(inf)\n",
      "    gp.train()\n",
      "    prob=gp.get_probabilities(features_test)\n",
      "\n",
      "    return prob, inf.get_name()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "labels={\n",
      "    \"m\":1,\n",
      "    \"r\":-1,\n",
      "       }\n",
      "from math import log\n",
      "def extract(inf, train_size):\n",
      "    #processing raw data\n",
      "    random.seed(1)\n",
      "    x=[]\n",
      "    y=[]\n",
      "    for line in open(inf):\n",
      "        line=line.strip()\n",
      "        info=line.split(',')\n",
      "        label=labels[info[-1].lower()]\n",
      "        x.append(map(float, info[:-1]))\n",
      "        y.append(label)\n",
      "        \n",
      "    #train_size should be less than the size of all available data points\n",
      "    assert train_size < len(x) \n",
      "    idx=range(len(y))\n",
      "    random.shuffle(idx)\n",
      "    train_idx=set(idx[:train_size])\n",
      "    test_idx=set(idx[train_size:])\n",
      "    x_train = np.asarray([value for (idx, value) in enumerate(x) if idx in train_idx]).T\n",
      "    y_train = np.asarray([label for (idx, label) in enumerate(y) if idx in train_idx])\n",
      "    x_test = np.asarray([value for (idx, value) in enumerate(x) if idx in test_idx]).T\n",
      "    y_test = np.asarray([label for (idx, label) in enumerate(y) if idx in test_idx])\n",
      "\n",
      "    y_train_positive=(y_train==1).sum()\n",
      "    y_train_negative=(y_train==-1).sum()\n",
      "    y_test_positive=(y_test==1).sum()\n",
      "    y_test_negative=(y_test==-1).sum()\n",
      "\n",
      "    prb_positive=float(y_train_positive)/len(y_train)\n",
      "    B=float(y_test_positive)*log(prb_positive,2)+float(y_test_negative)*log(1.0-prb_positive,2)\n",
      "    B=-B/len(y_test)\n",
      "\n",
      "    return x_train, y_train, x_test, y_test, B"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def approx_bit_plot(methods, linesearchs, likelihoods, plots, lScale, lSigma):\n",
      "    #used to plot information bit figures\n",
      "    #Note that information bit is used to measure effectiveness of an inference method\n",
      "    if len(plots.shape)==1:\n",
      "        rows=1\n",
      "        cols=plots.shape[0]\n",
      "    else:\n",
      "        (rows, cols) = plots.shape\n",
      "\n",
      "    methods=np.asarray(methods).reshape(rows, cols)\n",
      "    linesearchs=np.asarray(linesearchs).reshape(rows, cols)\n",
      "    likelihoods=np.asarray(likelihoods).reshape(rows, cols) \n",
      "\n",
      "    for r in xrange(rows):\n",
      "        for c in xrange(cols):\n",
      "            inference = methods[r][c]\n",
      "            linesearch = linesearchs[r][c]\n",
      "            likelihood = likelihoods[r][c]\n",
      "            try:\n",
      "                likelihood.set_noise_factor(1e-15)\n",
      "                likelihood.set_strict_scale(0.01)\n",
      "            except:\n",
      "                pass\n",
      "            scores=[]\n",
      "            for items in zip(lScale, lSigma):\n",
      "                for parameters in zip(items[0],items[1]):\n",
      "                    lscale=parameters[0]\n",
      "                    lsigma=parameters[1]\n",
      "                    (prob, inf_name)=learning(inference, linesearch, likelihood, x_train, x_test, y_train, y_test, \n",
      "                                              lscale, lsigma)\n",
      "                    #compute the information bit\n",
      "                    score=0.0\n",
      "                    for (label_idx, prb_idx) in zip(y_test, prob):\n",
      "                        if label_idx==1:\n",
      "                            score+=(1.0+label_idx)*log(prb_idx,2)\n",
      "                        elif label_idx==-1:\n",
      "                            score+=(1.0-label_idx)*log(1.0-prb_idx,2)\n",
      "                        else:\n",
      "                            score+=(1.0+label_idx)*log(prb_idx,2)+(1.0-label_idx)*log(1.0-prb_idx,2)\n",
      "                    score=score/(2.0*len(y_test))+B\n",
      "                    scores.append(score)\n",
      "            scores=np.asarray(scores).reshape(len(lScale),len(scores)/len(lScale))\n",
      "\n",
      "            if len(plots.shape)==1:\n",
      "                sub_plot=plots\n",
      "            else:\n",
      "                sub_plot=plots[r]\n",
      "            CS =sub_plot[c].contour(lScale, lSigma, scores)\n",
      "            sub_plot[c].clabel(CS, inline=1, fontsize=10)\n",
      "            sub_plot[c].set_title('information bit for %s'%inf_name)\n",
      "            sub_plot[c].set_xlabel('log_scale')\n",
      "            sub_plot[c].set_ylabel('log_sigma')\n",
      "            sub_plot[c].axis('equal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_size=108\n",
      "(x_train, y_train, x_test, y_test, B)=extract('./sonar.all-data', train_size)\n",
      "inference_methods =[\n",
      "                  EPInferenceMethod,\n",
      "                  KLDualInferenceMethod,\n",
      "                  SingleLaplacianInferenceMethod,\n",
      "                  KLApproxDiagonalInferenceMethod,\n",
      "                  KLCholeskyInferenceMethod,\n",
      "                  KLCovarianceInferenceMethod,\n",
      "                  ]\n",
      "\n",
      "likelihoods =[\n",
      "            LogitLikelihood(),\n",
      "            LogitDVGLikelihood(), #KLDual method uses a likelihood class that supports dual variational inference\n",
      "            LogitLikelihood(),\n",
      "            LogitVGLikelihood(), #KL method uses a likelihood class that supports variational inference\n",
      "            LogitVGLikelihood(), #KL method uses a likelihood class that supports variational inference\n",
      "            LogitVGLikelihood(), #KL method uses a likelihood class that supports variational inference\n",
      "            ]\n",
      "\n",
      "linesearchs =[\n",
      "            3,\n",
      "            1, #KLDual method using the type 3 line_search method will cause an error during computing the information bit\n",
      "            3,\n",
      "            3,\n",
      "            3,\n",
      "            3,\n",
      "            ]\n",
      "\n",
      "col_size=8\n",
      "lscale_min=0.0\n",
      "lscale_max=5.0\n",
      "lsigma_min=0.0\n",
      "lsigma_max=5.0\n",
      "delta=0.5\n",
      "scale=5.0\n",
      "\n",
      "lscale_list = np.arange(lscale_min, lscale_max, delta)\n",
      "lsigma_list = np.arange(lsigma_min, lsigma_max, delta*scale)\n",
      "lScale, lSigma = np.meshgrid(lscale_list, lsigma_list)\n",
      "f, plots =plt.subplots(2, 3, figsize=(col_size*3,col_size*2))\n",
      "\n",
      "approx_bit_plot(inference_methods, linesearchs, likelihoods, plots, lScale, lSigma)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the information bit is used to measure the accuracy of classification. \n",
      "If there is a perfect classification, the information bit should be 1.\n",
      "\n",
      "For these figures, Laplace method is worst among these methods in term of information bit. EP method, KLCholesky, KLCovariance and suprisely KLApproxDiagonal are equally good."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Another real-world example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the US Postal Service(USPS) database of handwritten digits as another example.\n",
      "\n",
      "The dataset refers to numeric data obtained from the scanning of handwritten digits from envelopes by the U.S. Postal Service. The original scanned digits are binary and of different sizes and orientations; the images here have been deslanted and size normalized, resulting in 16 x 16 grayscale images.\n",
      "\n",
      "We consider to classify the images of digit 3 from images of digit 5 in this example, which means only images of digit 3 and digit 5 are used. Note that this is example is also used in session 3.7.3 of the textbook,Gaussian Processes for Machine Learning."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#extract trainning set and test set from the dataset\n",
      "def binary_extract(idx, features, labels):\n",
      "        #binary classification\n",
      "        assert len(idx) == 2\n",
      "        positive_idx = (labels[idx[0],:] == 1)\n",
      "        negative_idx = (labels[idx[1],:] == 1)\n",
      "        binary_idx = (positive_idx | negative_idx)\n",
      "        ds = binary_idx.shape[-1]\n",
      "\n",
      "        bin_labels = np.zeros(ds)\n",
      "        bin_labels[positive_idx] = 1\n",
      "        bin_labels[negative_idx] = -1\n",
      "\n",
      "        binary_features = (features[:,binary_idx])\n",
      "        binary_labels = (bin_labels[binary_idx])\n",
      " \n",
      "        positive_count = bin_labels[positive_idx].shape[-1]\n",
      "        negative_count = bin_labels[negative_idx].shape[-1]\n",
      "        binary_count = binary_labels.shape[-1]\n",
      " \n",
      "        print \"There are %d positive samples and %d negative samples\" %(positive_count, negative_count)\n",
      "        return binary_features, binary_labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learning(inference, likelihood, x_train, x_test, y_train, y_test):\n",
      "    #train a GP classifer\n",
      "    error_eval = ErrorRateMeasure()\n",
      "    mean_func = ConstMean()\n",
      "    kernel_log_sigma = 1.0\n",
      "    kernel_sigma = 2*exp(2*kernel_log_sigma);\n",
      "    kernel_func = GaussianKernel(10, kernel_sigma)\n",
      "\n",
      "    #sample by 1\n",
      "    labels_train = BinaryLabels(y_train)\n",
      "    labels_test = BinaryLabels(y_test)\n",
      "    #feature by sample\n",
      "    features_train=RealFeatures(x_train)\n",
      "    features_test=RealFeatures(x_test)\n",
      "\n",
      "    kernel_log_scale = 1.0\n",
      "\n",
      "    inf = inference(kernel_func, features_train, mean_func, labels_train, likelihood)\n",
      "    print \"\\nusing %s\"%inf.get_name()\n",
      "    \n",
      "    inf.set_scale(exp(kernel_log_scale))\n",
      "    try:\n",
      "        inf.set_lbfgs_parameters(100,80,0,80)\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    start = time.time()\n",
      "    gp = GaussianProcessClassification(inf)\n",
      "    gp.train()\n",
      "    end = time.time()\n",
      "    print \"cost %.2f seconds at training\"%(end-start)\n",
      "    nlz=inf.get_negative_log_marginal_likelihood()\n",
      "    print \"the negative_log_marginal_likelihood is %.4f\"%nlz\n",
      "    start = time.time()\n",
      "    #classification on train_data\n",
      "    pred_labels_train = gp.apply_binary(features_train)\n",
      "    #classification on test_data\n",
      "    pred_labels_test = gp.apply_binary(features_test)\n",
      "    end = time.time()    \n",
      "    print \"cost %.2f seconds at prediction\"%(end-start)\n",
      "    \n",
      "    error_train = error_eval.evaluate(pred_labels_train, labels_train)\n",
      "    error_test = error_eval.evaluate(pred_labels_test, labels_test)\n",
      "    \n",
      "    print \"Train error : %.2f Test error: %.2f\\n\" % (error_train, error_test)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inf='./usps_resampled.mat'\n",
      "data=scipy.io.loadmat(inf)\n",
      "train_labels=data['train_labels']\n",
      "test_labels=data['test_labels']\n",
      "train_features=data['train_patterns']\n",
      "test_features=data['test_patterns']\n",
      "#using images of digit 3 and digit 5 from the dataset\n",
      "idx=[3,5]\n",
      "#Note that \n",
      "#y_train and y_test are followed the definition in the first session \n",
      "#the transpose of x_train and x_test are followed the definition in the first session\n",
      "print \"Training set statistics\"\n",
      "(x_train, y_train)=binary_extract(idx,train_features, train_labels)\n",
      "print \"Test set statistics\"\n",
      "(x_test, y_test)=binary_extract(idx,test_features, test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Laplace Method in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://en.wikipedia.org/wiki/Laplace%27s_method\">Laplace method</a> in GPC can be viewed as a special case of variational method.\n",
      "The idea of Laplace method is to use the second-order <a href=\"http://en.wikipedia.org/wiki/Taylor_series\">Taylor approximation</a> in the <a href=\"http://en.wikipedia.org/wiki/Mode_%28statistics%29\">mode</a>, $\\mathbf{\\hat{f}}$ of $p(\\mathbf{f}|\\mathbf{y})$ to approximate $p(\\mathbf{f}|\\mathbf{y})$, which is given below:\n",
      "\n",
      "$p(\\mathbf{f}|\\mathbf{y}) \\approx \\hat{p}(\\mathbf{f}|\\mathbf{y})$ followed by $N(\\mathbf{\\hat{f}}, H_{\\mathbf{\\hat{f}}}^{-1})$ ,where $H_{\\mathbf{\\hat{f}}}$ is the <a href=\"http://en.wikipedia.org/wiki/Hessian_matrix\">Hessian matrix</a> in $\\mathbf{\\hat{f}}$\n",
      "\n",
      "Therefore, the KL divergence, ${\\mathrm{KL}}(Q\\|P)$, is approximated by $\\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{\\hat{p}(\\mathbf{f}|y)}\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f}$.\n",
      "\n",
      "By **minimizing** $\\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{\\hat{p}(\\mathbf{f}|y)}\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f}$\n",
      "we can get $q(\\mathbf{f}|\\mathbf{y})= \\hat{p}(\\mathbf{f}|\\mathbf{y})$.\n",
      "In practice, we can use <a href=\"http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\">Newton-Raphson</a> optimizer or <a href=\"http://en.wikipedia.org/wiki/Quasi-Newton_method\">Quasi-Newton</a> optimizers(ie, <a href=\"http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\">Limited-memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno</a> (LBFGS)) to find $\\mathbf{\\hat{f}}$.\n",
      "Since the likelihood is inverse-logit, we can show that the objective function, $\\ln(p(\\mathbf{f}|\\mathbf{y}))$, is strictly <a href=\"http://en.wikipedia.org/wiki/Concave\">concave</a>, which means in theory these optimizers can find the same global solution $\\mathbf{\\hat{f}}$.\n",
      "We demonstrate how to apply Laplace method via Newton-Raphson optimizer and LBFGS optimizer in Shogun to the USPS data set as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inference_methods=[\n",
      "                  SingleLaplacianInferenceMethodWithLBFGS,     #using LBFGS optimizer\n",
      "                  SingleLaplacianInferenceMethod,              #using Newton-Raphson optimizer\n",
      "                  ]\n",
      "likelihood = LogitLikelihood()\n",
      "for inference in inference_methods:\n",
      "    learning(inference, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remark: Laplace method is the fastest method among all implemented variational methods in Shogun in practice if Laplace method is considered as a special case of variational inference. However, the second-order Taylor approximation in the mode of marginal likelihood is not always a good approximation, which can be observed in the figures in the previous session for the sonar data set."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Covariance variational method in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This method is mentioned in <a href=\"http://jmlr.org/papers/volume9/nickisch08a/nickisch08a.pdf\">the paper</a> of Nickisch and Rasmussen in 2008, which is called the KL method in the paper.\n",
      "The idea of this method is to maximize $E_q[ln(p(\\mathbf{f}))] + E_q[ln(p(\\mathbf{y}|\\mathbf{f}))] - E_q[ln(q(\\mathbf{f}|\\mathbf{y}))]$ with respect to $\\mu$ and $\\Sigma$, by reparametrizing $\\Sigma$ to reduce the dimension of variable to be optimized.\n",
      "However, such parametrization does not preserve convexity (concave in this setting) according to <a href=\"http://arxiv.org/abs/1306.1052\">the paper</a> of Khan et. al. in 2013. \n",
      "Except for this method, convexity holds in other variational methods implemented in Shogun. \n",
      "\n",
      "We demonstrate how to apply this variational method in Shogun to the USPS data set as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likelihood = LogitVGLikelihood()\n",
      "learning(KLCovarianceInferenceMethod, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remark: This method may be the first variational method used in GPC. However, it is the slowest method among all implemented variational methods in Shogun in practice."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Mean-field variational method in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This method is also called factorial variational method in <a href=\"http://jmlr.org/papers/volume9/nickisch08a/nickisch08a.pdf\">the paper</a> of Nickisch and Rasmussen in 2008.\n",
      "The idea of mean-field variatonal method is to enforce artificial structure on the co-variance matrix, $\\Sigma$ of $q(\\mathbf{f})$.\n",
      "In mean-field variatonal method, $\\Sigma$ is a diagonal positive-definite matrix.\n",
      "We demonstrate how to apply this variational method in Shogun to the USPS data set as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likelihood = LogitVGLikelihood()\n",
      "learning(KLApproxDiagonalInferenceMethod, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remark: This method could be as fast as the Laplace method in Shogun in practice but ignores all off-diagonal elements of the covariance matrix. However, in some case (ie, there are a lot of noise in data set), such structure regularization may bring some promising result compared to other variational methods, which can be observed in the figures in the previous session for the sonar data set."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Cholesky variational method in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This method is proposed in <a href=\"http://jmlr.org/proceedings/papers/v15/challis11a/challis11a.pdf\">the paper</a> of Challis and Barber in 2011.\n",
      "The idea of this method is to maximize $E_q[\\ln(p(\\mathbf{f}))] + E_q[\\ln(p(\\mathbf{y}|\\mathbf{f}))] - E_q[\\ln(q(\\mathbf{f}|\\mathbf{y}))]$ in terms of the Cholesky representation, C, for the covariance matrix of $q(\\mathbf{f})$, where $\\Sigma=CC^T$ and C is a lower triangular matrix.\n",
      "We demonstrate how to apply this variational method in Shogun to the USPS data set as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likelihood = LogitVGLikelihood()\n",
      "learning(KLCholeskyInferenceMethod, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remark: This method may be faster to learn the complete structure of the covariance matrix from data than covariance variational method.\n",
      "The reason is solving linear system related to $\\Sigma$ is required at each optimization step.\n",
      "In Cholesky representation, the time complexity at each optimization step is reduced to O(n^2) from O(n^3) compared to covariance variational method.\n",
      "However, the overall time complexity is still O(n^3)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Dual variational method in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This method is proposed in <a href=\"http://arxiv.org/abs/1306.1052\">the paper</a> of Khan et. al. in 2013.\n",
      "The idea of this method is to optimize $E_q[ln(p(\\mathbf{f}))] + E_q[ln(p(\\mathbf{y}|\\mathbf{f}))] - E_q[ln(q(\\mathbf{f}|\\mathbf{y}))]$ in <a href=\"http://en.wikipedia.org/wiki/Duality_%28optimization%29\">Lagrange dual</a> form instead of the primal form used in covariance variational method via explicitly expressing constraint in the covariance matrix, $\\Sigma$, in terms of auxiliary variable.\n",
      "However, the time complexity of each optimization step is still O(n^3) and variational bound usually is required to approximate $E_{q_i}[ln(p(\\mathbf{y_i}|\\mathbf{f_i})]$ for GPC in this method.\n",
      "We demonstrate how to apply this variational method in Shogun to the USPS data set as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likelihood = LogitDVGLikelihood()\n",
      "likelihood.set_strict_scale(0.1)\n",
      "learning(KLDualInferenceMethod, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remark: This method requires less memory than the Cholesky method, which is important in large-scale case.\n",
      "A promising observation in practice is that this method is faster than covariance variational method and may not be slower than the Cholesky method. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Variational bounds"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we discuss how to approximate $E_{q_i}[ln(p(\\mathbf{y_i}|\\mathbf{f_i})]$.\n",
      "Since this term is about one-dimensional integration, for GPC we can use <a href=\"http://en.wikipedia.org/wiki/Gauss%E2%80%93Hermite_quadrature\">Gauss\u2013Hermite quadrature</a> to appromxiate this term. In Shogun, the corresponding implementation of Bernoulli-logistic likelihood for variational inference is called CLogitVGLikelihood. This likelihood can be used for all variational methods except the dual variational method.\n",
      "\n",
      "The piecewise variational bound proposed at <a href=\"http://www.icml-2011.org/papers/376_icmlpaper.pdf\">the paper</a> of Benjamin M. Marlin et. al. in 2011 is also implemented in Shogun, which is called CLogitVGPiecewiseBoundLikelihood. This likelihood can be used for all variational methods except the dual variational method.\n",
      "\n",
      "For dual variational method, we use the variational bound in <a href=\"http://www.cs.cmu.edu/~lafferty/pub/ctm.pdf\">the paper</a> of DM Blei et. al. in 2007. The corresponding implementation in Shogun is called CLogitDVGLikelihood. Note that when using CLogitDVGLikelihood, the bound is only enabled for the dual variational method. When other variational methods use this class, it uses one-dimensional integration instead of the variational bound.\n",
      "\n",
      "For detailed discussion about variational bounds in GPC, please refer to <a href=\"https://circle.ubc.ca/handle/2429/43640\">Khan's work</a>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likelihood = LogitVGPiecewiseBoundLikelihood()\n",
      "likelihood.set_default_variational_bound()\n",
      "likelihood.set_noise_factor(1e-15)\n",
      "inference_methods=[\n",
      "                  KLCholeskyInferenceMethod,\n",
      "                  KLApproxDiagonalInferenceMethod,\n",
      "                  KLCovarianceInferenceMethod,\n",
      "                  ]\n",
      "for inference in inference_methods:\n",
      "    learning(inference, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Optimizing hyper-parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We demonstrate how to optimize hypyer-parameters in Shogun for applying the mean-field variational method to the USPS data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learning2(inference, likelihood, x_train, x_test, y_train, y_test):\n",
      "\n",
      "    error_eval = ErrorRateMeasure()\n",
      "    mean_func = ZeroMean()\n",
      "    kernel_log_sigma = 1.0\n",
      "    kernel_sigma = 2*exp(2*kernel_log_sigma);\n",
      "    kernel_func = GaussianKernel(10, kernel_sigma)\n",
      "\n",
      "    #sample by 1\n",
      "    labels_train = BinaryLabels(y_train)\n",
      "    labels_test = BinaryLabels(y_test)\n",
      "    #feature by sample\n",
      "    features_train=RealFeatures(x_train)\n",
      "    features_test=RealFeatures(x_test)\n",
      "\n",
      "    kernel_log_scale = 1.0\n",
      "\n",
      "    inf = inference(kernel_func, features_train, mean_func, labels_train, likelihood)\n",
      "    print \"\\nusing %s\"%inf.get_name()\n",
      "    \n",
      "    inf.set_scale(exp(kernel_log_scale))\n",
      "    try:\n",
      "            inf.set_lbfgs_parameters(100,80,3,80)\n",
      "    except:\n",
      "            pass\n",
      "\n",
      "    gp = GaussianProcessClassification(inf)\n",
      "\n",
      "    # evaluate our inference method for its derivatives\n",
      "    grad = GradientEvaluation(gp, features_train, labels_train, GradientCriterion(), False)\n",
      "    grad.set_function(inf)\n",
      "\n",
      "    # handles all of the above structures in memory\n",
      "    grad_search = GradientModelSelection(grad)\n",
      "\n",
      "    # search for best parameters and store them\n",
      "    best_combination = grad_search.select_model()\n",
      "\n",
      "    # apply best parameters to GP\n",
      "    best_combination.apply_to_machine(gp)\n",
      "\n",
      "    # we have to \"cast\" objects to the specific kernel interface we used (soon to be easier)\n",
      "    best_width=GaussianKernel.obtain_from_generic(inf.get_kernel()).get_width()\n",
      "    best_scale=inf.get_scale()\n",
      "    print \"Selected kernel bandwidth:\", best_width\n",
      "    print \"Selected kernel scale:\", best_scale\n",
      "\n",
      "    start = time.time()\n",
      "    gp.train()\n",
      "    end = time.time()\n",
      "    print \"cost %s seconds at training\"%(end-start)\n",
      "    nlz=inf.get_negative_log_marginal_likelihood()\n",
      "    print \"the negative_log_marginal_likelihood is %.4f\"%nlz\n",
      "    start = time.time()\n",
      "    #classification on train_data\n",
      "    pred_labels_train = gp.apply_binary(features_train)\n",
      "    #classification on test_data\n",
      "    pred_labels_test = gp.apply_binary(features_test)\n",
      "    end = time.time()    \n",
      "    print \"cost %s seconds at prediction\"%(end-start)\n",
      "    \n",
      "    error_train = error_eval.evaluate(pred_labels_train, labels_train)\n",
      "    error_test = error_eval.evaluate(pred_labels_test, labels_test)\n",
      "    \n",
      "    print \"Train error : %.2f Test error: %.2f\\n\" % (error_train, error_test);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likelihood = LogitVGLikelihood()\n",
      "likelihood.set_noise_factor(1e-15)\n",
      "learning2(KLApproxDiagonalInferenceMethod, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Soon to Come"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Large-scale inference in GPC"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}