{
 "metadata": {
  "name": "FGM"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Factor Graph Model with Shogun Machine Learning Toolbox"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Shell Hu (dom343@gmail.com) https://github.com/hushell"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this demo, we show how to train a factor graph model using structured SVM. We will go through the basic knowledge of factor graph and structured output learning. Next, the corresponding APIs in Shogun will be covered. For testing the scalability, we show an experiment on a real OCR dataset for handwritten chracter recognition. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Factor Graph"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Factor graphs are an explicit factorization representation of undirected graphical models in terms of a set of factors (potentials), each of which is defined on a clique in the orginal graph [1]. For example, a MRF distribution can be factorized as \n",
      "\n",
      "$$\n",
      "P(\\mathbf{y}) = \\frac{1}{Z} \\prod_{F \\in \\mathcal{F}} \\theta_F(\\mathbf{y}_F),\n",
      "$$\n",
      "\n",
      "where $F$ is the factor index, $\\theta_F(\\mathbf{y}_F)$ is the energy wrt assignment $\\mathbf{y}_F$. In this demo, we focus only on table representation of factors. Namely, each factor holds a energy table $\\theta_F$, which can be viewed as an unnormalized CPD. According to different factorization, there are different types of factors. Usually we assume the Markovian property is hold, that is, factors have the same parameterization if they belong to the same type, no matter how location or time changes. In addition, we have parameter free factor type, but nothing to learn for such kinds of types. More detailed implementation will be explained later."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Structured Prediction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Structured prediction typically involves an input $\\mathbf{x}$ (can be structured) and a structured output $\\mathbf{y}$. A joint feature map $\\Phi(\\mathbf{x},\\mathbf{y})$ is defined to incoporate structure information into the labels, such as chains, trees or general graphs. In general, the linear parameterization will be used to give the prediction rule. We leave the kernelized version for future work.\n",
      "\n",
      "$$\n",
      "\\hat{\\mathbf{y}} = \\underset{\\mathbf{y} \\in \\mathcal{Y}}{\\operatorname{argmax}} \\langle \\mathbf{w}, \\Phi(\\mathbf{x},\\mathbf{y}) \\rangle \n",
      "$$\n",
      "\n",
      "where $\\Phi(\\mathbf{x},\\mathbf{y})$ is the feature vector by mapping local factor features to corresponding locations in terms of $\\mathbf{y}$, and $\\mathbf{w}$ is the global parameter vector. In factor graph model, parameters are associated with a set of factor types. So $\\mathbf{w}$ is a collection of local parameters. \n",
      "\n",
      "The parameters are learned by regularized risk minimization, where the risk defined by user provided loss function $\\delta(\\mathbf{y},\\mathbf{\\hat{y}})$ is usually non-convex and non-differentiable. So the empirical risk is defined in terms of the surrogate hinge loss $H_i(\\mathbf{w}) = \\max_{\\mathbf{y} \\in \\mathcal{Y}} \\delta(\\mathbf{y}_i,\\mathbf{y}) - \\langle \\mathbf{w}, \\Psi_i(\\mathbf{y}) \\rangle $, which is an upper bound of the user defined loss. Here $\\Psi_i(\\mathbf{y}) = \\Phi(\\mathbf{x}_i,\\mathbf{y}_i) - \\Phi(\\mathbf{x}_i,\\mathbf{y})$. The training objective is given by\n",
      "\n",
      "$$\n",
      "\\min_{\\mathbf{w}} \\frac{\\lambda}{2} ||\\mathbf{w}||^2 + \\frac{1}{N} \\sum_{i=1}^N H_i(\\mathbf{w}). \n",
      "$$\n",
      "\n",
      "In Shogun's factor graph model, the corresponding implemented functions are:\n",
      "\n",
      "- <a href=\"http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CStructuredModel.html#a15bd99e15bbf0daa8a727d03dbbf4bcd\">FactorGraphModel::get_joint_feature_vector()</a> $\\longleftrightarrow \\Phi(\\mathbf{x}_i,\\mathbf{y})$ \n",
      "\n",
      "- <a href=\"http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CFactorGraphModel.html#a36665cfdd7ea2dfcc9b3c590947fe67f\">FactorGraphModel::argmax()</a> $\\longleftrightarrow H_i(\\mathbf{w})$\n",
      "\n",
      "- <a href=\"http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CFactorGraphModel.html#a17dac99e933f447db92482a6dce8489b\">FactorGraphModel::delta_loss()</a> $\\longleftrightarrow \\delta(\\mathbf{y}_i,\\mathbf{y})$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Experiment: OCR"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Show Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First of all, we load the OCR data from a prepared mat file. The raw data can be downloaded from <a href=\"http://www.seas.upenn.edu/~taskar/ocr/\">http://www.seas.upenn.edu/~taskar/ocr/</a>. It has 6876 handwriten words with an average length of 8 letters from 150 different persons. Each letter is rasterized into a binary image of size 16 by 8 pixels. Thus, each $\\mathbf{y}$ is a chain, and each node has 26 possible states denoting ${a,\\cdots,z}$. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import numpy as np\n",
      "import scipy.io"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset = scipy.io.loadmat('../../../data/ocr/ocr_taskar.mat')\n",
      "# patterns for training\n",
      "p_tr = dataset['patterns_train']\n",
      "# patterns for testing\n",
      "p_ts = dataset['patterns_test']\n",
      "# labels for training\n",
      "l_tr = dataset['labels_train']\n",
      "# labels for testing\n",
      "l_ts = dataset['labels_test']\n",
      "\n",
      "# feature dimension\n",
      "n_dims = p_tr[0,0].shape[0]\n",
      "# number of states\n",
      "n_stats = 26\n",
      "# number of training samples\n",
      "n_tr_samples = p_tr.shape[1]\n",
      "# number of testing samples\n",
      "n_ts_samples = p_ts.shape[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Few examples of the handwriten words are shown below. Note that the first capitalized letter has been removed. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def show_word(patterns, index):\n",
      "    \"\"\"show a word with padding\"\"\"\n",
      "    plt.rc('image', cmap='binary')\n",
      "    letters = patterns[0,index][:128,:]\n",
      "    n_letters = letters.shape[1]\n",
      "    for l in xrange(n_letters):\n",
      "        lett = np.transpose(np.reshape(letters[:,l], (8,16)))\n",
      "        lett = np.hstack((np.zeros((16,1)), lett, np.zeros((16,1))))\n",
      "        lett = np.vstack((np.zeros((1,10)), lett, np.zeros((1,10))))\n",
      "        subplot(1,n_letters,l)\n",
      "        imshow(lett)\n",
      "        plt.xticks(())\n",
      "        plt.yticks(())\n",
      "    plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_word(p_tr, 174)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_word(p_tr, 471)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_word(p_tr, 57)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Define Factor Types and Build Factor Graphs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's define 4 factor types, such that a word will be able to be modeled as a chain graph.\n",
      "\n",
      "- The unary factor type will be used to define unary potentials that capture the appearance probabilities of each letter. In our case, each letter has $16 \\times 8$ pixels, thus there are $(16 \\times 8 + 1) \\times 26$ parameters. \n",
      "- The pairwise factor type will be used to define pairwise potentials between each pair of letters. This type in fact gives the Potts potentials. There are $26 \\times 26$ parameters. \n",
      "- The bias factor type for the first letter is a compensation factor type, since the interaction is one-sided. So there are $26$ parameters to be learned.\n",
      "- The bias factor type for the last letter, which has the same intuition as the last item. There are also $26$ parameters.\n",
      "\n",
      "Putting all parameters together, the global parameter vector $\\mathbf{w}$ has length $4082$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import TableFactorType\n",
      "\n",
      "# unary, type_id = 0\n",
      "cards_u = np.array([n_stats], np.int32)\n",
      "w_gt_u = np.zeros(n_stats*n_dims)\n",
      "fac_type_u = TableFactorType(0, cards_u, w_gt_u)\n",
      "\n",
      "# pairwise, type_id = 1\n",
      "cards = np.array([n_stats,n_stats], np.int32)\n",
      "w_gt = np.zeros(n_stats*n_stats)\n",
      "fac_type = TableFactorType(1, cards, w_gt)\n",
      "\n",
      "# first bias, type_id = 2\n",
      "cards_s = np.array([n_stats], np.int32)\n",
      "w_gt_s = np.zeros(n_stats)\n",
      "fac_type_s = TableFactorType(2, cards_s, w_gt_s)\n",
      "\n",
      "# last bias, type_id = 3\n",
      "cards_t = np.array([n_stats], np.int32)\n",
      "w_gt_t = np.zeros(n_stats)\n",
      "fac_type_t = TableFactorType(3, cards_t, w_gt_t)\n",
      "\n",
      "# all initial parameters\n",
      "w_all = [w_gt_u,w_gt,w_gt_s,w_gt_t]\n",
      "\n",
      "# all factor types\n",
      "ftype_all = [fac_type_u,fac_type,fac_type_s,fac_type_t]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we write a function to construct the factor graphs and prepare labels for training. For each factor graph instance, the structure is a chain but the number of nodes and edges depend on the number of letters, where unary factors will be added for each letter, pairwise factors will be added for each pair of neighboring letters. Besides, the first and last letter will get an additional bias factor respectively.   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def prepare_data(x, y, ftype, num_samples):\n",
      "\t\"\"\"prepare FactorGraphFeatures and FactorGraphLabels \"\"\"\n",
      "\tfrom modshogun import Factor, TableFactorType, FactorGraph\n",
      "\tfrom modshogun import FactorGraphObservation, FactorGraphLabels, FactorGraphFeatures\n",
      "\n",
      "\tsamples = FactorGraphFeatures(num_samples)\n",
      "\tlabels = FactorGraphLabels(num_samples)\n",
      "\n",
      "\tfor i in xrange(num_samples):\n",
      "\t\tn_vars = x[0,i].shape[1]\n",
      "\t\tdata = x[0,i].astype(np.float64)\n",
      "\n",
      "\t\tvc = np.array([n_stats]*n_vars, np.int32)\n",
      "\t\tfg = FactorGraph(vc)\n",
      "\n",
      "\t\t# add unary factors\n",
      "\t\tfor v in xrange(n_vars):\n",
      "\t\t\tdatau = data[:,v]\n",
      "\t\t\tvindu = np.array([v], np.int32)\n",
      "\t\t\tfacu = Factor(ftype[0], vindu, datau)\n",
      "\t\t\tfg.add_factor(facu)\n",
      "\n",
      "\t\t# add pairwise factors\n",
      "\t\tfor e in xrange(n_vars-1):\n",
      "\t\t\tdatap = np.array([1.0])\n",
      "\t\t\tvindp = np.array([e,e+1], np.int32)\n",
      "\t\t\tfacp = Factor(ftype[1], vindp, datap)\n",
      "\t\t\tfg.add_factor(facp)\n",
      "\n",
      "\t\t# add bias factor to first letter\n",
      "\t\tdatas = np.array([1.0])\n",
      "\t\tvinds = np.array([0], np.int32)\n",
      "\t\tfacs = Factor(ftype[2], vinds, datas)\n",
      "\t\tfg.add_factor(facs)\n",
      "\n",
      "\t\t# add bias factor to last letter\n",
      "\t\tdatat = np.array([1.0])\n",
      "\t\tvindt = np.array([n_vars-1], np.int32)\n",
      "\t\tfact = Factor(ftype[3], vindt, datat)\n",
      "\t\tfg.add_factor(fact)\n",
      "\n",
      "        # add factor graph\n",
      "\t\tsamples.add_sample(fg)\n",
      "\n",
      "        # add corresponding label\n",
      "\t\tstates_gt = y[0,i].astype(np.int32)\n",
      "\t\tstates_gt = states_gt[0,:]; # mat to vector\n",
      "\t\tloss_weights = np.array([1.0/n_vars]*n_vars)\n",
      "\t\tfg_obs = FactorGraphObservation(states_gt, loss_weights)\n",
      "\t\tlabels.add_label(fg_obs)\n",
      "\n",
      "\treturn samples, labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prepare training pairs (factor graph, node states)\n",
      "samples, labels = prepare_data(p_tr, l_tr, ftype_all, n_tr_samples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An example of graph structure is visualized as below, from which you may have a better sense how a factor graph being built. Note that different colors are using to represent different factor types."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx as nx # pip install networkx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# create a graph\n",
      "G = nx.Graph()\n",
      "node_pos = {}\n",
      "\n",
      "# add variable nodes, assuming there are 3 letters\n",
      "G.add_nodes_from(['v0','v1','v2'])\n",
      "for i in xrange(3):\n",
      "    node_pos['v%d' % i] = (2*i,1)\n",
      "\n",
      "# add factor nodes\n",
      "G.add_nodes_from(['F0','F1','F2','F01','F12','Fs','Ft'])\n",
      "for i in xrange(3):\n",
      "    node_pos['F%d' % i] = (2*i,1.006)\n",
      "    \n",
      "for i in xrange(2):\n",
      "    node_pos['F%d%d' % (i,i+1)] = (2*i+1,1)\n",
      "    \n",
      "node_pos['Fs'] = (-1,1)\n",
      "node_pos['Ft'] = (5,1)\n",
      "\n",
      "# add edges to connect variable nodes and factor nodes\n",
      "G.add_edges_from([('v%d' % i,'F%d' % i) for i in xrange(3)])\n",
      "G.add_edges_from([('v%d' % i,'F%d%d' % (i,i+1)) for i in xrange(2)])\n",
      "G.add_edges_from([('v%d' % (i+1),'F%d%d' % (i,i+1)) for i in xrange(2)])\n",
      "G.add_edges_from([('v0','Fs'),('v2','Ft')])\n",
      "\n",
      "# draw graph\n",
      "fig, ax = plt.subplots(figsize=(6,2))\n",
      "nx.draw_networkx_nodes(G,node_pos,nodelist=['v0','v1','v2'],node_color='white',node_size=700,ax=ax)\n",
      "nx.draw_networkx_nodes(G,node_pos,nodelist=['F0','F1','F2'],node_color='yellow',node_shape='s',node_size=300,ax=ax)\n",
      "nx.draw_networkx_nodes(G,node_pos,nodelist=['F01','F12'],node_color='blue',node_shape='s',node_size=300,ax=ax)\n",
      "nx.draw_networkx_nodes(G,node_pos,nodelist=['Fs'],node_color='green',node_shape='s',node_size=300,ax=ax)\n",
      "nx.draw_networkx_nodes(G,node_pos,nodelist=['Ft'],node_color='purple',node_shape='s',node_size=300,ax=ax)\n",
      "nx.draw_networkx_edges(G,node_pos,alpha=0.7)\n",
      "plt.axis('off')\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can create the factor graph model and start training. We will use the max-product belief propagation to do MAP inference."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import FactorGraphModel, TREE_MAX_PROD\n",
      "\n",
      "# create model and register factor types\n",
      "model = FactorGraphModel(samples, labels, TREE_MAX_PROD)\n",
      "model.add_factor_type(ftype_all[0])\n",
      "model.add_factor_type(ftype_all[1])\n",
      "model.add_factor_type(ftype_all[2])\n",
      "model.add_factor_type(ftype_all[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We choose the dual bundle method solver (<a href=\"http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CDualLibQPBMSOSVM.html\">DualLibQPBMSOSVM</a>) [2], since in practice it is slightly faster than the primal n-slack cutting plane solver (<a a href=\"http://www.shogun-toolbox.org/doc/en/latest/PrimalMosekSOSVM_8h.html\">PrimalMosekSOSVM</a>) [3]. However, it still will take a while until convergence. Briefly, in each iteration, a gradually tighter piece-wise linear lower bound of the objective function will be constructed by adding more cutting planes (most violated constraints), then the approximate QP will be solved. Finding a cutting plane involves calling the max oracle $H_i(\\mathbf{w})$ and in average $N$ calls are required in an iteration. This is basically why the training is time consuming. Fortunately, our new block coordinate Franke-Wolfe solver [4] is coming soon, which is one order of magnititute faster than cutting plane algorithms in theory. We will compare all the solvers later in this demo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import DualLibQPBMSOSVM\n",
      "\n",
      "# create bundle method SOSVM, there are few variants can be chosen\n",
      "# BMRM, Proximal Point BMRM, Proximal Point P-BMRM, NCBM\n",
      "# usually the default one i.e. BMRM is good enough\n",
      "bmrm = DualLibQPBMSOSVM(model, labels, 0.01)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check the dualality gap to see if the training has converged. By the weak duality theorem, the optimal value of the primal problem is always greater than or equal to dual problem. Thus, we could expect the duality gap will decrease during the time. A relative small and statble duality gap may indicate the convergence. In fact, we don't need the gap becomes zero, since we know it is not far away from the local minima."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import BmrmStatistics\n",
      "\n",
      "hist_Fp = statistics.get_hist_Fp_vector()\n",
      "hist_Fd = statistics.get_hist_Fd_vector()\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "# plot duality gaps\n",
      "xs = range(hist_Fd.size)\n",
      "ax.plot(xs, hist_Fp-hist_Fd, label='duality gap')\n",
      "ax.set_xlabel('iteration')\n",
      "ax.set_ylabel('duality gap')\n",
      "_ = ax.set_title('training progress')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are other statitics may also help to check if the solution is good or not, such as the number of cutting planes, from which we may have a sense how good the piece-wise lower bound is. In general, the number of cutting planes should be much less than the dimension of the parameter vector."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'number of cutting planes: %d' % statistics.nCP\n",
      "print 'number of active cutting planes: %d' % statistics.nzA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Besides, we would like to show the pairwise weights, which may learn important co-occurrances of letters. The hinton diagram is a wanderful tool for visualizing 2D data, in which positive and negative values are represented by white and black squares, respectively, and the size of each square represents the magnitude of each value. In our case, a smaller number i.e. a large black square indicates the two letters tend to coincide.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hinton(matrix, max_weight=None, ax=None):\n",
      "    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n",
      "    ax = ax if ax is not None else plt.gca()\n",
      "\n",
      "    if not max_weight:\n",
      "        max_weight = 2**np.ceil(np.log(np.abs(matrix).max())/np.log(2))\n",
      "\n",
      "    ax.patch.set_facecolor('gray')\n",
      "    ax.set_aspect('equal', 'box')\n",
      "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
      "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
      "\n",
      "    for (x,y),w in np.ndenumerate(matrix):\n",
      "        color = 'white' if w > 0 else 'black'\n",
      "        size = np.sqrt(np.abs(w))\n",
      "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
      "                             facecolor=color, edgecolor=color)\n",
      "        ax.add_patch(rect)\n",
      "\n",
      "    ax.autoscale_view()\n",
      "    ax.invert_yaxis()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get pairwise parameters, also accessible from\n",
      "# w[n_dims*n_stats:n_dims*n_stats+n_stats*n_stats]\n",
      "w_p = ftype_all[1].get_w()\n",
      "w_p = np.reshape(w_p,(n_stats,n_stats))\n",
      "hinton(w_p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Inference"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we show how to do inference with the learned model parameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get testing data\n",
      "samples_ts, labels_ts = prepare_data(p_ts, l_ts, ftype_all, n_ts_samples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import FactorGraphFeatures, FactorGraphObservation, TREE_MAX_PROD, MAPInference\n",
      "\n",
      "# get a factor graph instance from test data\n",
      "fg0 = samples_ts.get_sample(0)\n",
      "fg0.compute_energies()\n",
      "fg0.connect_components()\n",
      "\n",
      "# create a MAP inference using tree max-product\n",
      "infer_met = MAPInference(fg0, TREE_MAX_PROD)\n",
      "infer_met.inference()\n",
      "\n",
      "# get inference results\n",
      "y_pred = infer_met.get_structured_outputs()\n",
      "y_truth = FactorGraphObservation.obtain_from_generic(labels_ts.get_label(0))\n",
      "print y_pred.get_data()\n",
      "print y_truth.get_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the end, we check training errors and testing errors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import LabelsFactory\n",
      "\n",
      "# training errors\n",
      "lbs_bmrm = LabelsFactory.to_structured(bmrm.apply())\n",
      "acc_loss = 0.0\n",
      "ave_loss = 0.0\n",
      "for i in xrange(n_tr_samples):\n",
      "\ty_pred = lbs_bmrm.get_label(i)\n",
      "\ty_truth = labels.get_label(i)\n",
      "\tacc_loss = acc_loss + model.delta_loss(y_truth, y_pred)\n",
      "\n",
      "ave_loss = acc_loss / n_tr_samples\n",
      "print('Average training error is %.4f' % ave_loss)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# testing errors\n",
      "bmrm.set_features(samples_ts)\n",
      "bmrm.set_labels(labels_ts)\n",
      "\n",
      "lbs_bmrm_ts = LabelsFactory.to_structured(bmrm.apply())\n",
      "acc_loss = 0.0\n",
      "ave_loss_ts = 0.0\n",
      "\n",
      "for i in xrange(n_ts_samples):\n",
      "\t#result = model.argmax(w,i)\n",
      "\t#y_pred = FactorGraphObservation.obtain_from_generic(result.argmax)\n",
      "\ty_pred = lbs_bmrm_ts.get_label(i)\n",
      "\ty_truth = labels_ts.get_label(i)\n",
      "\tacc_loss = acc_loss + model.delta_loss(y_truth, y_pred)\n",
      "\n",
      "ave_loss_ts = acc_loss / n_ts_samples\n",
      "print('Average testing error is %.4f' % ave_loss_ts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[1] Kschischang, F. R., B. J. Frey, and H.-A. Loeliger, Factor graphs and the sum-product algorithm, IEEE Transactions on Information Theory 2001.\n",
      "\n",
      "[2] Teo, C.H., Vishwanathan, S.V.N, Smola, A. and Quoc, V.Le, Bundle Methods for Regularized Risk Minimization, JMLR 2010.\n",
      "\n",
      "[3] Tsochantaridis, I., Hofmann, T., Joachims, T., Altun, Y., Support Vector Machine Learning for Interdependent and Structured Ouput Spaces, ICML 2004.\n",
      "\n",
      "[4] Lacoste-Julien, S., Jaggi, M., Schmidt, M., Pletscher, P., Block-Coordinate Frank-Wolfe Optimization for Structural SVMs, ICML 2013."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}