{
 "metadata": {
  "name": "gp_regression"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gaussian Process Regression with Shogun"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import all necessary modules from Shogun"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import RealFeatures, RegressionLabels, GaussianKernel, Math\n",
      "from modshogun import GaussianLikelihood, ZeroMean, ExactInferenceMethod, GaussianProcessRegression"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate some data, a 1d noisy sine wave, evaluated at random points"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n=15\n",
      "x_range=4*pi\n",
      "y_noise_variance=0.1\n",
      "y_amplitude=1\n",
      "y_frequency=1\n",
      "\n",
      "X=random.rand(1,n)*x_range\n",
      "Y=sin(X*y_frequency)*y_amplitude+randn(n)*sqrt(y_noise_variance)\n",
      "X_test=linspace(0,x_range, 200)\n",
      "Y_true=sin(X_test)\n",
      "\n",
      "plot(X_test,Y_true, 'b-')\n",
      "plot(X,Y, 'ro')\n",
      "_=legend(['data generating model', 'noisy observations'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Convert data into Shogun representation, print dimensions to be sure data was passed in correct "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labels=RegressionLabels(Y.ravel())\n",
      "feats_train=RealFeatures(X)\n",
      "feats_test=RealFeatures(reshape(X_test, (1, len(X_test))))\n",
      "\n",
      "print feats_train.get_num_vectors()\n",
      "print feats_train.get_num_features()\n",
      "print feats_test.get_num_vectors()\n",
      "print feats_test.get_num_features()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specify a Shogun GP (exact GP-regression) with fixed hyper-parameters and pass it the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kernel_sigma=1\n",
      "gp_obs_noise=0.5\n",
      "\n",
      "kernel=GaussianKernel(10, kernel_sigma)\n",
      "mean=ZeroMean()\n",
      "lik=GaussianLikelihood(gp_obs_noise)\n",
      "inf=ExactInferenceMethod(kernel, feats_train, mean, labels, lik)\n",
      "gp = GaussianProcessRegression(inf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train GP and plot its predictions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_=gp.train()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform inference and plot predictions on full range"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictions=gp.apply(feats_test)\n",
      "Y_test=predictions.get_labels()\n",
      "\n",
      "plot(X_test,Y_true, 'b')\n",
      "plot(X_test, Y_test, 'r-')\n",
      "plot(X,Y, 'ro')\n",
      "_=legend(['data generating model', 'mean predictions', 'noisy observations'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far so good. The nice thing is: we have a distribution over the predictions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean = gp.get_mean_vector(feats_test)\n",
      "variance = gp.get_variance_vector(feats_test)\n",
      "\n",
      "# print 95% confidence region\n",
      "plot(X_test,Y_true, 'b')\n",
      "plot(X_test, Y_test, 'r-')\n",
      "plot(X,Y, 'ro')\n",
      "error=1.96*sqrt(variance)\n",
      "fill_between(X_test,mean-error,mean+error,color='grey')\n",
      "\n",
      "ylim([-y_amplitude,y_amplitude+1])\n",
      "_=legend(['data generating model', 'mean predictions', 'noisy observations'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can even visualise it more fancy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import norm\n",
      "\n",
      "means = gp.get_mean_vector(feats_test)\n",
      "variances = gp.get_variance_vector(feats_test)\n",
      "\n",
      "y_values=linspace(-y_amplitude-2*y_noise_variance, y_amplitude+2*y_noise_variance)\n",
      "D=zeros((len(y_values), len(X_test)))\n",
      "\n",
      "# evaluate normal distribution at every prediction point (column)\n",
      "for i in range(shape(D)[1]):\n",
      "    norm.pdf(y_values, means[i], variances[i])\n",
      "    D[:,i]=norm.pdf(y_values, means[i], variances[i])\n",
      "    \n",
      "pcolor(X_test,y_values,D)\n",
      "plot(X_test,Y_true, 'b')\n",
      "plot(X_test, Y_test, 'r-')\n",
      "_=plot(X,Y, 'ro')\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets learn the best model parameters with Maximum Likelihood II"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from shogun.ModelSelection import GradientModelSelection, ModelSelectionParameters, R_LINEAR, R_EXP\n",
      "from shogun.Regression import GradientCriterion, GradientEvaluation\n",
      "\n",
      "kernel=GaussianKernel(10, kernel_sigma)\n",
      "mean=ZeroMean()\n",
      "lik=GaussianLikelihood(gp_obs_noise)\n",
      "inf=ExactInferenceMethod(kernel, feats_train, mean, labels, lik)\n",
      "gp = GaussianProcessRegression(inf)\n",
      "\n",
      "# construct model selection parameter tree\n",
      "root=ModelSelectionParameters();\n",
      "c1=ModelSelectionParameters(\"inference_method\", inf);\n",
      "root.append_child(c1);\n",
      "\n",
      "c2=ModelSelectionParameters(\"likelihood_model\", lik);\n",
      "c1.append_child(c2);\n",
      "\n",
      "c3=ModelSelectionParameters(\"sigma\");\n",
      "c2.append_child(c3);\n",
      "c3.build_values(-1.0, 1.0, R_EXP);\n",
      "\n",
      "c4=ModelSelectionParameters(\"scale\");\n",
      "c1.append_child(c4);\n",
      "c4.build_values(-1.0, 1.0, R_EXP);\n",
      "\n",
      "c5=ModelSelectionParameters(\"kernel\", kernel);\n",
      "c1.append_child(c5);\n",
      "\n",
      "c6=ModelSelectionParameters(\"width\");\n",
      "c5.append_child(c6);\n",
      "c6.build_values(-1.0, 1.0, R_EXP);\n",
      "\n",
      "\n",
      "# Criterion for Gradient Search\n",
      "crit = GradientCriterion()\n",
      "\n",
      "# Evaluate our inference method for its derivatives\n",
      "grad = GradientEvaluation(gp, feats_train, labels, crit)\n",
      "\n",
      "grad.set_function(inf) \n",
      "gp.print_modsel_params() \n",
      "root.print_tree() \n",
      "\n",
      "# gradient descent on marginal likelihood\n",
      "grad_search = GradientModelSelection(root, grad) \n",
      "\n",
      "# Set autolocking to false to get rid of warnings\t\n",
      "grad.set_autolock(False) \n",
      "\n",
      "# Search for best parameters\n",
      "best_combination = grad_search.select_model(True)\n",
      "\n",
      "# apply them to gp\n",
      "best_combination.apply_to_machine(gp)\n",
      "\n",
      "# training and inference with learned parameters\n",
      "gp.train()\n",
      "predictions=gp.apply(feats_test)\n",
      "Y_test=predictions.get_labels()\n",
      "\n",
      "# visualise\n",
      "means = gp.get_mean_vector(feats_test)\n",
      "variances = gp.get_variance_vector(feats_test)\n",
      "\n",
      "y_values=linspace(-y_amplitude-2*y_noise_variance, y_amplitude+2*y_noise_variance)\n",
      "D=zeros((len(y_values), len(X_test)))\n",
      "\n",
      "# evaluate normal distribution at every prediction point (column)\n",
      "for i in range(shape(D)[1]):\n",
      "    norm.pdf(y_values, means[i], variances[i])\n",
      "    D[:,i]=norm.pdf(y_values, means[i], variances[i])\n",
      "    \n",
      "pcolor(X_test,y_values,D)\n",
      "plot(X_test,Y_true, 'b')\n",
      "plot(X_test, Y_test, 'r-')\n",
      "_=plot(X,Y, 'ro')\n",
      "\n",
      "# print best parameters\n",
      "print \"kernel width\", kernel.get_width()\n",
      "print \"kernel scalling\", inf.get_scale()\n",
      "print \"noise level\", lik.get_sigma()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}